{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06f3793b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/thien/Code/NTN/Hest_NTN_UDA/JMMD/computeCA/run_JMMD_GAN\n",
      "/home/thien/Code/NTN/Hest_NTN_UDA\n",
      "N_samp_source =  2048\n",
      "N_samp_target =  2048\n",
      "train_size =  128\n",
      "val_size =  48\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Source-Only WGAN-GP Training Script\n",
    "\n",
    "This script trains a WGAN-GP model using only the source domain data, without domain adaptation.\n",
    "- Training: Source domain training set\n",
    "- Validation: Source domain validation set  \n",
    "- Testing: Target domain validation set (to measure generalization performance)\n",
    "\n",
    "The model architecture remains the same, but no domain adaptation techniques (JMMD) are applied.\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy.io import savemat\n",
    "import h5py\n",
    "\n",
    "# Add the root project directory\n",
    "try:\n",
    "    code_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    project_root = os.path.abspath(os.path.join(code_dir, '..', '..', '..'))\n",
    "except NameError:\n",
    "    # Running in Jupyter Notebook\n",
    "    code_dir = os.getcwd()\n",
    "    project_root = os.path.abspath(os.path.join(code_dir, '..', '..', '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "print(code_dir)\n",
    "print(project_root) # Hest_NTN_UDA/\n",
    "\n",
    "from Domain_Adversarial.helper import loader, plotfig, PAD\n",
    "from Domain_Adversarial.helper.utils import H5BatchLoader\n",
    "from Domain_Adversarial.helper.utils_GAN import visualize_H\n",
    "from JMMD.helper.utils_GAN import save_checkpoint_jmmd as save_checkpoint\n",
    "\n",
    "SNR = -5\n",
    "# source_data_file_path_label = os.path.abspath(os.path.join(code_dir, '..', 'generatedChan', 'OpenNTN','H_perfect.mat'))\n",
    "source_data_file_path = os.path.abspath(os.path.join(code_dir, '..', '..', '..', 'generatedChan', 'MATLAB', 'TDL_A_30', f'SNR_{SNR}dB', 'matlabNTN.mat'))\n",
    "target_data_file_path = os.path.abspath(os.path.join(code_dir, '..', '..', '..', 'generatedChan', 'MATLAB', 'TDL_C_30', f'SNR_{SNR}dB', 'matlabNTN.mat'))\n",
    "norm_approach = 'minmax' # can be set to 'std'\n",
    "lower_range = -1 \n",
    "    # if norm_approach = 'minmax': \n",
    "        # =  0 for scaling to  [0 1]\n",
    "        # = -1 for scaling to [-1 1]\n",
    "    # if norm_approach = 'std': can be any value, but need to be defined\n",
    "weights = {\n",
    "    # Core loss weights\n",
    "    'adv_weight': 0.005,        # GAN adversarial loss weight\n",
    "    'est_weight': 1.0,          # Estimation loss weight (main task)\n",
    "    'domain_weight': 0.0,      # No domain_weight since we're not doing domain adaptation\n",
    "    \n",
    "    # Smoothness regularization weights\n",
    "    'temporal_weight': 0.02,    # Temporal smoothness penalty\n",
    "    'frequency_weight': 0.1,    # Frequency smoothness penalty\n",
    "}\n",
    "\n",
    "if norm_approach == 'minmax':\n",
    "    if lower_range == 0:\n",
    "        norm_txt = 'Using min-max [0 1]'\n",
    "    elif lower_range ==-1:\n",
    "        norm_txt = 'Using min-max [-1 1]'\n",
    "elif norm_approach == 'no':\n",
    "    norm_txt = 'No'\n",
    "    \n",
    "# Paths to save\n",
    "path_temp = project_root + f'/JMMD/model/GAN_onlySource/{SNR}_dB/'\n",
    "os.makedirs(os.path.dirname(path_temp), exist_ok=True)\n",
    "idx_save_path = loader.find_incremental_filename(path_temp,'ver', '_', '')\n",
    "\n",
    "save_model = False\n",
    "model_path = project_root + f'/JMMD/model/GAN_onlySource/{SNR}_dB/ver' + str(idx_save_path) + '_'\n",
    "# figure_path = code_dir + '/model/GAN/ver' + str(idx_save_path) + '_/figure'\n",
    "model_readme = model_path + '/readme.txt'\n",
    "\n",
    "batch_size=16\n",
    "\n",
    "# ============ Source data ==============\n",
    "source_file = h5py.File(source_data_file_path, 'r')\n",
    "H_true_source = source_file['H_perfect']\n",
    "N_samp_source = H_true_source.shape[0]\n",
    "print('N_samp_source = ', N_samp_source)\n",
    "\n",
    "# ============ Target data ==============\n",
    "target_file = h5py.File(target_data_file_path, 'r')\n",
    "H_true_target = target_file['H_perfect']\n",
    "N_samp_target = H_true_target.shape[0]\n",
    "print('N_samp_target = ', N_samp_target)\n",
    "\n",
    "# Store random state \n",
    "rng_state = np.random.get_state()\n",
    "\n",
    "# --- Set a temporary seed for reproducible split ---\n",
    "np.random.seed(1234)   # any fixed integer seed\n",
    "# Random but repeatable split\n",
    "indices_source = np.arange(N_samp_source)\n",
    "np.random.shuffle(indices_source)\n",
    "indices_target = np.arange(N_samp_target)\n",
    "np.random.shuffle(indices_target)\n",
    "# Restore previous random state (so other code stays random)\n",
    "np.random.set_state(rng_state)\n",
    "#\n",
    "train_size = int(np.floor(N_samp_source * 0.9) // batch_size * batch_size)\n",
    "val_size = N_samp_source - train_size\n",
    "\n",
    "# Repeat the indices to match the maximum number of samples\n",
    "N_samp = max(N_samp_source, N_samp_target) \n",
    "indices_source = np.resize(indices_source, N_samp)\n",
    "indices_target = np.resize(indices_target, N_samp)\n",
    "\n",
    "# =======================================================\n",
    "## Divide the indices into training and validation sets\n",
    "# indices_train_source = indices_source[:train_size]\n",
    "# indices_val_source   = indices_source[train_size:train_size + val_size]\n",
    "\n",
    "# indices_train_target = indices_target[:train_size]\n",
    "# indices_val_target   = indices_target[train_size:train_size + val_size]\n",
    "\n",
    "# to test code\n",
    "indices_train_source = indices_source[:128]\n",
    "indices_val_source = indices_source[2000:]\n",
    "indices_train_target = indices_target[:128]\n",
    "indices_val_target = indices_target[2000:]\n",
    "\n",
    "print('train_size = ', indices_train_source.shape[0])\n",
    "print('val_size = ', indices_val_source.shape[0])\n",
    "\n",
    "class DataLoaders:\n",
    "    def __init__(self, file, indices_train, indices_val, tag='prac', batch_size=32): \n",
    "        # tag = 'prac' or 'li' or 'ls'\n",
    "        self.true_train = H5BatchLoader(file, dataset_name='H_perfect', batch_size=batch_size, shuffled_indices=indices_train)\n",
    "        self.true_val = H5BatchLoader(file, dataset_name='H_perfect', batch_size=batch_size, shuffled_indices=indices_val)\n",
    "\n",
    "        self.input_train = H5BatchLoader(file, f'H_{tag}', batch_size=batch_size, shuffled_indices=indices_train)\n",
    "        self.input_val = H5BatchLoader(file, f'H_{tag}', batch_size=batch_size, shuffled_indices=indices_val)\n",
    "\n",
    "# Source domain\n",
    "class_dict_source = {\n",
    "    'GAN_practical': DataLoaders(source_file, indices_train_source, indices_val_source, tag='prac', batch_size=batch_size),\n",
    "    'GAN_linear': DataLoaders(source_file, indices_train_source, indices_val_source, tag='li', batch_size=batch_size),\n",
    "    'GAN_ls': DataLoaders(source_file, indices_train_source, indices_val_source, tag='ls', batch_size=batch_size)\n",
    "}\n",
    "\n",
    "# Target domain\n",
    "class_dict_target = {\n",
    "    'GAN_practical': DataLoaders(target_file, indices_train_target, indices_val_target, tag='prac', batch_size=batch_size),\n",
    "    'GAN_linear': DataLoaders(target_file, indices_train_target, indices_val_target, tag='li', batch_size=batch_size),\n",
    "    'GAN_ls': DataLoaders(target_file, indices_train_target, indices_val_target, tag='ls', batch_size=batch_size)\n",
    "}\n",
    "\n",
    "loss_fn_ce = tf.keras.losses.MeanSquaredError()  # Channel estimation loss (generator loss)\n",
    "loss_fn_bce = tf.keras.losses.BinaryCrossentropy(from_logits=False) # Binary cross-entropy loss for discriminator\n",
    "\n",
    "from JMMD.helper.utils_GAN import GAN\n",
    "from JMMD.helper.utils_GAN import train_step_wgan_gp_source_only, val_step_wgan_gp_source_only, post_val\n",
    "\n",
    "import time\n",
    "start = time.perf_counter()\n",
    "\n",
    "# n_epochs= 300\n",
    "# epoch_min = 50\n",
    "# epoch_step = 50\n",
    "n_epochs= 3\n",
    "epoch_min = 0\n",
    "epoch_step = 1\n",
    "\n",
    "sub_folder_ = ['GAN_linear']  # ['GAN_linear', 'GAN_practical', 'GAN_ls']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8b26fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: GAN_linear\n",
      "Calculating Wasserstein-1 distance for original input training datasets (before training)...\n",
      "X shape =  (256, 528)\n",
      "X1 shape =  (128, 528) y1 shape =  (128,)\n",
      "(128, 528) (128,)\n",
      "C: 0.01, Error rate: 0.5234\n",
      "C: 0.1, Error rate: 0.0000\n",
      "C: 0.5, Error rate: 0.0000\n",
      "C: 1.0, Error rate: 0.0000\n",
      "C: 2.0, Error rate: 0.0000\n",
      "C: 5.0, Error rate: 0.0000\n",
      "C: 10.0, Error rate: 0.0000\n",
      "C: 50.0, Error rate: 0.0000\n",
      "C: 100.0, Error rate: 0.0000\n",
      "C: 500.0, Error rate: 0.0000\n",
      "C: 1000.0, Error rate: 0.0000\n",
      "Best C: 0.1, Best error rate: 0.0000\n",
      "PAD = 2.0000\n",
      "PAD = 2.0000\n",
      "Fitted PCA on batch: source 128/128, target 128/128\n",
      "Reduced source shape: (128, 100), target shape: (128, 100)\n",
      "== C: 0.01, Error rate: 0.5312\n",
      "== C: 0.1, Error rate: 0.5312\n",
      "== C: 0.5, Error rate: 0.1641\n",
      "== C: 1.0, Error rate: 0.1250\n",
      "== C: 2.0, Error rate: 0.1250\n",
      "== C: 5.0, Error rate: 0.1094\n",
      "== C: 10.0, Error rate: 0.1094\n",
      "== C: 50.0, Error rate: 0.1094\n",
      "== C: 100.0, Error rate: 0.1094\n",
      "== C: 500.0, Error rate: 0.1094\n",
      "== C: 1000.0, Error rate: 0.1094\n",
      "Best C: 5.0, Best error rate: 0.1094\n",
      "============ PAD (SVM) = 1.5625\n",
      "LDA Error rate: 0.0547\n",
      "============ PAD (LDA) = 1.7812\n",
      "Logistic Regression Error rate: 0.1797\n",
      "============ PAD (LogReg) = 1.2812\n",
      "Time 14.615852538030595 seconds\n",
      "epoch 1/3\n",
      "Fitting IncrementalPCA on batches from features_source.h5 and features_target.h5\n",
      "Fitted PCA on batch: source 56/56, target 56/56\n",
      "Reduced source shape: (56, 100), target shape: (56, 100)\n",
      "== C: 0.01, Error rate: 0.5536\n",
      "== C: 0.1, Error rate: 0.5536\n",
      "== C: 0.5, Error rate: 0.5536\n",
      "== C: 1.0, Error rate: 0.0714\n",
      "== C: 2.0, Error rate: 0.0357\n",
      "== C: 5.0, Error rate: 0.0357\n",
      "== C: 10.0, Error rate: 0.0357\n",
      "== C: 50.0, Error rate: 0.0357\n",
      "== C: 100.0, Error rate: 0.0357\n",
      "== C: 500.0, Error rate: 0.0357\n",
      "== C: 1000.0, Error rate: 0.0357\n",
      "Best C: 2.0, Best error rate: 0.0357\n",
      "============ PAD (SVM) = 1.8571\n",
      "LDA Error rate: 0.5893\n",
      "Flip the predictions\n",
      "============ PAD (LDA) = 0.3571\n",
      "Logistic Regression Error rate: 0.5536\n",
      "Flip the predictions\n",
      "============ PAD (LogReg) = 0.2143\n",
      "Time 16.18124373606406 seconds\n",
      "epoch 1/3 Average Training Loss: 0.255515\n",
      "epoch 1/3 Average Estimation Loss (in Source domain): 0.219027\n",
      "epoch 1/3 Average Disc Loss (in Source domain): 19506.837891\n",
      "epoch 1/3 Source-only training (No domain adaptation): 0.000000\n",
      "epoch 1/3 Testing performance on Target domain: 0.237842\n",
      "epoch 1/3 (Val) Weighted Total Loss: 113.298477\n",
      "epoch 1/3 (Val) Average Estimation Loss (mean): 0.169172\n",
      "epoch 1/3 (Val) Average Estimation Loss (Source): 0.169172\n",
      "epoch 1/3 (Val) Average Estimation Loss (Target): 0.225430\n",
      "epoch 1/3 (Val) GAN Discriminator Loss: 22625.861328\n",
      "epoch 1/3 (Val) NMSE (Source): 3.574018, NMSE (Target): 4.887490, NMSE (Mean): 3.574018\n",
      "epoch 1/3 (Val) Domain Accuracy (Average): 0.5000\n",
      "epoch 1/3 (Val) Smoothness Loss: 0.000000\n",
      "Time 22.503643981879577 seconds\n",
      "epoch 2/3\n",
      "Fitting IncrementalPCA on batches from features_source.h5 and features_target.h5\n",
      "Fitted PCA on batch: source 56/56, target 56/56\n",
      "Reduced source shape: (56, 100), target shape: (56, 100)\n",
      "== C: 0.01, Error rate: 0.5536\n",
      "== C: 0.1, Error rate: 0.5536\n",
      "== C: 0.5, Error rate: 0.5536\n",
      "== C: 1.0, Error rate: 0.1250\n",
      "== C: 2.0, Error rate: 0.0357\n",
      "== C: 5.0, Error rate: 0.0357\n",
      "== C: 10.0, Error rate: 0.0357\n",
      "== C: 50.0, Error rate: 0.0357\n",
      "== C: 100.0, Error rate: 0.0357\n",
      "== C: 500.0, Error rate: 0.0357\n",
      "== C: 1000.0, Error rate: 0.0357\n",
      "Best C: 2.0, Best error rate: 0.0357\n",
      "============ PAD (SVM) = 1.8571\n",
      "LDA Error rate: 0.6607\n",
      "Flip the predictions\n",
      "============ PAD (LDA) = 0.6429\n",
      "Logistic Regression Error rate: 0.5714\n",
      "Flip the predictions\n",
      "============ PAD (LogReg) = 0.2857\n",
      "Time 24.14529184694402 seconds\n",
      "epoch 2/3 Average Training Loss: 0.183435\n",
      "epoch 2/3 Average Estimation Loss (in Source domain): 0.165803\n",
      "epoch 2/3 Average Disc Loss (in Source domain): 21159.066406\n",
      "epoch 2/3 Source-only training (No domain adaptation): 0.000000\n",
      "epoch 2/3 Testing performance on Target domain: 0.202844\n",
      "epoch 2/3 (Val) Weighted Total Loss: 103.913345\n",
      "epoch 2/3 (Val) Average Estimation Loss (mean): 0.132983\n",
      "epoch 2/3 (Val) Average Estimation Loss (Source): 0.132983\n",
      "epoch 2/3 (Val) Average Estimation Loss (Target): 0.196287\n",
      "epoch 2/3 (Val) GAN Discriminator Loss: 20756.074219\n",
      "epoch 2/3 (Val) NMSE (Source): 2.813979, NMSE (Target): 4.247760, NMSE (Mean): 2.813979\n",
      "epoch 2/3 (Val) Domain Accuracy (Average): 0.5000\n",
      "epoch 2/3 (Val) Smoothness Loss: 0.000000\n",
      "Time 29.724598184926435 seconds\n",
      "epoch 3/3\n",
      "Fitting IncrementalPCA on batches from features_source.h5 and features_target.h5\n",
      "Fitted PCA on batch: source 56/56, target 56/56\n",
      "Reduced source shape: (56, 100), target shape: (56, 100)\n",
      "== C: 0.01, Error rate: 0.5536\n",
      "== C: 0.1, Error rate: 0.5536\n",
      "== C: 0.5, Error rate: 0.5536\n",
      "== C: 1.0, Error rate: 0.0893\n",
      "== C: 2.0, Error rate: 0.0536\n",
      "== C: 5.0, Error rate: 0.0536\n",
      "== C: 10.0, Error rate: 0.0536\n",
      "== C: 50.0, Error rate: 0.0536\n",
      "== C: 100.0, Error rate: 0.0536\n",
      "== C: 500.0, Error rate: 0.0536\n",
      "== C: 1000.0, Error rate: 0.0536\n",
      "Best C: 2.0, Best error rate: 0.0536\n",
      "============ PAD (SVM) = 1.7857\n",
      "LDA Error rate: 0.6964\n",
      "Flip the predictions\n",
      "============ PAD (LDA) = 0.7857\n",
      "Logistic Regression Error rate: 0.5714\n",
      "Flip the predictions\n",
      "============ PAD (LogReg) = 0.2857\n",
      "Time 31.10812490200624 seconds\n",
      "epoch 3/3 Average Training Loss: 0.147743\n",
      "epoch 3/3 Average Estimation Loss (in Source domain): 0.138759\n",
      "epoch 3/3 Average Disc Loss (in Source domain): 26287.878906\n",
      "epoch 3/3 Source-only training (No domain adaptation): 0.000000\n",
      "epoch 3/3 Testing performance on Target domain: 0.179260\n",
      "epoch 3/3 (Val) Weighted Total Loss: 156.767563\n",
      "epoch 3/3 (Val) Average Estimation Loss (mean): 0.114784\n",
      "epoch 3/3 (Val) Average Estimation Loss (Source): 0.114784\n",
      "epoch 3/3 (Val) Average Estimation Loss (Target): 0.175947\n",
      "epoch 3/3 (Val) GAN Discriminator Loss: 31330.556641\n",
      "epoch 3/3 (Val) NMSE (Source): 2.431082, NMSE (Target): 3.801380, NMSE (Mean): 2.431082\n",
      "epoch 3/3 (Val) Domain Accuracy (Average): 0.5000\n",
      "epoch 3/3 (Val) Smoothness Loss: 0.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for sub_folder in sub_folder_:\n",
    "    print(f\"Processing: {sub_folder}\")\n",
    "    pad_metrics = {\n",
    "        'pad_pca_lda': {},      # Dictionary to store LDA PAD values by epoch\n",
    "        'pad_pca_logreg': {},   # Dictionary to store LogReg PAD values by epoch\n",
    "        'pad_pca_svm': {},      # Dictionary to store SVM PAD values by epoch\n",
    "        'w_dist': {}            # Dictionary to store Wasserstein distances by epoch\n",
    "    }\n",
    "    linear_interp = False\n",
    "    if sub_folder == 'GAN_linear':\n",
    "        linear_interp =True # flag to clip values that go beyond the estimated pilot (min, max)\n",
    "    ##\n",
    "    loader_H_true_train_source = class_dict_source[sub_folder].true_train\n",
    "    loader_H_input_train_source = class_dict_source[sub_folder].input_train\n",
    "    loader_H_true_val_source = class_dict_source[sub_folder].true_val\n",
    "    loader_H_input_val_source = class_dict_source[sub_folder].input_val\n",
    "    \n",
    "    loader_H_true_train_target = class_dict_target[sub_folder].true_train\n",
    "    loader_H_input_train_target = class_dict_target[sub_folder].input_train\n",
    "    loader_H_true_val_target = class_dict_target[sub_folder].true_val\n",
    "    loader_H_input_val_target = class_dict_target[sub_folder].input_val\n",
    "    ##\n",
    "    \n",
    "    # Distribution of original input training datasets (or before training)    \n",
    "    plotfig.plotHist(loader_H_input_train_source, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name='source_beforeTrain', percent=100)\n",
    "    plotfig.plotHist(loader_H_input_train_target, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name='target_beforeTrain', percent=100)\n",
    "    \n",
    "    plotfig.plotHist(loader_H_input_train_source, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name='source_beforeTrain', percent=99)\n",
    "    plotfig.plotHist(loader_H_input_train_target, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name='target_beforeTrain', percent=99)\n",
    "    \n",
    "    plotfig.plotHist(loader_H_input_train_source, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name='source_beforeTrain', percent=95)\n",
    "    plotfig.plotHist(loader_H_input_train_target, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name='target_beforeTrain', percent=95)\n",
    "\n",
    "    plotfig.plotHist(loader_H_input_train_source, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name='source_beforeTrain', percent=90)\n",
    "    plotfig.plotHist(loader_H_input_train_target, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name='target_beforeTrain', percent=90)\n",
    "\n",
    "    # Calculate Wasserstein-1 distance for original input training datasets (before training)\n",
    "    print(\"Calculating Wasserstein-1 distance for original input training datasets (before training)...\")\n",
    "    w_dist_epoc = plotfig.wasserstein_approximate(loader_H_input_train_source, loader_H_input_train_target)\n",
    "    pad_metrics['w_dist']['before_training'] = w_dist_epoc\n",
    "    \n",
    "    # Calculate     PAD for original input training datasets with SVM\n",
    "    pad_svm = PAD.original_PAD(loader_H_input_train_source, loader_H_input_train_target)\n",
    "    print(f\"PAD = {pad_svm:.4f}\")\n",
    "    \n",
    "    # Calculate PCA_PAD for original input training datasets with PCA_SVM, PCA_LDA, PCA_LogReg\n",
    "    X_features_, y_features_ = PAD.extract_features_with_pca(loader_H_input_train_source, loader_H_input_train_target, pca_components=100)\n",
    "    pad_pca_svm_epoc = PAD.calc_pad_svm(X_features_, y_features_)\n",
    "    pad_pca_lda_epoc = PAD.calc_pad_lda(X_features_, y_features_)\n",
    "    pad_pca_logreg_epoc = PAD.calc_pad_logreg(X_features_, y_features_)\n",
    "    \n",
    "    pad_metrics['pad_pca_lda']['before_training'] = pad_pca_lda_epoc\n",
    "    pad_metrics['pad_pca_logreg']['before_training'] = pad_pca_logreg_epoc  \n",
    "    pad_metrics['pad_pca_svm']['before_training'] = pad_pca_svm_epoc\n",
    "    ## \n",
    "    \n",
    "    if not os.path.exists(os.path.dirname(model_path + '/' + sub_folder +'/')):\n",
    "        os.makedirs(os.path.dirname(model_path + '/' + sub_folder + '/'))   # Domain_Adversarial/model/_/ver_/{sub_folder}\n",
    "\n",
    "    #\n",
    "    train_metrics = {\n",
    "        'train_loss': [],           # total training loss \n",
    "        'train_est_loss': [],       # estimation loss\n",
    "        'train_disc_loss': [],      # discriminator loss\n",
    "        'train_domain_loss': [],    # JMMD loss (replaces domain loss)\n",
    "        'train_est_loss_target': [] # target estimation loss (monitoring)\n",
    "    }\n",
    "    \n",
    "    # \n",
    "    val_metrics = {\n",
    "        'val_loss': [],                 # total validation loss\n",
    "        'val_gan_disc_loss': [],        # GAN discriminator loss\n",
    "        'val_domain_disc_loss': [],     # JMMD loss (replaces domain discriminator)\n",
    "        'val_est_loss_source': [],      # source estimation loss\n",
    "        'val_est_loss_target': [],      # target estimation loss  \n",
    "        'val_est_loss': [],             # average estimation loss\n",
    "        'source_acc': [],               # source domain accuracy (placeholder for JMMD)\n",
    "        'target_acc': [],               # target domain accuracy (placeholder for JMMD)\n",
    "        'acc': [],                      # average accuracy (placeholder for JMMD)\n",
    "        'nmse_val_source': [],          # source NMSE\n",
    "        'nmse_val_target': [],          # target NMSE\n",
    "        'nmse_val': [],                  # average NMSE\n",
    "        'val_smoothness_loss': []\n",
    "    }\n",
    "    #\n",
    "    H_to_save = {}          # list to save to .mat file for H\n",
    "    perform_to_save = {}    # list to save to .mat file for nmse, losses,...\n",
    "\n",
    "    # \n",
    "    model = GAN(n_subc=312, gen_l2=None, disc_l2=1e-5)  # l2 regularization for generator and discriminator\n",
    "    # \n",
    "    gen_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4, beta_1=0.5, beta_2=0.9)\n",
    "    disc_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, beta_1=0.5, beta_2=0.9)  # WGAN-GP uses Adam optimizer with beta_1=0.5\n",
    "    # \n",
    "    ####\n",
    "    optimizer = [gen_optimizer, disc_optimizer]  # \n",
    "    ####\n",
    "    \n",
    "    flag = 1 # flag to plot and save H_true\n",
    "    epoc_pad = []    # epochs that calculating pad (return_features == True)\n",
    "    for epoch in range(n_epochs):\n",
    "        # ===================== Training =====================\n",
    "        loader_H_true_train_source.reset()\n",
    "        # loader_H_practical_train_source.reset()\n",
    "        loader_H_input_train_source.reset()\n",
    "        loader_H_true_train_target.reset()\n",
    "        # loader_H_practical_train_target.reset()\n",
    "        loader_H_input_train_target.reset()\n",
    "                \n",
    "        # loader_H = [loader_H_practical_train_source, loader_H_true_train_source, loader_H_practical_train_target, loader_H_true_train_target]\n",
    "        loader_H = [loader_H_input_train_source, loader_H_true_train_source, loader_H_input_train_target, loader_H_true_train_target]\n",
    "\n",
    "        # Only 2 loss functions needed for JMMD\n",
    "        loss_fn = [loss_fn_ce, loss_fn_bce]\n",
    "    \n",
    "        ##########################\n",
    "        if epoch in [int(n_epochs * r) for r in [0, 0.25, 0.5, 0.75]] or epoch == n_epochs-1:\n",
    "            # return_features == return features to calculate PAD\n",
    "            return_features = True\n",
    "            epoc_pad.append(epoch)\n",
    "        else:\n",
    "            return_features = False\n",
    "\n",
    "        ##########################\n",
    "        # \n",
    "        train_step_output = train_step_wgan_gp_source_only(model, loader_H, loss_fn, optimizer, lower_range=-1, \n",
    "                        save_features=return_features, weights=weights, linear_interp=linear_interp)\n",
    "\n",
    "        train_epoc_loss_est        = train_step_output.avg_epoc_loss_est\n",
    "        train_epoc_loss_d          = train_step_output.avg_epoc_loss_d\n",
    "        train_epoc_loss_domain     = train_step_output.avg_epoc_loss_domain  # Now contains JMMD loss\n",
    "        train_epoc_loss            = train_step_output.avg_epoc_loss\n",
    "        train_epoc_loss_est_target = train_step_output.avg_epoc_loss_est_target\n",
    "                # train_epoc_loss        = total train loss = loss_est + lambda_jmmd * jmmd_loss\n",
    "                # train_epoc_loss_est    = loss in estimation network in source domain (labels available)\n",
    "                # train_epoc_loss_domain = JMMD loss (statistical distribution matching)\n",
    "                # train_epoc_loss_est_target - just to monitor - the machine can not calculate because no label available in source domain\n",
    "                # All are already calculated in average over training dataset (source/target - respectively)\n",
    "        print(\"Time\", time.perf_counter() - start, \"seconds\")\n",
    "        # Note: No PAD calculation for source-only training since we're not doing domain adaptation\n",
    "            \n",
    "        # Calculate PAD for the extracted features\n",
    "        if return_features:\n",
    "            features_source_file = \"features_source.h5\"\n",
    "            features_target_file = \"features_target.h5\"\n",
    "            print(f\"epoch {epoch+1}/{n_epochs}\")\n",
    "            ## Calculate PCA_PAD for extracted features with PCA_SVM, PCA_LDA, PCA_LogReg\n",
    "            X_features, y_features = PAD.extract_features_with_pca(features_source_file, features_target_file, pca_components=100)\n",
    "            pad_svm_epoc = PAD.calc_pad_svm(X_features, y_features)\n",
    "            pad_lda_epoc = PAD.calc_pad_lda(X_features, y_features)\n",
    "            pad_logreg_epoc = PAD.calc_pad_logreg(X_features, y_features)\n",
    "            pad_metrics['pad_pca_svm'][f'epoch_{epoch+1}'] = pad_svm_epoc\n",
    "            pad_metrics['pad_pca_lda'][f'epoch_{epoch+1}'] = pad_lda_epoc\n",
    "            pad_metrics['pad_pca_logreg'][f'epoch_{epoch+1}'] = pad_logreg_epoc\n",
    "            \n",
    "            ## Distribution of extracted features\n",
    "            plotfig.plotHist(features_source_file, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name=f'source_epoch_{epoch+1}', percent=99)\n",
    "            plotfig.plotHist(features_target_file, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name=f'target_epoch_{epoch+1}', percent=99)\n",
    "            #\n",
    "            plotfig.plotHist(features_source_file, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name=f'source_epoch_{epoch+1}', percent=100)\n",
    "            plotfig.plotHist(features_target_file, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name=f'target_epoch_{epoch+1}', percent=100)\n",
    "            #\n",
    "            # Calculate Wasserstein-1 distance for extracted features\n",
    "            # print(\"Calculating Wasserstein-1 distance for extracted features ...\")\n",
    "            # w_dist_epoc = plotfig.wasserstein_approximate(features_source_file, features_target_file)\n",
    "            # w_dist.append(w_dist_epoc)\n",
    "            \n",
    "\n",
    "            if os.path.exists(features_source_file):\n",
    "                os.remove(features_source_file)\n",
    "            if os.path.exists(features_target_file):\n",
    "                os.remove(features_target_file)\n",
    "            print(\"Time\", time.perf_counter() - start, \"seconds\")\n",
    "        \n",
    "        # Average loss for the epoch\n",
    "        train_metrics['train_loss'].append(train_epoc_loss)\n",
    "        print(f\"epoch {epoch+1}/{n_epochs} Average Training Loss: {train_epoc_loss:.6f}\")\n",
    "        \n",
    "        train_metrics['train_est_loss'].append(train_epoc_loss_est)\n",
    "        print(f\"epoch {epoch+1}/{n_epochs} Average Estimation Loss (in Source domain): {train_epoc_loss_est:.6f}\")\n",
    "        \n",
    "        train_metrics['train_disc_loss'].append(train_epoc_loss_d)\n",
    "        print(f\"epoch {epoch+1}/{n_epochs} Average Disc Loss (in Source domain): {train_epoc_loss_d:.6f}\")\n",
    "        \n",
    "        train_metrics['train_domain_loss'].append(train_epoc_loss_domain)\n",
    "        print(f\"epoch {epoch+1}/{n_epochs} Source-only training (No domain adaptation): {train_epoc_loss_domain:.6f}\")  # Updated print message\n",
    "        \n",
    "        train_metrics['train_est_loss_target'].append(train_epoc_loss_est_target)\n",
    "        print(f\"epoch {epoch+1}/{n_epochs} Testing performance on Target domain: {train_epoc_loss_est_target:.6f}\")\n",
    "        \n",
    "        \n",
    "        # ===================== Evaluation =====================\n",
    "        loader_H_true_val_source.reset()\n",
    "        loader_H_input_val_source.reset()\n",
    "        loader_H_true_val_target.reset()\n",
    "        loader_H_input_val_target.reset()\n",
    "        loader_H_eval = [loader_H_input_val_source, loader_H_true_val_source, loader_H_input_val_target, loader_H_true_val_target]\n",
    "\n",
    "        # \n",
    "        loss_fn = [loss_fn_ce, loss_fn_bce]\n",
    "        \n",
    "        # eval_func = utils_UDA_FiLM.val_step\n",
    "        if (epoch==epoch_min) or (epoch+1>epoch_min and (epoch-epoch_min)%epoch_step==0) or epoch==n_epochs-1:\n",
    "            # \n",
    "            H_sample, epoc_val_return = val_step_wgan_gp_source_only(model, loader_H_eval, loss_fn, lower_range, \n",
    "                                            weights=weights, linear_interp=linear_interp)\n",
    "            visualize_H(H_sample, H_to_save, epoch, plotfig.figChan, flag, model_path, sub_folder, domain_weight=0.0)\n",
    "            flag = 0  # after the first epoch, no need to save H_true anymore\n",
    "            \n",
    "        else:\n",
    "            # \n",
    "            _, epoc_val_return = val_step_wgan_gp_source_only(model, loader_H_eval, loss_fn, lower_range, \n",
    "                                        weights=weights, linear_interp=linear_interp)\n",
    "        \n",
    "        post_val(epoc_val_return, epoch, n_epochs, val_metrics, domain_weight=0.0)\n",
    "        \n",
    "        if (epoch==epoch_min) or (epoch+1>epoch_min and (epoch-epoch_min)%epoch_step==0) or epoch==n_epochs-1:\n",
    "            # \n",
    "            all_metrics = {\n",
    "                'figLoss': plotfig.figLoss, \n",
    "                'savemat': savemat,\n",
    "                'pad_metrics': pad_metrics, \n",
    "                'epoc_pad': epoc_pad,\n",
    "                'pad_svm': pad_svm, \n",
    "                'weights': weights, \n",
    "                'optimizer': optimizer\n",
    "            }\n",
    "            # Combine all metrics\n",
    "            all_metrics.update(train_metrics)  # Add training metrics\n",
    "            all_metrics.update(val_metrics)    # Add validation metrics\n",
    "\n",
    "            save_checkpoint(model, save_model, model_path, sub_folder, epoch, all_metrics)\n",
    "    \n",
    "    # end of epoch loop\n",
    "    # =====================            \n",
    "    # Save performances\n",
    "    # Save H matrix\n",
    "    savemat(model_path + '/' + sub_folder + '/H_visualize/H_trix.mat', H_to_save)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
