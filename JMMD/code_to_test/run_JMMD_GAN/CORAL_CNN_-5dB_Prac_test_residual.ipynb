{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d6064d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-15 15:06:46.044822: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765829206.058871  115650 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765829206.063155  115650 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1765829206.074244  115650 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765829206.074256  115650 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765829206.074257  115650 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765829206.074259  115650 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-12-15 15:06:46.078204: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/thien/Code/NTN/Hest_NTN_UDA/JMMD/code_to_test/run_JMMD_GAN\n",
      "/home/thien/Code/NTN/Hest_NTN_UDA\n",
      "WeightScheduler initialized with reconstruction_first strategy:\n",
      "  - Domain weight: 0.01 → 1.5\n",
      "  - Est weight: 1.5 → 0.8\n",
      "  - Adv weight: 0.005 → 0.005\n",
      "  - Warmup epochs: 80\n",
      "  - Schedule type: linear\n",
      "N_samp_source =  2048\n",
      "N_samp_target =  2048\n",
      "train_size =  96\n",
      "val_size =  16\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy.io import savemat\n",
    "import h5py\n",
    "\n",
    "# Add the root project directory\n",
    "try:\n",
    "    code_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    project_root = os.path.abspath(os.path.join(code_dir, '..', '..', '..'))\n",
    "except NameError:\n",
    "    # Running in Jupyter Notebook\n",
    "    code_dir = os.getcwd()\n",
    "    project_root = os.path.abspath(os.path.join(code_dir, '..', '..', '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "print(code_dir)\n",
    "print(project_root) # Hest_NTN_UDA/\n",
    "\n",
    "from Domain_Adversarial.helper import loader, plotfig, PAD\n",
    "from Domain_Adversarial.helper.utils import H5BatchLoader\n",
    "from Domain_Adversarial.helper.utils_GAN import visualize_H\n",
    "from JMMD.helper.utils_GAN import save_checkpoint_jmmd as save_checkpoint\n",
    "from JMMD.helper.utils_GAN import WeightScheduler\n",
    "\n",
    "SNR = -5\n",
    "# source_data_file_path_label = os.path.abspath(os.path.join(code_dir, '..', 'generatedChan', 'OpenNTN','H_perfect.mat'))\n",
    "source_data_file_path = os.path.abspath(os.path.join(code_dir, '..', '..', '..', 'generatedChan', 'MATLAB', 'TDL_D_30_sim', f'SNR_{SNR}dB', 'matlabNTN.mat'))\n",
    "target_data_file_path = os.path.abspath(os.path.join(code_dir, '..', '..', '..', 'generatedChan', 'MATLAB', 'TDL_B_100_300_sim', f'SNR_{SNR}dB', 'matlabNTN.mat'))\n",
    "norm_approach = 'minmax' # can be set to 'std'\n",
    "lower_range = -1 \n",
    "    # if norm_approach = 'minmax': \n",
    "        # =  0 for scaling to  [0 1]\n",
    "        # = -1 for scaling to [-1 1]\n",
    "    # if norm_approach = 'std': can be any value, but need to be defined\n",
    "# weights = {\n",
    "#     # Core loss weights\n",
    "#     'adv_weight': 0.05,        # GAN adversarial loss weight\n",
    "#     'est_weight': 0.6,          # Estimation loss weight (main task)\n",
    "#     'domain_weight': 1.5,       # CORAL loss weight (domain adaptation)\n",
    "    \n",
    "#     # Smoothness regularization weights\n",
    "#     'temporal_weight': 0.02,    # Temporal smoothness penalty\n",
    "#     'frequency_weight': 0.1,    # Frequency smoothness penalty\n",
    "# }\n",
    "# print('adv_weight = ', weights['adv_weight'], ', est_weight = ', weights['est_weight'], ', domain_weight = ', weights['domain_weight'])\n",
    "\n",
    "scheduler = WeightScheduler(strategy='reconstruction_first', start_domain_weight=0.01, end_domain_weight=1.5,\n",
    "                            start_est_weight=1.5, end_est_weight=0.8, warmup_epochs=80) \n",
    "                            # adv_weight = 0.005 default\n",
    "                            # warmup_epochs=150 default\n",
    "                            # schedule_type = 'linear' default\n",
    "\n",
    "\n",
    "if norm_approach == 'minmax':\n",
    "    if lower_range == 0:\n",
    "        norm_txt = 'Using min-max [0 1]'\n",
    "    elif lower_range ==-1:\n",
    "        norm_txt = 'Using min-max [-1 1]'\n",
    "elif norm_approach == 'no':\n",
    "    norm_txt = 'No'\n",
    "    \n",
    "# Paths to save\n",
    "path_temp = code_dir + f'/results/'\n",
    "os.makedirs(os.path.dirname(path_temp), exist_ok=True)\n",
    "idx_save_path = loader.find_incremental_filename(path_temp,'ver', '_', '')\n",
    "\n",
    "save_model = False\n",
    "model_path = code_dir + f'/results/ver' + str(idx_save_path) + '_'\n",
    "# figure_path = code_dir + '/model/GAN/ver' + str(idx_save_path) + '_/figure'\n",
    "model_readme = model_path + '/readme.txt'\n",
    "\n",
    "batch_size= 8 # 16\n",
    "\n",
    "# ============ Source data ==============\n",
    "source_file = h5py.File(source_data_file_path, 'r')\n",
    "H_true_source = source_file['H_perfect']\n",
    "N_samp_source = H_true_source.shape[0]\n",
    "print('N_samp_source = ', N_samp_source)\n",
    "\n",
    "# ============ Target data ==============\n",
    "target_file = h5py.File(target_data_file_path, 'r')\n",
    "H_true_target = target_file['H_perfect']\n",
    "N_samp_target = H_true_target.shape[0]\n",
    "print('N_samp_target = ', N_samp_target)\n",
    "\n",
    "# Store random state \n",
    "rng_state = np.random.get_state()\n",
    "\n",
    "# --- Set a temporary seed for reproducible split ---\n",
    "np.random.seed(1234)   # any fixed integer seed\n",
    "# Random but repeatable split\n",
    "indices_source = np.arange(N_samp_source)\n",
    "np.random.shuffle(indices_source)\n",
    "indices_target = np.arange(N_samp_target)\n",
    "np.random.shuffle(indices_target)\n",
    "# Restore previous random state (so other code stays random)\n",
    "np.random.set_state(rng_state)\n",
    "#\n",
    "train_size = int(np.floor(N_samp_source * 0.9) // batch_size * batch_size)\n",
    "val_size = N_samp_source - train_size\n",
    "\n",
    "# Repeat the indices to match the maximum number of samples\n",
    "N_samp = max(N_samp_source, N_samp_target) \n",
    "indices_source = np.resize(indices_source, N_samp)\n",
    "indices_target = np.resize(indices_target, N_samp)\n",
    "\n",
    "# =======================================================\n",
    "## Divide the indices into training and validation sets\n",
    "# indices_train_source = indices_source[:train_size]\n",
    "# indices_val_source   = indices_source[train_size:train_size + val_size]\n",
    "\n",
    "# indices_train_target = indices_target[:train_size]\n",
    "# indices_val_target   = indices_target[train_size:train_size + val_size]\n",
    "\n",
    "# to test code\n",
    "indices_train_source = indices_source[:96]\n",
    "indices_val_source = indices_source[2032:]\n",
    "indices_train_target = indices_target[:96]\n",
    "indices_val_target = indices_target[2032:]\n",
    "\n",
    "print('train_size = ', indices_train_source.shape[0])\n",
    "print('val_size = ', indices_val_source.shape[0])\n",
    "\n",
    "class DataLoaders:\n",
    "    def __init__(self, file, indices_train, indices_val, tag='prac', batch_size=32): \n",
    "        # tag = 'prac' or 'li' or 'ls'\n",
    "        self.true_train = H5BatchLoader(file, dataset_name='H_perfect', batch_size=batch_size, shuffled_indices=indices_train)\n",
    "        self.true_val = H5BatchLoader(file, dataset_name='H_perfect', batch_size=batch_size, shuffled_indices=indices_val)\n",
    "\n",
    "        self.input_train = H5BatchLoader(file, f'H_{tag}', batch_size=batch_size, shuffled_indices=indices_train)\n",
    "        self.input_val = H5BatchLoader(file, f'H_{tag}', batch_size=batch_size, shuffled_indices=indices_val)\n",
    "\n",
    "# Source domain\n",
    "class_dict_source = {\n",
    "    'GAN_practical': DataLoaders(source_file, indices_train_source, indices_val_source, tag='prac', batch_size=batch_size),\n",
    "    'GAN_linear': DataLoaders(source_file, indices_train_source, indices_val_source, tag='li', batch_size=batch_size),\n",
    "    'GAN_ls': DataLoaders(source_file, indices_train_source, indices_val_source, tag='ls', batch_size=batch_size)\n",
    "}\n",
    "\n",
    "# Target domain\n",
    "class_dict_target = {\n",
    "    'GAN_practical': DataLoaders(target_file, indices_train_target, indices_val_target, tag='prac', batch_size=batch_size),\n",
    "    'GAN_linear': DataLoaders(target_file, indices_train_target, indices_val_target, tag='li', batch_size=batch_size),\n",
    "    'GAN_ls': DataLoaders(target_file, indices_train_target, indices_val_target, tag='ls', batch_size=batch_size)\n",
    "}\n",
    "\n",
    "loss_fn_ce = tf.keras.losses.MeanSquaredError()  # Channel estimation loss (generator loss)\n",
    "loss_fn_bce = tf.keras.losses.BinaryCrossentropy(from_logits=False) # Binary cross-entropy loss for discriminator\n",
    "\n",
    "from JMMD.helper.utils_GAN import CNNGenerator\n",
    "from JMMD.helper.utils_GAN import post_val, train_step_cnn_residual_coral, val_step_cnn_residual_coral\n",
    "\n",
    "import time\n",
    "start = time.perf_counter()\n",
    "\n",
    "# n_epochs= 300 # 300\n",
    "# epoch_min = 100\n",
    "# epoch_step = 20\n",
    "n_epochs= 5\n",
    "epoch_min = 0\n",
    "epoch_step = 1\n",
    "\n",
    "sub_folder_ = ['GAN_practical']  # ['GAN_linear', 'GAN_practical', 'GAN_ls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a13d8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: GAN_practical\n",
      "Epoch 1/5, Weights: {'adv_weight': 0.005, 'est_weight': 1.4982691637903556, 'domain_weight': 0.013684208503385697, 'temporal_weight': 0.02, 'frequency_weight': 0.1}\n",
      "MemoryEfficientCORALLoss initialized:\n",
      "  - Max features: 1024\n",
      "  - Global pooling: True\n",
      "  - GP weight: 0.7, DR weight: 0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1765829210.237758  115650 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9548 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:1a:00.0, compute capability: 7.5\n",
      "I0000 00:00:1765829210.238366  115650 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 9548 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:67:00.0, compute capability: 7.5\n",
      "I0000 00:00:1765829210.238856  115650 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 9486 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:68:00.0, compute capability: 7.5\n",
      "I0000 00:00:1765829213.023826  115650 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-15 15:06:56.557466: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.30GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-12-15 15:06:56.569203: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.05GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-12-15 15:06:57.071971: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.55GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-12-15 15:06:57.086872: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.05GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "    Residual norm (avg): 0.627582\n",
      "Time 15.606680525932461 seconds\n",
      "epoch 1/5 Average Training Loss: 4.213179\n",
      "epoch 1/5 Average Estimation Loss (in Source domain): 0.744336\n",
      "epoch 1/5 Average Disc Loss (in Source domain): 0.000000\n",
      "epoch 1/5 Average CORAL Loss: 225.600754\n",
      "epoch 1/5 For observation only - Average Estimation Loss in Target domain: 0.000000\n",
      "MemoryEfficientCORALLoss initialized:\n",
      "  - Max features: 1024\n",
      "  - Global pooling: True\n",
      "  - GP weight: 0.7, DR weight: 0.3\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "    Validation residual norm (avg): 0.426564\n",
      "epoch 1/5 (Val) Weighted Total Loss: 1.317063\n",
      "epoch 1/5 (Val) Average Estimation Loss (mean): 0.409809\n",
      "epoch 1/5 (Val) Average Estimation Loss (Source): 0.351727\n",
      "epoch 1/5 (Val) Average Estimation Loss (Target): 0.467891\n",
      "epoch 1/5 (Val) GAN Discriminator Loss: 0.000000\n",
      "epoch 1/5 (Val) JMMD Loss: 50.853226\n",
      "epoch 1/5 (Val) NMSE (Source): 0.726373, NMSE (Target): 0.663341, NMSE (Mean): 0.694857\n",
      "epoch 1/5 (Val) Domain Accuracy (Average): 0.5000\n",
      "epoch 1/5 (Val) Smoothness Loss: 0.007173\n",
      "Epoch 2/5, Weights: {'adv_weight': 0.005, 'est_weight': 1.481382104496194, 'domain_weight': 0.08066455103457426, 'temporal_weight': 0.02, 'frequency_weight': 0.1}\n",
      "MemoryEfficientCORALLoss initialized:\n",
      "  - Max features: 1024\n",
      "  - Global pooling: True\n",
      "  - GP weight: 0.7, DR weight: 0.3\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "    Residual norm (avg): 0.526958\n",
      "Time 24.999641066999175 seconds\n",
      "epoch 2/5 Average Training Loss: 2.537843\n",
      "epoch 2/5 Average Estimation Loss (in Source domain): 0.464840\n",
      "epoch 2/5 Average Disc Loss (in Source domain): 0.000000\n",
      "epoch 2/5 Average CORAL Loss: 22.853125\n",
      "epoch 2/5 For observation only - Average Estimation Loss in Target domain: 0.000000\n",
      "MemoryEfficientCORALLoss initialized:\n",
      "  - Max features: 1024\n",
      "  - Global pooling: True\n",
      "  - GP weight: 0.7, DR weight: 0.3\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "    Validation residual norm (avg): 0.265036\n",
      "epoch 2/5 (Val) Weighted Total Loss: 1.219995\n",
      "epoch 2/5 (Val) Average Estimation Loss (mean): 0.353125\n",
      "epoch 2/5 (Val) Average Estimation Loss (Source): 0.305332\n",
      "epoch 2/5 (Val) Average Estimation Loss (Target): 0.400919\n",
      "epoch 2/5 (Val) GAN Discriminator Loss: 0.000000\n",
      "epoch 2/5 (Val) JMMD Loss: 8.594863\n",
      "epoch 2/5 (Val) NMSE (Source): 0.488607, NMSE (Target): 0.524811, NMSE (Mean): 0.506709\n",
      "epoch 2/5 (Val) Domain Accuracy (Average): 0.5000\n",
      "epoch 2/5 (Val) Smoothness Loss: 0.003581\n",
      "Epoch 3/5, Weights: {'adv_weight': 0.005, 'est_weight': 1.3379673484493124, 'domain_weight': 0.755, 'temporal_weight': 0.02, 'frequency_weight': 0.1}\n",
      "MemoryEfficientCORALLoss initialized:\n",
      "  - Max features: 1024\n",
      "  - Global pooling: True\n",
      "  - GP weight: 0.7, DR weight: 0.3\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "    Residual norm (avg): 0.368377\n",
      "Time 33.11886259098537 seconds\n",
      "epoch 3/5 Average Training Loss: 6.233931\n",
      "epoch 3/5 Average Estimation Loss (in Source domain): 0.512444\n",
      "epoch 3/5 Average Disc Loss (in Source domain): 0.000000\n",
      "epoch 3/5 Average CORAL Loss: 7.344760\n",
      "epoch 3/5 For observation only - Average Estimation Loss in Target domain: 0.000000\n",
      "MemoryEfficientCORALLoss initialized:\n",
      "  - Max features: 1024\n",
      "  - Global pooling: True\n",
      "  - GP weight: 0.7, DR weight: 0.3\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "    Validation residual norm (avg): 0.129630\n",
      "epoch 3/5 (Val) Weighted Total Loss: 2.097728\n",
      "epoch 3/5 (Val) Average Estimation Loss (mean): 0.415842\n",
      "epoch 3/5 (Val) Average Estimation Loss (Source): 0.387533\n",
      "epoch 3/5 (Val) Average Estimation Loss (Target): 0.444151\n",
      "epoch 3/5 (Val) GAN Discriminator Loss: 0.000000\n",
      "epoch 3/5 (Val) JMMD Loss: 2.038796\n",
      "epoch 3/5 (Val) NMSE (Source): 0.450117, NMSE (Target): 0.537834, NMSE (Mean): 0.493976\n",
      "epoch 3/5 (Val) Domain Accuracy (Average): 0.5000\n",
      "epoch 3/5 (Val) Smoothness Loss: 0.002055\n",
      "Epoch 4/5, Weights: {'adv_weight': 0.005, 'est_weight': 0.9620326515506877, 'domain_weight': 1.4293354489654255, 'temporal_weight': 0.02, 'frequency_weight': 0.1}\n",
      "MemoryEfficientCORALLoss initialized:\n",
      "  - Max features: 1024\n",
      "  - Global pooling: True\n",
      "  - GP weight: 0.7, DR weight: 0.3\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "    Residual norm (avg): 0.291087\n",
      "Time 41.243235375965014 seconds\n",
      "epoch 4/5 Average Training Loss: 2.540402\n",
      "epoch 4/5 Average Estimation Loss (in Source domain): 0.497455\n",
      "epoch 4/5 Average Disc Loss (in Source domain): 0.000000\n",
      "epoch 4/5 Average CORAL Loss: 1.441170\n",
      "epoch 4/5 For observation only - Average Estimation Loss in Target domain: 0.000000\n",
      "MemoryEfficientCORALLoss initialized:\n",
      "  - Max features: 1024\n",
      "  - Global pooling: True\n",
      "  - GP weight: 0.7, DR weight: 0.3\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "    Validation residual norm (avg): 0.101693\n",
      "epoch 4/5 (Val) Weighted Total Loss: 1.593725\n",
      "epoch 4/5 (Val) Average Estimation Loss (mean): 0.416029\n",
      "epoch 4/5 (Val) Average Estimation Loss (Source): 0.387034\n",
      "epoch 4/5 (Val) Average Estimation Loss (Target): 0.445023\n",
      "epoch 4/5 (Val) GAN Discriminator Loss: 0.000000\n",
      "epoch 4/5 (Val) JMMD Loss: 0.833997\n",
      "epoch 4/5 (Val) NMSE (Source): 0.406061, NMSE (Target): 0.528379, NMSE (Mean): 0.467220\n",
      "epoch 4/5 (Val) Domain Accuracy (Average): 0.5000\n",
      "epoch 4/5 (Val) Smoothness Loss: 0.001431\n",
      "Epoch 5/5, Weights: {'adv_weight': 0.005, 'est_weight': 0.8186178955038063, 'domain_weight': 1.496315791496614, 'temporal_weight': 0.02, 'frequency_weight': 0.1}\n",
      "MemoryEfficientCORALLoss initialized:\n",
      "  - Max features: 1024\n",
      "  - Global pooling: True\n",
      "  - GP weight: 0.7, DR weight: 0.3\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "    Residual norm (avg): 0.277051\n",
      "Time 49.42532355396543 seconds\n",
      "epoch 5/5 Average Training Loss: 1.359374\n",
      "epoch 5/5 Average Estimation Loss (in Source domain): 0.463845\n",
      "epoch 5/5 Average Disc Loss (in Source domain): 0.000000\n",
      "epoch 5/5 Average CORAL Loss: 0.653720\n",
      "epoch 5/5 For observation only - Average Estimation Loss in Target domain: 0.000000\n",
      "MemoryEfficientCORALLoss initialized:\n",
      "  - Max features: 1024\n",
      "  - Global pooling: True\n",
      "  - GP weight: 0.7, DR weight: 0.3\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "  Processing layer 1: (8, 236544)\n",
      "    Dense reduction branch: (8, 236544) → (8, 1024)\n",
      "  Processing layer 2: (8, 118272)\n",
      "    Dense reduction branch: (8, 118272) → (8, 1024)\n",
      "    Validation residual norm (avg): 0.100836\n",
      "epoch 5/5 (Val) Weighted Total Loss: 1.083967\n",
      "epoch 5/5 (Val) Average Estimation Loss (mean): 0.385848\n",
      "epoch 5/5 (Val) Average Estimation Loss (Source): 0.357337\n",
      "epoch 5/5 (Val) Average Estimation Loss (Target): 0.414359\n",
      "epoch 5/5 (Val) GAN Discriminator Loss: 0.000000\n",
      "epoch 5/5 (Val) JMMD Loss: 0.512494\n",
      "epoch 5/5 (Val) NMSE (Source): 0.358827, NMSE (Target): 0.488050, NMSE (Mean): 0.423438\n",
      "epoch 5/5 (Val) Domain Accuracy (Average): 0.5000\n",
      "epoch 5/5 (Val) Smoothness Loss: 0.001253\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for sub_folder in sub_folder_:\n",
    "    print(f\"Processing: {sub_folder}\")\n",
    "    pad_metrics = {\n",
    "        'pad_pca_lda': {},      # Dictionary to store LDA PAD values by epoch\n",
    "        'pad_pca_logreg': {},   # Dictionary to store LogReg PAD values by epoch\n",
    "        'pad_pca_svm': {},      # Dictionary to store SVM PAD values by epoch\n",
    "        'w_dist': {}            # Dictionary to store Wasserstein distances by epoch\n",
    "    }\n",
    "    linear_interp = False\n",
    "    # if sub_folder == 'GAN_linear':\n",
    "    #     linear_interp =True # flag to clip values that go beyond the estimated pilot (min, max)\n",
    "    ##\n",
    "    loader_H_true_train_source = class_dict_source[sub_folder].true_train\n",
    "    loader_H_input_train_source = class_dict_source[sub_folder].input_train\n",
    "    loader_H_true_val_source = class_dict_source[sub_folder].true_val\n",
    "    loader_H_input_val_source = class_dict_source[sub_folder].input_val\n",
    "    \n",
    "    loader_H_true_train_target = class_dict_target[sub_folder].true_train\n",
    "    loader_H_input_train_target = class_dict_target[sub_folder].input_train\n",
    "    loader_H_true_val_target = class_dict_target[sub_folder].true_val\n",
    "    loader_H_input_val_target = class_dict_target[sub_folder].input_val\n",
    "    ##\n",
    "    \n",
    "    # Distribution of original input training datasets (or before training)    \n",
    "    # plotfig.plotHist(loader_H_input_train_source, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name='source_beforeTrain', percent=100)\n",
    "    # plotfig.plotHist(loader_H_input_train_target, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name='target_beforeTrain', percent=100)\n",
    "    \n",
    "    # plotfig.plotHist(loader_H_input_train_source, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name='source_beforeTrain', percent=99)\n",
    "    # plotfig.plotHist(loader_H_input_train_target, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name='target_beforeTrain', percent=99)\n",
    "    \n",
    "    # plotfig.plotHist(loader_H_input_train_source, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name='source_beforeTrain', percent=95)\n",
    "    # plotfig.plotHist(loader_H_input_train_target, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name='target_beforeTrain', percent=95)\n",
    "\n",
    "    # plotfig.plotHist(loader_H_input_train_source, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name='source_beforeTrain', percent=90)\n",
    "    # plotfig.plotHist(loader_H_input_train_target, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name='target_beforeTrain', percent=90)\n",
    "\n",
    "    # Calculate Wasserstein-1 distance for original input training datasets (before training)\n",
    "    # print(\"Calculating Wasserstein-1 distance for original input training datasets (before training)...\")\n",
    "    # w_dist_epoc = plotfig.wasserstein_approximate(loader_H_input_train_source, loader_H_input_train_target)\n",
    "    # pad_metrics['w_dist']['before_training'] = w_dist_epoc\n",
    "    \n",
    "    # # Calculate     PAD for original input training datasets with SVM\n",
    "    # pad_svm = PAD.original_PAD(loader_H_input_train_source, loader_H_input_train_target)\n",
    "    # print(f\"PAD = {pad_svm:.4f}\")\n",
    "    \n",
    "    # # Calculate PCA_PAD for original input training datasets with PCA_SVM, PCA_LDA, PCA_LogReg\n",
    "    # X_features_, y_features_ = PAD.extract_features_with_pca(loader_H_input_train_source, loader_H_input_train_target, pca_components=100)\n",
    "    # pad_pca_svm_epoc = PAD.calc_pad_svm(X_features_, y_features_)\n",
    "    # pad_pca_lda_epoc = PAD.calc_pad_lda(X_features_, y_features_)\n",
    "    # pad_pca_logreg_epoc = PAD.calc_pad_logreg(X_features_, y_features_)\n",
    "    \n",
    "    # pad_metrics['pad_pca_lda']['before_training'] = pad_pca_lda_epoc\n",
    "    # pad_metrics['pad_pca_logreg']['before_training'] = pad_pca_logreg_epoc  \n",
    "    # pad_metrics['pad_pca_svm']['before_training'] = pad_pca_svm_epoc\n",
    "    ## \n",
    "    \n",
    "    if not os.path.exists(os.path.dirname(model_path + '/' + sub_folder +'/')):\n",
    "        os.makedirs(os.path.dirname(model_path + '/' + sub_folder + '/'))   # Domain_Adversarial/model/_/ver_/{sub_folder}\n",
    "\n",
    "    #\n",
    "    train_metrics = {\n",
    "        'train_loss': [],           # total training loss \n",
    "        'train_est_loss': [],       # estimation loss\n",
    "        'train_disc_loss': [],      # discriminator loss\n",
    "        'train_domain_loss': [],    # CORAL loss (replaces domain loss)\n",
    "        'train_est_loss_target': [] # target estimation loss (monitoring)\n",
    "    }\n",
    "    \n",
    "    # \n",
    "    val_metrics = {\n",
    "        'val_loss': [],                 # total validation loss\n",
    "        'val_gan_disc_loss': [],        # GAN discriminator loss\n",
    "        'val_domain_disc_loss': [],     # CORAL loss (replaces domain discriminator)\n",
    "        'val_est_loss_source': [],      # source estimation loss\n",
    "        'val_est_loss_target': [],      # target estimation loss  \n",
    "        'val_est_loss': [],             # average estimation loss\n",
    "        'source_acc': [],               # source domain accuracy (placeholder for CORAL)\n",
    "        'target_acc': [],               # target domain accuracy (placeholder for CORAL)\n",
    "        'acc': [],                      # average accuracy (placeholder for CORAL)\n",
    "        'nmse_val_source': [],          # source NMSE\n",
    "        'nmse_val_target': [],          # target NMSE\n",
    "        'nmse_val': [],                  # average NMSE\n",
    "        'val_smoothness_loss': []\n",
    "    }\n",
    "    #\n",
    "    H_to_save = {}          # list to save to .mat file for H\n",
    "    perform_to_save = {}    # list to save to .mat file for nmse, losses,...\n",
    "\n",
    "    # \n",
    "    model = CNNGenerator(extract_layers=['block_5','block_6'])\n",
    "    # \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4, beta_1=0.5, beta_2=0.9)\n",
    "    # \n",
    "    \n",
    "    flag = 1 # flag to plot and save H_true\n",
    "    epoc_pad = []    # epochs that calculating pad (return_features == True)\n",
    "    for epoch in range(n_epochs):\n",
    "        # get weights \n",
    "        weights = scheduler.get_weights_domain_first_smooth(epoch, n_epochs)\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}, Weights: {weights}\")\n",
    "        \n",
    "        # ===================== Training =====================\n",
    "        loader_H_true_train_source.reset()\n",
    "        # loader_H_practical_train_source.reset()\n",
    "        loader_H_input_train_source.reset()\n",
    "        loader_H_true_train_target.reset()\n",
    "        # loader_H_practical_train_target.reset()\n",
    "        loader_H_input_train_target.reset()\n",
    "                \n",
    "        # loader_H = [loader_H_practical_train_source, loader_H_true_train_source, loader_H_practical_train_target, loader_H_true_train_target]\n",
    "        loader_H = [loader_H_input_train_source, loader_H_true_train_source, loader_H_input_train_target, loader_H_true_train_target]\n",
    "\n",
    "        # Only 2 loss functions needed for JMMD\n",
    "        loss_fn = [loss_fn_ce, loss_fn_bce]\n",
    "    \n",
    "        ##########################\n",
    "        # if epoch==0 or epoch == n_epochs-1:\n",
    "        #     # return_features == return features to calculate PAD\n",
    "        #     return_features = True\n",
    "        #     epoc_pad.append(epoch)\n",
    "        # else:\n",
    "        #     return_features = False\n",
    "\n",
    "        ##########################\n",
    "        # \n",
    "        train_step_output = train_step_cnn_residual_coral(model, loader_H, loss_fn, optimizer, lower_range=-1, \n",
    "                        save_features=False, weights=weights, linear_interp=linear_interp)\n",
    "\n",
    "        train_epoc_loss_est        = train_step_output.avg_epoc_loss_est\n",
    "        train_epoc_loss_d          = train_step_output.avg_epoc_loss_d\n",
    "        train_epoc_loss_domain     = train_step_output.avg_epoc_loss_domain  # Now contains CORAL loss\n",
    "        train_epoc_loss            = train_step_output.avg_epoc_loss\n",
    "        train_epoc_loss_est_target = train_step_output.avg_epoc_loss_est_target\n",
    "                # train_epoc_loss        = total train loss = loss_est + lambda_coral * coral_loss\n",
    "                # train_epoc_loss_est    = loss in estimation network in source domain (labels available)\n",
    "                # train_epoc_loss_domain = JMMD loss (statistical distribution matching)\n",
    "                # train_epoc_loss_est_target - just to monitor - the machine can not calculate because no label available in source domain\n",
    "                # All are already calculated in average over training dataset (source/target - respectively)\n",
    "        print(\"Time\", time.perf_counter() - start, \"seconds\")\n",
    "        # Calculate PAD for the extracted features\n",
    "        # if return_features and (weights['domain_weight']!=0) and (epoch==0 or epoch == n_epochs-1):\n",
    "        #     features_source_file = \"features_source.h5\"\n",
    "        #     features_target_file = \"features_target.h5\"\n",
    "        #     print(f\"epoch {epoch+1}/{n_epochs}\")\n",
    "        #     ## Calculate PCA_PAD for extracted features with PCA_SVM, PCA_LDA, PCA_LogReg\n",
    "        #     X_features, y_features = PAD.extract_features_with_pca(features_source_file, features_target_file, pca_components=100)\n",
    "        #     pad_svm_epoc = PAD.calc_pad_svm(X_features, y_features)\n",
    "        #     pad_lda_epoc = PAD.calc_pad_lda(X_features, y_features)\n",
    "        #     pad_logreg_epoc = PAD.calc_pad_logreg(X_features, y_features)\n",
    "        #     pad_metrics['pad_pca_svm'][f'epoch_{epoch+1}'] = pad_svm_epoc\n",
    "        #     pad_metrics['pad_pca_lda'][f'epoch_{epoch+1}'] = pad_lda_epoc\n",
    "        #     pad_metrics['pad_pca_logreg'][f'epoch_{epoch+1}'] = pad_logreg_epoc\n",
    "            \n",
    "        #     ## Distribution of extracted features\n",
    "        #     plotfig.plotHist(features_source_file, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name=f'source_epoch_{epoch+1}', percent=99)\n",
    "        #     plotfig.plotHist(features_target_file, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name=f'target_epoch_{epoch+1}', percent=99)\n",
    "        #     #\n",
    "        #     plotfig.plotHist(features_source_file, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name=f'source_epoch_{epoch+1}', percent=100)\n",
    "        #     plotfig.plotHist(features_target_file, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name=f'target_epoch_{epoch+1}', percent=100)\n",
    "        #     #\n",
    "        #     # Calculate Wasserstein-1 distance for extracted features\n",
    "        #     # print(\"Calculating Wasserstein-1 distance for extracted features ...\")\n",
    "        #     # w_dist_epoc = plotfig.wasserstein_approximate(features_source_file, features_target_file)\n",
    "        #     # w_dist.append(w_dist_epoc)\n",
    "            \n",
    "\n",
    "        #     if os.path.exists(features_source_file):\n",
    "        #         os.remove(features_source_file)\n",
    "        #     if os.path.exists(features_target_file):\n",
    "        #         os.remove(features_target_file)\n",
    "        #     print(\"Time\", time.perf_counter() - start, \"seconds\")\n",
    "            \n",
    "        \n",
    "        # Average loss for the epoch\n",
    "        train_metrics['train_loss'].append(train_epoc_loss)\n",
    "        print(f\"epoch {epoch+1}/{n_epochs} Average Training Loss: {train_epoc_loss:.6f}\")\n",
    "        \n",
    "        train_metrics['train_est_loss'].append(train_epoc_loss_est)\n",
    "        print(f\"epoch {epoch+1}/{n_epochs} Average Estimation Loss (in Source domain): {train_epoc_loss_est:.6f}\")\n",
    "        \n",
    "        train_metrics['train_disc_loss'].append(train_epoc_loss_d)\n",
    "        print(f\"epoch {epoch+1}/{n_epochs} Average Disc Loss (in Source domain): {train_epoc_loss_d:.6f}\")\n",
    "        \n",
    "        train_metrics['train_domain_loss'].append(train_epoc_loss_domain)\n",
    "        print(f\"epoch {epoch+1}/{n_epochs} Average CORAL Loss: {train_epoc_loss_domain:.6f}\")  # Updated print message\n",
    "        \n",
    "        train_metrics['train_est_loss_target'].append(train_epoc_loss_est_target)\n",
    "        print(f\"epoch {epoch+1}/{n_epochs} For observation only - Average Estimation Loss in Target domain: {train_epoc_loss_est_target:.6f}\")\n",
    "        \n",
    "        \n",
    "        # ===================== Evaluation =====================\n",
    "        loader_H_true_val_source.reset()\n",
    "        loader_H_input_val_source.reset()\n",
    "        loader_H_true_val_target.reset()\n",
    "        loader_H_input_val_target.reset()\n",
    "        loader_H_eval = [loader_H_input_val_source, loader_H_true_val_source, loader_H_input_val_target, loader_H_true_val_target]\n",
    "\n",
    "        # \n",
    "        loss_fn = [loss_fn_ce, loss_fn_bce]\n",
    "        \n",
    "        # eval_func = utils_UDA_FiLM.val_step\n",
    "        if (epoch==epoch_min) or (epoch+1>epoch_min and (epoch-epoch_min)%epoch_step==0) and epoch!=n_epochs-1:\n",
    "            # \n",
    "            H_sample, epoc_val_return = val_step_cnn_residual_coral(model, loader_H_eval, loss_fn, lower_range, \n",
    "                                            weights=weights, linear_interp=linear_interp)\n",
    "            visualize_H(H_sample, H_to_save, epoch, plotfig.figChan, flag, model_path, sub_folder, domain_weight=weights['domain_weight'])\n",
    "            flag = 0  # after the first epoch, no need to save H_true anymore\n",
    "        elif epoch==n_epochs-1:\n",
    "            _, epoc_val_return, H_val_gen = val_step_cnn_residual_coral(model, loader_H_eval, loss_fn, lower_range, \n",
    "                                            weights=weights, linear_interp=linear_interp, return_H_gen=True)    \n",
    "        else:\n",
    "            # \n",
    "            _, epoc_val_return = val_step_cnn_residual_coral(model, loader_H_eval, loss_fn, lower_range, \n",
    "                                        weights=weights, linear_interp=linear_interp)\n",
    "        \n",
    "        post_val(epoc_val_return, epoch, n_epochs, val_metrics, domain_weight=weights['domain_weight'])\n",
    "        \n",
    "        if (epoch==epoch_min) or (epoch+1>epoch_min and (epoch-epoch_min)%epoch_step==0) or epoch==n_epochs-1:\n",
    "            # \n",
    "            all_metrics = {\n",
    "                'figLoss': plotfig.figLoss, \n",
    "                'savemat': savemat,\n",
    "                # 'pad_metrics': pad_metrics, \n",
    "                # 'epoc_pad': epoc_pad,\n",
    "                # 'pad_svm': pad_svm, \n",
    "                'weights': weights, \n",
    "                'optimizer': optimizer\n",
    "            }\n",
    "            # Combine all metrics\n",
    "            all_metrics.update(train_metrics)  # Add training metrics\n",
    "            all_metrics.update(val_metrics)    # Add validation metrics\n",
    "\n",
    "            save_checkpoint(model, save_model, model_path, sub_folder, epoch, all_metrics)\n",
    "    \n",
    "    # end of epoch loop\n",
    "    # =====================            \n",
    "    # Save performances\n",
    "    # Save H matrix\n",
    "    savemat(model_path + '/' + sub_folder + '/H_visualize/H_trix.mat', H_to_save)\n",
    "    savemat(model_path + '/' + sub_folder + '/H_visualize/H_val_generated.mat', \n",
    "        {'H_val_gen': H_val_gen,\n",
    "        'indices_val_source': indices_val_source,\n",
    "        'indices_val_target': indices_val_target})\n",
    "# end of trainmode  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
