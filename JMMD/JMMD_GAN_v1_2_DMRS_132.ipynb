{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "465b0cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy.io import savemat\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# import utils\n",
    "# import loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5df69823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/thien/Code/NTN/Hest_NTN_UDA/JMMD\n",
      "/home/thien/Code/NTN/Hest_NTN_UDA\n"
     ]
    }
   ],
   "source": [
    "# Add the root project directory\n",
    "try:\n",
    "    base_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    project_root = os.path.abspath(os.path.join(base_dir, '..', '..'))\n",
    "except NameError:\n",
    "    # Running in Jupyter Notebook\n",
    "    code_dir = os.getcwd()\n",
    "    project_root = os.path.abspath(os.path.join(code_dir, '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "print(code_dir)\n",
    "print(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "403ed309",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Domain_Adversarial.helper import loader, plotfig, PAD\n",
    "from Domain_Adversarial.helper.utils import H5BatchLoader\n",
    "from Domain_Adversarial.helper.utils_GAN import visualize_H, post_val\n",
    "from helper.utils_GAN import save_checkpoint_ as save_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ed0a4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8588be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a (16, 792, 14, 2) matrix with random values\n",
    "# random_matrix = np.random.randn(16, 132, 14, 2)\n",
    "# random_matrix.shape\n",
    "\n",
    "# GAN_model = utils_GAN.GAN(n_subc=132)\n",
    "# out_put = GAN_model(random_matrix)\n",
    "# print(out_put.gen_out.shape)\n",
    "# print(out_put.disc_out.shape)\n",
    "# print(out_put.extracted_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f90e823",
   "metadata": {},
   "outputs": [],
   "source": [
    "SNR = 0\n",
    "# source_data_file_path_label = os.path.abspath(os.path.join(code_dir, '..', 'generatedChan', 'OpenNTN','H_perfect.mat'))\n",
    "source_data_file_path = os.path.abspath(os.path.join(code_dir, '..', 'generatedChan', 'MATLAB', 'tdlA_tdlC', 'tdlA', 'matlabNTN.mat'))\n",
    "target_data_file_path = os.path.abspath(os.path.join(code_dir, '..', 'generatedChan', 'MATLAB', 'tdlA_tdlC', 'tdlC', 'matlabNTN.mat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6803a541",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_approach = 'minmax' # can be set to 'std'\n",
    "lower_range = -1 \n",
    "    # if norm_approach = 'minmax': \n",
    "        # =  0 for scaling to  [0 1]\n",
    "        # = -1 for scaling to [-1 1]\n",
    "    # if norm_approach = 'std': can be any value, but need to be defined\n",
    "adv_weight=0.005\n",
    "est_weight=1\n",
    "domain_weight=0.5  # for Domain Discriminator, 0 for no Domain Discriminator\n",
    "\n",
    "# snr_start = -25\n",
    "# snr_step = 5\n",
    "# snr_end = 25\n",
    "# SNR = np.arange(snr_start, snr_end+1, snr_step)\n",
    "\n",
    "# SNR = np.array([0])\n",
    "\n",
    "# if len(SNR) >1:\n",
    "#     SNR_txt = f'{snr_start}:{snr_step}:{snr_end}'\n",
    "# else:\n",
    "#     SNR_txt = f'{SNR[0]}'\n",
    "    \n",
    "# ============ CNN settings ==============\n",
    "if norm_approach == 'minmax':\n",
    "    if lower_range == 0:\n",
    "        norm_txt = 'Using min-max [0 1]'\n",
    "    elif lower_range ==-1:\n",
    "        norm_txt = 'Using min-max [-1 1]'\n",
    "elif norm_approach == 'no':\n",
    "    norm_txt = 'No'\n",
    "    \n",
    "CNN_activation = 'Tanh'\n",
    "CNN_DropOut = 0.2\n",
    "if CNN_DropOut != 0:\n",
    "    dropOut_txt = f'Add p={CNN_DropOut} DropOut'\n",
    "    \n",
    "# ============ Adversarial for Domain Discriminator settings ==============\n",
    "    \n",
    "    \n",
    "# # create readme.txt file\n",
    "# content = f\"\"\"Generated by file 'Domain_Adversarial/UDA_CNN_v3.ipynb'.\n",
    "# 28 GHz fc,\n",
    "# Source dataset got from {source_data_file_path},\n",
    "# Target dataset got from {target_data_file_path},\n",
    "# Learning rate {learning_rate},\n",
    "# {norm_txt} scaler for each sample\n",
    "# Using {CNN_activation} as activation function of CNN\n",
    "# {dropOut_txt}\n",
    "# ========= For Domain Discriminator ==========\n",
    "# Extract features after layer {extract_layer} of CNN\n",
    "# {text_lambda}\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73be6772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to save\n",
    "idx_save_path = loader.find_incremental_filename(code_dir + '/model/GAN','ver', '_', '')\n",
    "\n",
    "save_model = 0\n",
    "model_path = code_dir + '/model/GAN/ver' + str(idx_save_path) + '_'\n",
    "# figure_path = code_dir + '/model/GAN/ver' + str(idx_save_path) + '_/figure'\n",
    "model_readme = model_path + '/readme.txt'\n",
    "# figure_readme = figure_path + '/readme.txt'\n",
    "\n",
    "# if not os.path.exists(os.path.dirname(model_readme)):\n",
    "#     os.makedirs(os.path.dirname(model_readme))\n",
    "# # if not os.path.exists(os.path.dirname(figure_readme)):\n",
    "# #     os.makedirs(os.path.dirname(figure_readme))\n",
    "\n",
    "# # Open the file in write mode ('w'). If the file does not exist, it will be created.\n",
    "# with open(model_readme, 'w') as file:\n",
    "#     # Write the content to the file\n",
    "#     file.write(content)\n",
    "\n",
    "# # with open(figure_readme, 'w') as file:\n",
    "# #     # Write the content to the file\n",
    "# #     file.write(content)\n",
    "\n",
    "# print(f\"File '{model_readme}'  created and content written.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0fbd42",
   "metadata": {},
   "source": [
    "## Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6bc85339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_samp_source =  2048\n",
      "N_samp_target =  2048\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "batch_size=16\n",
    "\n",
    "# ============ Source data ==============\n",
    "source_file = h5py.File(source_data_file_path, 'r')\n",
    "H_true_source = source_file['H_perfect']\n",
    "N_samp_source = H_true_source.shape[0]\n",
    "print('N_samp_source = ', N_samp_source)\n",
    "\n",
    "# ============ Target data ==============\n",
    "target_file = h5py.File(target_data_file_path, 'r')\n",
    "H_true_target = target_file['H_perfect']\n",
    "N_samp_target = H_true_target.shape[0]\n",
    "print('N_samp_target = ', N_samp_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4247fd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_size =  96\n",
      "val_size =  16\n"
     ]
    }
   ],
   "source": [
    "# Store random state \n",
    "rng_state = np.random.get_state()\n",
    "\n",
    "# --- Set a temporary seed for reproducible split ---\n",
    "np.random.seed(1234)   # any fixed integer seed\n",
    "# Random but repeatable split\n",
    "indices_source = np.arange(N_samp_source)\n",
    "np.random.shuffle(indices_source)\n",
    "indices_target = np.arange(N_samp_target)\n",
    "np.random.shuffle(indices_target)\n",
    "# Restore previous random state (so other code stays random)\n",
    "np.random.set_state(rng_state)\n",
    "#\n",
    "train_size = int(np.floor(N_samp_source * 0.9) // batch_size * batch_size)\n",
    "val_size = N_samp_source - train_size\n",
    "\n",
    "# Repeat the indices to match the maximum number of samples\n",
    "N_samp = max(N_samp_source, N_samp_target) \n",
    "indices_source = np.resize(indices_source, N_samp)\n",
    "indices_target = np.resize(indices_target, N_samp)\n",
    "\n",
    "# =======================================================\n",
    "## Divide the indices into training and validation sets\n",
    "# indices_train_source = indices_source[:train_size]\n",
    "# indices_val_source   = indices_source[train_size:train_size + val_size]\n",
    "\n",
    "# indices_train_target = indices_target[:train_size]\n",
    "# indices_val_target   = indices_target[train_size:train_size + val_size]\n",
    "\n",
    "# to test code\n",
    "indices_train_source = indices_source[:96]\n",
    "indices_val_source = indices_source[2032:]\n",
    "indices_train_target = indices_target[:96]\n",
    "indices_val_target = indices_target[2032:]\n",
    "\n",
    "print('train_size = ', indices_train_source.shape[0])\n",
    "print('val_size = ', indices_val_source.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65452f39",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2bdf7c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaders:\n",
    "    def __init__(self, file, indices_train, indices_val, tag='prac', batch_size=32): \n",
    "        # tag = 'prac' or 'li' or 'ls'\n",
    "        self.true_train = H5BatchLoader(file, dataset_name='H_perfect', batch_size=batch_size, shuffled_indices=indices_train)\n",
    "        self.true_val = H5BatchLoader(file, dataset_name='H_perfect', batch_size=batch_size, shuffled_indices=indices_val)\n",
    "\n",
    "        self.input_train = H5BatchLoader(file, f'H_{tag}', batch_size=batch_size, shuffled_indices=indices_train)\n",
    "        self.input_val = H5BatchLoader(file, f'H_{tag}', batch_size=batch_size, shuffled_indices=indices_val)\n",
    "\n",
    "# Source domain\n",
    "class_dict_source = {\n",
    "    'GAN_practical': DataLoaders(source_file, indices_train_source, indices_val_source, tag='prac', batch_size=batch_size),\n",
    "    'GAN_linear': DataLoaders(source_file, indices_train_source, indices_val_source, tag='li', batch_size=batch_size),\n",
    "    'GAN_ls': DataLoaders(source_file, indices_train_source, indices_val_source, tag='ls', batch_size=batch_size)\n",
    "}\n",
    "\n",
    "# Target domain\n",
    "class_dict_target = {\n",
    "    'GAN_practical': DataLoaders(target_file, indices_train_target, indices_val_target, tag='prac', batch_size=batch_size),\n",
    "    'GAN_linear': DataLoaders(target_file, indices_train_target, indices_val_target, tag='li', batch_size=batch_size),\n",
    "    'GAN_ls': DataLoaders(target_file, indices_train_target, indices_val_target, tag='ls', batch_size=batch_size)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "363fec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn_ce = tf.keras.losses.MeanSquaredError()  # Channel estimation loss (generator loss)\n",
    "loss_fn_bce = tf.keras.losses.BinaryCrossentropy(from_logits=False) # Binary cross-entropy loss for discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7cb63f",
   "metadata": {},
   "source": [
    "# Load  Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70c2a627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper.utils_GAN import GAN\n",
    "from helper.utils_GAN import train_step_wgan_gp_jmmd, val_step_wgan_gp_jmmd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53176091",
   "metadata": {},
   "source": [
    "# Running Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e1d67064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: GAN_practical\n",
      "Calculating Wasserstein-1 distance for original input training datasets (before training)...\n",
      "X shape =  (192, 528)\n",
      "X1 shape =  (96, 528) y1 shape =  (96,)\n",
      "(96, 528) (96,)\n",
      "C: 0.01, Error rate: 0.5208\n",
      "C: 0.1, Error rate: 0.3021\n",
      "C: 0.5, Error rate: 0.1458\n",
      "C: 1.0, Error rate: 0.0833\n",
      "C: 2.0, Error rate: 0.0104\n",
      "C: 5.0, Error rate: 0.0104\n",
      "C: 10.0, Error rate: 0.0000\n",
      "C: 50.0, Error rate: 0.0000\n",
      "C: 100.0, Error rate: 0.0000\n",
      "C: 500.0, Error rate: 0.0000\n",
      "C: 1000.0, Error rate: 0.0000\n",
      "Best C: 10.0, Best error rate: 0.0000\n",
      "PAD = 2.0000\n",
      "PAD = 2.0000\n",
      "Fitted PCA on batch: source 96/96, target 96/96\n",
      "Reduced source shape: (96, 100), target shape: (96, 100)\n",
      "== C: 0.01, Error rate: 0.5208\n",
      "== C: 0.1, Error rate: 0.4271\n",
      "== C: 0.5, Error rate: 0.1875\n",
      "== C: 1.0, Error rate: 0.1354\n",
      "== C: 2.0, Error rate: 0.0312\n",
      "== C: 5.0, Error rate: 0.0104\n",
      "== C: 10.0, Error rate: 0.0000\n",
      "== C: 50.0, Error rate: 0.0000\n",
      "== C: 100.0, Error rate: 0.0000\n",
      "== C: 500.0, Error rate: 0.0000\n",
      "== C: 1000.0, Error rate: 0.0000\n",
      "Best C: 10.0, Best error rate: 0.0000\n",
      "============ PAD (SVM) = 2.0000\n",
      "LDA Error rate: 0.3229\n",
      "============ PAD (LDA) = 0.7083\n",
      "Logistic Regression Error rate: 0.5208\n",
      "Flip the predictions\n",
      "============ PAD (LogReg) = 0.0833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1761165474.823121    8348 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9553 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:1a:00.0, compute capability: 7.5\n",
      "I0000 00:00:1761165474.823716    8348 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 9548 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:67:00.0, compute capability: 7.5\n",
      "I0000 00:00:1761165474.824162    8348 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 9486 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:68:00.0, compute capability: 7.5\n",
      "I0000 00:00:1761165477.622716    8348 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 18.48281657300322 seconds\n",
      "epoch 1/5\n",
      "Fitting IncrementalPCA on batches from features_source.h5 and features_target.h5\n",
      "Fitted PCA on batch: source 96/96, target 96/96\n",
      "Reduced source shape: (96, 100), target shape: (96, 100)\n",
      "== C: 0.01, Error rate: 0.5208\n",
      "== C: 0.1, Error rate: 0.0104\n",
      "== C: 0.5, Error rate: 0.0000\n",
      "== C: 1.0, Error rate: 0.0000\n",
      "== C: 2.0, Error rate: 0.0000\n",
      "== C: 5.0, Error rate: 0.0000\n",
      "== C: 10.0, Error rate: 0.0000\n",
      "== C: 50.0, Error rate: 0.0000\n",
      "== C: 100.0, Error rate: 0.0000\n",
      "== C: 500.0, Error rate: 0.0000\n",
      "== C: 1000.0, Error rate: 0.0000\n",
      "Best C: 0.5, Best error rate: 0.0000\n",
      "============ PAD (SVM) = 2.0000\n",
      "LDA Error rate: 0.0312\n",
      "============ PAD (LDA) = 1.8750\n",
      "Logistic Regression Error rate: 0.0000\n",
      "============ PAD (LogReg) = 2.0000\n",
      "Time 20.975789706000796 seconds\n",
      "epoch 1/5 Average Training Loss: 2.519886\n",
      "epoch 1/5 Average Estimation Loss (in Source domain): 1.745616\n",
      "epoch 1/5 Average Disc Loss (in Source domain): 1246.546875\n",
      "epoch 1/5 Average JMMD Loss: 1.513635\n",
      "epoch 1/5 For observation only - Average Estimation Loss in Target domain: 2.912853\n",
      "epoch 1/5 (Val) Weighted Total Loss: 16.136040\n",
      "epoch 1/5 (Val) Average Estimation Loss (mean): 4.812811\n",
      "epoch 1/5 (Val) Average Estimation Loss (Source): 4.050889\n",
      "epoch 1/5 (Val) Average Estimation Loss (Target): 5.574733\n",
      "epoch 1/5 (Val) GAN Discriminator Loss: 1968.615967\n",
      "epoch 1/5 (Val) Domain Discriminator Loss: 2.960295\n",
      "epoch 1/5 (Val) NMSE (Source): 1.521722, NMSE (Target): 1.523492, NMSE (Mean): 1.522607\n",
      "epoch 1/5 (Val) Domain Discriminator Accuracy (Average): 0.5000\n",
      "Time 27.14024652200169 seconds\n",
      "epoch 2/5\n",
      "Fitting IncrementalPCA on batches from features_source.h5 and features_target.h5\n",
      "Fitted PCA on batch: source 96/96, target 96/96\n",
      "Reduced source shape: (96, 100), target shape: (96, 100)\n",
      "== C: 0.01, Error rate: 0.5208\n",
      "== C: 0.1, Error rate: 0.0208\n",
      "== C: 0.5, Error rate: 0.0000\n",
      "== C: 1.0, Error rate: 0.0000\n",
      "== C: 2.0, Error rate: 0.0000\n",
      "== C: 5.0, Error rate: 0.0000\n",
      "== C: 10.0, Error rate: 0.0000\n",
      "== C: 50.0, Error rate: 0.0000\n",
      "== C: 100.0, Error rate: 0.0000\n",
      "== C: 500.0, Error rate: 0.0000\n",
      "== C: 1000.0, Error rate: 0.0000\n",
      "Best C: 0.5, Best error rate: 0.0000\n",
      "============ PAD (SVM) = 2.0000\n",
      "LDA Error rate: 0.0521\n",
      "============ PAD (LDA) = 1.7917\n",
      "Logistic Regression Error rate: 0.0000\n",
      "============ PAD (LogReg) = 2.0000\n",
      "Time 29.33134744200288 seconds\n",
      "epoch 2/5 Average Training Loss: 2.392245\n",
      "epoch 2/5 Average Estimation Loss (in Source domain): 1.698898\n",
      "epoch 2/5 Average Disc Loss (in Source domain): 1120.468140\n",
      "epoch 2/5 Average JMMD Loss: 1.359423\n",
      "epoch 2/5 For observation only - Average Estimation Loss in Target domain: 2.902269\n",
      "epoch 2/5 (Val) Weighted Total Loss: 16.890675\n",
      "epoch 2/5 (Val) Average Estimation Loss (mean): 4.745140\n",
      "epoch 2/5 (Val) Average Estimation Loss (Source): 3.963688\n",
      "epoch 2/5 (Val) Average Estimation Loss (Target): 5.526591\n",
      "epoch 2/5 (Val) GAN Discriminator Loss: 2157.961914\n",
      "epoch 2/5 (Val) Domain Discriminator Loss: 2.711453\n",
      "epoch 2/5 (Val) NMSE (Source): 1.488912, NMSE (Target): 1.522130, NMSE (Mean): 1.505521\n",
      "epoch 2/5 (Val) Domain Discriminator Accuracy (Average): 0.5000\n",
      "Time 34.62950791100229 seconds\n",
      "epoch 3/5\n",
      "Fitting IncrementalPCA on batches from features_source.h5 and features_target.h5\n",
      "Fitted PCA on batch: source 96/96, target 96/96\n",
      "Reduced source shape: (96, 100), target shape: (96, 100)\n",
      "== C: 0.01, Error rate: 0.5208\n",
      "== C: 0.1, Error rate: 0.0521\n",
      "== C: 0.5, Error rate: 0.0000\n",
      "== C: 1.0, Error rate: 0.0000\n",
      "== C: 2.0, Error rate: 0.0000\n",
      "== C: 5.0, Error rate: 0.0000\n",
      "== C: 10.0, Error rate: 0.0000\n",
      "== C: 50.0, Error rate: 0.0000\n",
      "== C: 100.0, Error rate: 0.0000\n",
      "== C: 500.0, Error rate: 0.0000\n",
      "== C: 1000.0, Error rate: 0.0000\n",
      "Best C: 0.5, Best error rate: 0.0000\n",
      "============ PAD (SVM) = 2.0000\n",
      "LDA Error rate: 0.0521\n",
      "============ PAD (LDA) = 1.7917\n",
      "Logistic Regression Error rate: 0.0000\n",
      "============ PAD (LogReg) = 2.0000\n",
      "Time 36.84098897400327 seconds\n",
      "epoch 3/5 Average Training Loss: 2.303952\n",
      "epoch 3/5 Average Estimation Loss (in Source domain): 1.665695\n",
      "epoch 3/5 Average Disc Loss (in Source domain): 1246.883667\n",
      "epoch 3/5 Average JMMD Loss: 1.254130\n",
      "epoch 3/5 For observation only - Average Estimation Loss in Target domain: 2.901709\n",
      "epoch 3/5 (Val) Weighted Total Loss: 16.467499\n",
      "epoch 3/5 (Val) Average Estimation Loss (mean): 4.701636\n",
      "epoch 3/5 (Val) Average Estimation Loss (Source): 3.908979\n",
      "epoch 3/5 (Val) Average Estimation Loss (Target): 5.494293\n",
      "epoch 3/5 (Val) GAN Discriminator Loss: 2100.598633\n",
      "epoch 3/5 (Val) Domain Discriminator Loss: 2.525740\n",
      "epoch 3/5 (Val) NMSE (Source): 1.465108, NMSE (Target): 1.521482, NMSE (Mean): 1.493295\n",
      "epoch 3/5 (Val) Domain Discriminator Accuracy (Average): 0.5000\n",
      "Time 42.05075850400317 seconds\n",
      "epoch 4/5\n",
      "Fitting IncrementalPCA on batches from features_source.h5 and features_target.h5\n",
      "Fitted PCA on batch: source 96/96, target 96/96\n",
      "Reduced source shape: (96, 100), target shape: (96, 100)\n",
      "== C: 0.01, Error rate: 0.5208\n",
      "== C: 0.1, Error rate: 0.0521\n",
      "== C: 0.5, Error rate: 0.0000\n",
      "== C: 1.0, Error rate: 0.0000\n",
      "== C: 2.0, Error rate: 0.0000\n",
      "== C: 5.0, Error rate: 0.0000\n",
      "== C: 10.0, Error rate: 0.0000\n",
      "== C: 50.0, Error rate: 0.0000\n",
      "== C: 100.0, Error rate: 0.0000\n",
      "== C: 500.0, Error rate: 0.0000\n",
      "== C: 1000.0, Error rate: 0.0000\n",
      "Best C: 0.5, Best error rate: 0.0000\n",
      "============ PAD (SVM) = 2.0000\n",
      "LDA Error rate: 0.1562\n",
      "============ PAD (LDA) = 1.3750\n",
      "Logistic Regression Error rate: 0.0000\n",
      "============ PAD (LogReg) = 2.0000\n",
      "Time 44.597716434000176 seconds\n",
      "epoch 4/5 Average Training Loss: 2.237945\n",
      "epoch 4/5 Average Estimation Loss (in Source domain): 1.640180\n",
      "epoch 4/5 Average Disc Loss (in Source domain): 1113.658447\n",
      "epoch 4/5 Average JMMD Loss: 1.176237\n",
      "epoch 4/5 For observation only - Average Estimation Loss in Target domain: 2.903464\n",
      "epoch 4/5 (Val) Weighted Total Loss: 18.106266\n",
      "epoch 4/5 (Val) Average Estimation Loss (mean): 4.668665\n",
      "epoch 4/5 (Val) Average Estimation Loss (Source): 3.867302\n",
      "epoch 4/5 (Val) Average Estimation Loss (Target): 5.470029\n",
      "epoch 4/5 (Val) GAN Discriminator Loss: 2449.168457\n",
      "epoch 4/5 (Val) Domain Discriminator Loss: 2.383520\n",
      "epoch 4/5 (Val) NMSE (Source): 1.444956, NMSE (Target): 1.521666, NMSE (Mean): 1.483311\n",
      "epoch 4/5 (Val) Domain Discriminator Accuracy (Average): 0.5000\n",
      "Time 49.6725049930028 seconds\n",
      "epoch 5/5\n",
      "Fitting IncrementalPCA on batches from features_source.h5 and features_target.h5\n",
      "Fitted PCA on batch: source 96/96, target 96/96\n",
      "Reduced source shape: (96, 100), target shape: (96, 100)\n",
      "== C: 0.01, Error rate: 0.5208\n",
      "== C: 0.1, Error rate: 0.1354\n",
      "== C: 0.5, Error rate: 0.0000\n",
      "== C: 1.0, Error rate: 0.0000\n",
      "== C: 2.0, Error rate: 0.0000\n",
      "== C: 5.0, Error rate: 0.0000\n",
      "== C: 10.0, Error rate: 0.0000\n",
      "== C: 50.0, Error rate: 0.0000\n",
      "== C: 100.0, Error rate: 0.0000\n",
      "== C: 500.0, Error rate: 0.0000\n",
      "== C: 1000.0, Error rate: 0.0000\n",
      "Best C: 0.5, Best error rate: 0.0000\n",
      "============ PAD (SVM) = 2.0000\n",
      "LDA Error rate: 0.0729\n",
      "============ PAD (LDA) = 1.7083\n",
      "Logistic Regression Error rate: 0.0000\n",
      "============ PAD (LogReg) = 2.0000\n",
      "Time 51.898226042001625 seconds\n",
      "epoch 5/5 Average Training Loss: 2.185002\n",
      "epoch 5/5 Average Estimation Loss (in Source domain): 1.618342\n",
      "epoch 5/5 Average Disc Loss (in Source domain): 1037.195923\n",
      "epoch 5/5 Average JMMD Loss: 1.116338\n",
      "epoch 5/5 For observation only - Average Estimation Loss in Target domain: 2.909360\n",
      "epoch 5/5 (Val) Weighted Total Loss: 18.123379\n",
      "epoch 5/5 (Val) Average Estimation Loss (mean): 4.641687\n",
      "epoch 5/5 (Val) Average Estimation Loss (Source): 3.831890\n",
      "epoch 5/5 (Val) Average Estimation Loss (Target): 5.451485\n",
      "epoch 5/5 (Val) GAN Discriminator Loss: 2469.126709\n",
      "epoch 5/5 (Val) Domain Discriminator Loss: 2.272118\n",
      "epoch 5/5 (Val) NMSE (Source): 1.426286, NMSE (Target): 1.522139, NMSE (Mean): 1.474212\n",
      "epoch 5/5 (Val) Domain Discriminator Accuracy (Average): 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "start = time.perf_counter()\n",
    "\n",
    "n_epochs= 5\n",
    "epoch_min = 0\n",
    "epoch_step = 1\n",
    "# n_epochs= 3\n",
    "# epoch_min = 0\n",
    "# epoch_step = 1\n",
    "\n",
    "sub_folder_ = ['GAN_practical']  # ['GAN_linear', 'GAN_practical', 'GAN_ls']\n",
    "\n",
    "\n",
    "for sub_folder in sub_folder_:\n",
    "    print(f\"Processing: {sub_folder}\")\n",
    "    w_dist = []\n",
    "    pad_pca_lda = []\n",
    "    pad_pca_logreg = []\n",
    "    pad_pca_svm = []\n",
    "    linear_interp = False\n",
    "    if sub_folder == 'GAN_linear':\n",
    "        linear_interp =True # flag to clip values that go beyond the estimated pilot (min, max)\n",
    "    ##\n",
    "    loader_H_true_train_source = class_dict_source[sub_folder].true_train\n",
    "    loader_H_input_train_source = class_dict_source[sub_folder].input_train\n",
    "    loader_H_true_val_source = class_dict_source[sub_folder].true_val\n",
    "    loader_H_input_val_source = class_dict_source[sub_folder].input_val\n",
    "    \n",
    "    loader_H_true_train_target = class_dict_target[sub_folder].true_train\n",
    "    loader_H_input_train_target = class_dict_target[sub_folder].input_train\n",
    "    loader_H_true_val_target = class_dict_target[sub_folder].true_val\n",
    "    loader_H_input_val_target = class_dict_target[sub_folder].input_val\n",
    "    ##\n",
    "    \n",
    "    # Distribution of original input training datasets (or before training)    \n",
    "    plotfig.plotHist(loader_H_input_train_source, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name='source_beforeTrain', percent=100)\n",
    "    plotfig.plotHist(loader_H_input_train_target, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name='target_beforeTrain', percent=100)\n",
    "    \n",
    "    plotfig.plotHist(loader_H_input_train_source, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name='source_beforeTrain', percent=99)\n",
    "    plotfig.plotHist(loader_H_input_train_target, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name='target_beforeTrain', percent=99)\n",
    "    \n",
    "    plotfig.plotHist(loader_H_input_train_source, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name='source_beforeTrain', percent=95)\n",
    "    plotfig.plotHist(loader_H_input_train_target, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name='target_beforeTrain', percent=95)\n",
    "\n",
    "    plotfig.plotHist(loader_H_input_train_source, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name='source_beforeTrain', percent=90)\n",
    "    plotfig.plotHist(loader_H_input_train_target, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name='target_beforeTrain', percent=90)\n",
    "\n",
    "    # Calculate Wasserstein-1 distance for original input training datasets (before training)\n",
    "    print(\"Calculating Wasserstein-1 distance for original input training datasets (before training)...\")\n",
    "    w_dist_epoc = plotfig.wasserstein_approximate(loader_H_input_train_source, loader_H_input_train_target)\n",
    "    w_dist.append(w_dist_epoc)\n",
    "    \n",
    "    # Calculate     PAD for original input training datasets with SVM\n",
    "    pad_svm = PAD.original_PAD(loader_H_input_train_source, loader_H_input_train_target)\n",
    "    print(f\"PAD = {pad_svm:.4f}\")\n",
    "    \n",
    "    # Calculate PCA_PAD for original input training datasets with PCA_SVM, PCA_LDA, PCA_LogReg\n",
    "    X_features_, y_features_ = PAD.extract_features_with_pca(loader_H_input_train_source, loader_H_input_train_target, pca_components=100)\n",
    "    pad_pca_svm_epoc = PAD.calc_pad_svm(X_features_, y_features_)\n",
    "    pad_pca_lda_epoc = PAD.calc_pad_lda(X_features_, y_features_)\n",
    "    pad_pca_logreg_epoc = PAD.calc_pad_logreg(X_features_, y_features_)\n",
    "    \n",
    "    pad_pca_lda.append(pad_pca_lda_epoc)\n",
    "    pad_pca_logreg.append(pad_pca_logreg_epoc)\n",
    "    pad_pca_svm.append(pad_pca_svm_epoc)\n",
    "    ## \n",
    "    \n",
    "    if not os.path.exists(os.path.dirname(model_path + '/' + sub_folder +'/')):\n",
    "        os.makedirs(os.path.dirname(model_path + '/' + sub_folder + '/'))   # Domain_Adversarial/model/_/ver_/{sub_folder}\n",
    "\n",
    "    train_loss          = [] # (epoch,1)\n",
    "    train_est_loss      = [] \n",
    "    train_disc_loss     = [] \n",
    "    train_domain_loss   = []\n",
    "    train_est_loss_target = []\n",
    "    #    \n",
    "    val_loss, val_gan_disc_loss, val_domain_disc_loss,\\\n",
    "    val_est_loss_source, val_est_loss_target, val_est_loss,\\\n",
    "    source_acc, target_acc, acc,\\\n",
    "    nmse_val_source, nmse_val_target, nmse_val = [[] for _ in range(12)]\n",
    "    #\n",
    "    H_to_save = {}          # list to save to .mat file for H\n",
    "    perform_to_save = {}    # list to save to .mat file for nmse, losses,...\n",
    "\n",
    "    # ✅ Use local GAN class instead of utils_GAN.GAN\n",
    "    model = GAN(n_subc=312, gen_l2=None, disc_l2=1e-5)  # l2 regularization for generator and discriminator\n",
    "    # ✅ Fixed indentation issue\n",
    "    gen_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4, beta_1=0.5, beta_2=0.9)\n",
    "    disc_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, beta_1=0.5, beta_2=0.9)  # WGAN-GP uses Adam optimizer with beta_1=0.5\n",
    "    # ❌ No domain optimizer needed for JMMD\n",
    "    ####\n",
    "    optimizer = [gen_optimizer, disc_optimizer]  # ✅ Only 2 optimizers for JMMD\n",
    "    ####\n",
    "    \n",
    "    flag = 1 # flag to plot and save H_true\n",
    "    epoc_pad = []    # epochs that calculating pad (return_features == True)\n",
    "    for epoch in range(n_epochs):\n",
    "        # ===================== Training =====================\n",
    "        loader_H_true_train_source.reset()\n",
    "        # loader_H_practical_train_source.reset()\n",
    "        loader_H_input_train_source.reset()\n",
    "        loader_H_true_train_target.reset()\n",
    "        # loader_H_practical_train_target.reset()\n",
    "        loader_H_input_train_target.reset()\n",
    "                \n",
    "        # loader_H = [loader_H_practical_train_source, loader_H_true_train_source, loader_H_practical_train_target, loader_H_true_train_target]\n",
    "        loader_H = [loader_H_input_train_source, loader_H_true_train_source, loader_H_input_train_target, loader_H_true_train_target]\n",
    "        \n",
    "        # ✅ Only 2 loss functions needed for JMMD\n",
    "        loss_fn = [loss_fn_ce, loss_fn_bce]\n",
    "    \n",
    "        ##########################\n",
    "        if epoch in [int(n_epochs * r) for r in [0, 0.25, 0.5, 0.75]] or epoch == n_epochs-1:\n",
    "            # return_features == return features to calculate PAD\n",
    "            return_features = True\n",
    "            epoc_pad.append(epoch)\n",
    "        else:\n",
    "            return_features = False\n",
    "\n",
    "        ##########################\n",
    "        # ✅ Already correctly calling train_step_wgan_gp_jmmd with proper parameters\n",
    "        train_step_output = train_step_wgan_gp_jmmd(model, loader_H, loss_fn, optimizer, lower_range=-1, \n",
    "                        save_features=True, adv_weight=adv_weight, temporal_weight=0.02, frequency_weight=0.1,\n",
    "                        est_weight=est_weight, jmmd_weight=domain_weight, linear_interp=linear_interp)\n",
    "\n",
    "        train_epoc_loss_est        = train_step_output.avg_epoc_loss_est\n",
    "        train_epoc_loss_d          = train_step_output.avg_epoc_loss_d\n",
    "        train_epoc_loss_domain     = train_step_output.avg_epoc_loss_domain  # Now contains JMMD loss\n",
    "        train_epoc_loss            = train_step_output.avg_epoc_loss\n",
    "        train_epoc_loss_est_target = train_step_output.avg_epoc_loss_est_target\n",
    "                # train_epoc_loss        = total train loss = loss_est + lambda_jmmd * jmmd_loss\n",
    "                # train_epoc_loss_est    = loss in estimation network in source domain (labels available)\n",
    "                # train_epoc_loss_domain = JMMD loss (statistical distribution matching)\n",
    "                # train_epoc_loss_est_target - just to monitor - the machine can not calculate because no label available in source domain\n",
    "                # All are already calculated in average over training dataset (source/target - respectively)\n",
    "        print(\"Time\", time.perf_counter() - start, \"seconds\")\n",
    "        # Calculate PAD for the extracted features\n",
    "        if return_features and (domain_weight!=0):\n",
    "            features_source_file = \"features_source.h5\"\n",
    "            features_target_file = \"features_target.h5\"\n",
    "            print(f\"epoch {epoch+1}/{n_epochs}\")\n",
    "            ## Calculate PCA_PAD for extracted features with PCA_SVM, PCA_LDA, PCA_LogReg\n",
    "            X_features, y_features = PAD.extract_features_with_pca(features_source_file, features_target_file, pca_components=100)\n",
    "            pad_svm_epoc = PAD.calc_pad_svm(X_features, y_features)\n",
    "            pad_pca_svm.append(pad_svm_epoc)\n",
    "            #\n",
    "            pad_lda_epoc = PAD.calc_pad_lda(X_features, y_features)\n",
    "            pad_pca_lda.append(pad_lda_epoc)\n",
    "            #\n",
    "            pad_logreg_epoc = PAD.calc_pad_logreg(X_features, y_features)\n",
    "            pad_pca_logreg.append(pad_logreg_epoc)\n",
    "            \n",
    "            ## Distribution of extracted features\n",
    "            plotfig.plotHist(features_source_file, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name=f'source_epoch_{epoch+1}', percent=99)\n",
    "            plotfig.plotHist(features_target_file, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name=f'target_epoch_{epoch+1}', percent=99)\n",
    "            #\n",
    "            plotfig.plotHist(features_source_file, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name=f'source_epoch_{epoch+1}', percent=100)\n",
    "            plotfig.plotHist(features_target_file, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name=f'target_epoch_{epoch+1}', percent=100)\n",
    "            #\n",
    "            # Calculate Wasserstein-1 distance for extracted features\n",
    "            # print(\"Calculating Wasserstein-1 distance for extracted features ...\")\n",
    "            # w_dist_epoc = plotfig.wasserstein_approximate(features_source_file, features_target_file)\n",
    "            # w_dist.append(w_dist_epoc)\n",
    "            \n",
    "\n",
    "            if os.path.exists(features_source_file):\n",
    "                os.remove(features_source_file)\n",
    "            if os.path.exists(features_target_file):\n",
    "                os.remove(features_target_file)\n",
    "            print(\"Time\", time.perf_counter() - start, \"seconds\")\n",
    "            \n",
    "        \n",
    "        # Average loss for the epoch\n",
    "        train_loss.append(train_epoc_loss)\n",
    "        print(f\"epoch {epoch+1}/{n_epochs} Average Training Loss: {train_epoc_loss:.6f}\")\n",
    "        #\n",
    "        train_est_loss.append(train_epoc_loss_est)\n",
    "        print(f\"epoch {epoch+1}/{n_epochs} Average Estimation Loss (in Source domain): {train_epoc_loss_est:.6f}\")\n",
    "        #\n",
    "        train_disc_loss.append(train_epoc_loss_d)\n",
    "        print(f\"epoch {epoch+1}/{n_epochs} Average Disc Loss (in Source domain): {train_epoc_loss_d:.6f}\")\n",
    "        #\n",
    "        train_domain_loss.append(train_epoc_loss_domain)\n",
    "        print(f\"epoch {epoch+1}/{n_epochs} Average JMMD Loss: {train_epoc_loss_domain:.6f}\")  # ✅ Updated print message\n",
    "        #\n",
    "        train_est_loss_target.append(train_epoc_loss_est_target)\n",
    "        print(f\"epoch {epoch+1}/{n_epochs} For observation only - Average Estimation Loss in Target domain: {train_epoc_loss_est_target:.6f}\")\n",
    "        \n",
    "        \n",
    "        # ===================== Evaluation =====================\n",
    "        loader_H_true_val_source.reset()\n",
    "        loader_H_input_val_source.reset()\n",
    "        loader_H_true_val_target.reset()\n",
    "        loader_H_input_val_target.reset()\n",
    "        loader_H_eval = [loader_H_input_val_source, loader_H_true_val_source, loader_H_input_val_target, loader_H_true_val_target]\n",
    "\n",
    "        # ✅ Only 2 loss functions needed for JMMD validation\n",
    "        loss_fn = [loss_fn_ce, loss_fn_bce]\n",
    "        \n",
    "        # eval_func = utils_UDA_FiLM.val_step\n",
    "        if (epoch==epoch_min) or (epoch+1>epoch_min and (epoch-epoch_min)%epoch_step==0) or epoch==n_epochs-1:\n",
    "            # ✅ Already correctly calling val_step_wgan_gp_jmmd\n",
    "            H_sample, epoc_val_return = val_step_wgan_gp_jmmd(model, loader_H_eval, loss_fn, lower_range, \n",
    "                                            adv_weight=adv_weight, est_weight=est_weight, jmmd_weight=domain_weight, linear_interp=linear_interp)\n",
    "            visualize_H(H_sample, H_to_save, epoch, plotfig.figChan, flag, model_path, sub_folder, domain_weight=domain_weight)\n",
    "            flag = 0  # after the first epoch, no need to save H_true anymore\n",
    "            \n",
    "        else:\n",
    "            # ✅ Already correctly calling val_step_wgan_gp_jmmd\n",
    "            _, epoc_val_return = val_step_wgan_gp_jmmd(model, loader_H_eval, loss_fn, lower_range, \n",
    "                                            adv_weight=adv_weight, est_weight=est_weight, jmmd_weight=domain_weight, linear_interp=linear_interp)\n",
    "        \n",
    "        post_val(epoc_val_return, epoch, n_epochs, val_est_loss, val_est_loss_source, val_loss, val_est_loss_target,\n",
    "            val_gan_disc_loss, val_domain_disc_loss, nmse_val_source, nmse_val_target, nmse_val, source_acc, target_acc, acc, domain_weight=domain_weight)\n",
    "        \n",
    "        \n",
    "        if (epoch==epoch_min) or (epoch+1>epoch_min and (epoch-epoch_min)%epoch_step==0) or epoch==n_epochs-1:\n",
    "            metrics = {'figLoss': plotfig.figLoss, 'savemat': savemat,\n",
    "                'train_loss': train_loss, 'train_est_loss': train_est_loss, 'train_domain_loss': train_domain_loss, 'train_est_loss_target': train_est_loss_target,\n",
    "                'val_est_loss': val_est_loss, 'val_est_loss_source': val_est_loss_source, 'val_loss': val_loss, 'val_est_loss_target': val_est_loss_target,\n",
    "                'val_gan_disc_loss': val_gan_disc_loss, 'val_domain_disc_loss': val_domain_disc_loss, 'source_acc': source_acc, 'target_acc': target_acc,\n",
    "                'acc': acc, 'nmse_val_source': nmse_val_source, 'nmse_val_target': nmse_val_target, 'nmse_val': nmse_val,\n",
    "                'pad_pca_svm': pad_pca_svm, 'pad_pca_lda': pad_pca_lda, 'pad_pca_logreg': pad_pca_logreg, 'epoc_pad': epoc_pad,\n",
    "                'pad_svm': pad_svm, 'train_disc_loss': train_disc_loss, 'domain_weight': domain_weight, 'optimizer': optimizer}\n",
    "\n",
    "            save_checkpoint(model, save_model, model_path, sub_folder, epoch, metrics)\n",
    "    \n",
    "    # end of epoch loop\n",
    "    # =====================            \n",
    "    # Save performances\n",
    "    # Save H matrix\n",
    "    savemat(model_path + '/' + sub_folder + '/H_visualize/H_trix.mat', H_to_save)\n",
    "\n",
    "# end of trainmode   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a59395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.75,\n",
       " 1.9166666666666665,\n",
       " 1.9166666666666665,\n",
       " 1.9166666666666665,\n",
       " 1.9583333333333335,\n",
       " 1.9583333333333335]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_pca_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc1ace9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.75, 1.5, 1.2083333333333335, 1.375, 1.4583333333333335, 1.3333333333333335]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_pca_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b842dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoc_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7699aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.08333333333333304, 1.9583333333333335, 1.9583333333333335, 2.0, 2.0, 2.0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_pca_logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2544db10",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmse_source_LI = []\n",
    "nmse_target_LI = []\n",
    "nmse_source_practical = []\n",
    "nmse_target_practical = []\n",
    "nmse_source_LI_GAN = []\n",
    "nmse_target_LI_GAN = []\n",
    "nmse_source_practical_GAN = []\n",
    "nmse_target_practical_GAN = []\n",
    "\n",
    "SNR_dB_ = np.arange(-15, 1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ebbbd1",
   "metadata": {},
   "source": [
    "-5 practical\n",
    "-5 linear\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
