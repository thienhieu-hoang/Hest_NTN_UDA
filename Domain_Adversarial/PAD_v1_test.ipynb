{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matlab 5dB: /home/thien/Code/H_predict_UDA/H_predict_Sionna/Generate_Data/CDL_Channel/generatedChannel/ver3_/5dB/mapBaseData.mat\n",
    "Sionna 5dB: /home/thien/Code/H_predict_UDA/H_predict_Sionna/Generate_Data/Sionna/generatedChannel/ver3_/sionnaTrue.mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to calculate PAD of original datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) construct the data set U using both source and target representations of the training samples; \n",
    "\n",
    "(2) randomly split U in two subsets of equal size; \n",
    "\n",
    "(3) train linear SVMs on the first subset of U using a large range of C values; \n",
    "\n",
    "(4) compute the error of all obtained classifiers on the second subset of U; \n",
    "\n",
    "(5) use the lowest error to compute the PAD value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/thien/Code/H_predict_UDA/H_predict_Sionna/Domain_Adaptation/Domain_Adversarial\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "notebook_dir = os.getcwd()\n",
    "print(notebook_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/thien/Code/H_predict_UDA/H_predict_Sionna/Generate_Data/CDL_Channel/generatedChannel/ver3_/5dB/mapBaseData.mat\n",
      "/home/thien/Code/H_predict_UDA/H_predict_Sionna/Generate_Data/Sionna/generatedChannel/ver3_\n"
     ]
    }
   ],
   "source": [
    "source_data_file_path = os.path.abspath(os.path.join(notebook_dir, '..', '..', 'Generate_Data', 'CDL_Channel', \n",
    "                                                    'generatedChannel', 'ver3_', '5dB', 'mapBaseData.mat'))\n",
    "target_data_file_path = os.path.abspath(os.path.join(notebook_dir, '..', '..', 'Generate_Data', 'Sionna', \n",
    "                                                    'generatedChannel', 'ver3_'))\n",
    "print(source_data_file_path)\n",
    "print(target_data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_samp_source =  2048\n",
      "N_samp_target =  2048\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "batch_size=16\n",
    "\n",
    "# ============ Source data ==============\n",
    "source_file = h5py.File(source_data_file_path, 'r')\n",
    "H_true_source = source_file['H_true']\n",
    "N_samp_source = H_true_source.shape[0]\n",
    "print('N_samp_source = ', N_samp_source)\n",
    "\n",
    "# ============ Target data ==============\n",
    "target_file = h5py.File(target_data_file_path+'/sionnaTrue.mat', 'r')\n",
    "H_true_target = target_file['H_true']\n",
    "N_samp_target = H_true_target.shape[0]\n",
    "print('N_samp_target = ', N_samp_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H_duplicate_2to5: shape=(2048, 792, 84), dtype=[('real', '<f8'), ('imag', '<f8')]\n",
      "H_equalized_1_6_11: shape=(2048, 792, 3), dtype=[('real', '<f8'), ('imag', '<f8')]\n",
      "H_linear_1_6_11: shape=(2048, 792, 3), dtype=[('real', '<f8'), ('imag', '<f8')]\n",
      "H_linear_1to6: shape=(2048, 792, 84), dtype=[('real', '<f8'), ('imag', '<f8')]\n",
      "H_linear_2to5: shape=(2048, 792, 84), dtype=[('real', '<f8'), ('imag', '<f8')]\n",
      "H_time: shape=(2048, 168944), dtype=[('real', '<f8'), ('imag', '<f8')]\n",
      "H_true: shape=(2048, 792, 154), dtype=[('real', '<f8'), ('imag', '<f8')]\n"
     ]
    }
   ],
   "source": [
    "for key in source_file.keys():\n",
    "        data = source_file[key]\n",
    "        if isinstance(data, h5py.Dataset):\n",
    "            print(f\"{key}: shape={data.shape}, dtype={data.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_size =  1840\n",
      "val_size =  208\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "indices_source = np.arange(N_samp_source)\n",
    "np.random.shuffle(indices_source)\n",
    "indices_target = np.arange(N_samp_target)\n",
    "np.random.shuffle(indices_target)\n",
    "#\n",
    "train_size = int(np.floor(N_samp_source * 0.9) // batch_size * batch_size)\n",
    "val_size = N_samp_source - train_size\n",
    "print('train_size = ', train_size)\n",
    "print('val_size = ', val_size)\n",
    "\n",
    "# Repeat the indices to match the maximum number of samples\n",
    "N_samp = max(N_samp_source, N_samp_target) \n",
    "indices_source = np.resize(indices_source, N_samp)\n",
    "indices_target = np.resize(indices_target, N_samp)\n",
    "\n",
    "# =======================================================\n",
    "## Divide the indices into training and validation sets\n",
    "indices_train_source = indices_source[:train_size]\n",
    "\n",
    "indices_train_target = indices_target[:train_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 10:48:00.655932: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750949280.670526   40645 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750949280.674880   40645 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750949280.686340   40645 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750949280.686352   40645 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750949280.686354   40645 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750949280.686356   40645 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-26 10:48:00.690463: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(notebook_dir, '..', '..')))\n",
    "import Est_btween_CSIRS.helper.utils as utils_CNN\n",
    "# =========== Source dataset ==============\n",
    "loader_H_1_6_11_train_source = utils_CNN.H5BatchLoader(source_file, 'H_linear_1_6_11', batch_size=batch_size, shuffled_indices=indices_train_source)\n",
    "    # channel at symbol 2 of slots 1,6,11 (channel corresponding to CSI-RS 1, 2, 3)\n",
    "\n",
    "# =========== Target dataset ==============\n",
    "loader_H_1_6_11_train_target = utils_CNN.H5BatchLoader(target_file, 'H_linear_1_6', batch_size=batch_size, shuffled_indices=indices_train_target)\n",
    "    # channel at symbol 2 of slots 1,6 (channel corresponding to CSI-RS 1, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for SVM\n",
    "Can explode the memory with large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_H_1_6_11_train_source.reset()\n",
    "loader_H_1_6_11_train_target.reset()\n",
    "all_features = []\n",
    "all_labels = []\n",
    "#\n",
    "for batch_idx in range(loader_H_1_6_11_train_target.total_batches):\n",
    "    # batch_target: shape (batch_size, 792, 2)\n",
    "    batch_target = loader_H_1_6_11_train_target.next_batch()  # (batch_size, 792,2)\n",
    "    real_target = batch_target['real']  # (batch_size, 792,2)\n",
    "    imag_target = batch_target['imag']  # (batch_size, 792,2)\n",
    "    real_flat = real_target.reshape(real_target.shape[0], -1)  # (batch_size, 1584)\n",
    "    imag_flat = imag_target.reshape(imag_target.shape[0], -1)  # (batch_size, 1584)\n",
    "    combined_target = np.concatenate([real_flat, imag_flat], axis=1)  # (batch_size, 3168)\n",
    "    target_labels = (np.ones(combined_target.shape[0], dtype=int))\n",
    "    # \n",
    "    batch_source = loader_H_1_6_11_train_source.next_batch()  # (batch_size, 792, 3)\n",
    "    batch_source = batch_source[:,:,0:2]\n",
    "    real_source = batch_source['real']  # (batch_size, 792,2)\n",
    "    imag_source = batch_source['imag']  # (batch_size, 792,2)\n",
    "    real_flat_source = real_source.reshape(real_source.shape[0], -1)  # (batch_size, 1584)\n",
    "    imag_flat_source = imag_source.reshape(imag_source.shape[0], -1)  # (batch_size, 1584)\n",
    "    combined_source = np.concatenate([real_flat_source, imag_flat_source], axis=1)  # (batch_size, 3168)\n",
    "    source_labels = (np.zeros(combined_source.shape[0], dtype=int))\n",
    "\n",
    "    # --- Combine and append ---\n",
    "    all_features.append(combined_source)\n",
    "    all_features.append(combined_target)\n",
    "    all_labels.append(source_labels)\n",
    "    all_labels.append(target_labels)\n",
    "\n",
    "# Stack all batches into a single dataset\n",
    "X = np.vstack(all_features)  # shape: (n_samples, 3168)\n",
    "y = np.concatenate(all_labels)  # shape: (n_samples,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1840, 3168) (1840,)\n",
      "(1840, 3168) (1840,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X: (3680, 3168), y: (3680,)\n",
    "X1, X2, y1, y2 = train_test_split(X, y, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "print(X1.shape, y1.shape)  # (1840, 3168) (1840,)\n",
    "print(X2.shape, y2.shape)  # (1840, 3168) (1840,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 0.01, Error rate: 0.0245\n",
      "C: 0.1, Error rate: 0.0239\n",
      "C: 0.5, Error rate: 0.0212\n",
      "C: 1.0, Error rate: 0.0201\n",
      "C: 2.0, Error rate: 0.0120\n",
      "C: 5.0, Error rate: 0.0011\n",
      "C: 10.0, Error rate: 0.0011\n",
      "C: 50.0, Error rate: 0.0011\n",
      "C: 100.0, Error rate: 0.0011\n",
      "C: 500.0, Error rate: 0.0011\n",
      "C: 1000.0, Error rate: 0.0011\n",
      "Best C: 5.0, Best error rate: 0.0011\n",
      "PAD = 1.9957\n"
     ]
    }
   ],
   "source": [
    "# Now train SVM\n",
    "from sklearn.svm import SVC\n",
    "C_values = [0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 50.0, 100.0, 500.0, 1000.0]\n",
    "best_epsilon = 1.0\n",
    "best_C = None\n",
    "for C in C_values:\n",
    "    svm = SVC(C=C, probability=True)\n",
    "    svm.fit(X1, y1)\n",
    "    accuracy = svm.score(X2, y2)\n",
    "    error_rate = 1 - accuracy\n",
    "    print(f\"C: {C}, Error rate: {error_rate:.4f}\")\n",
    "    if error_rate < best_epsilon:\n",
    "        best_epsilon = error_rate\n",
    "        best_C = C\n",
    "print(f\"Best C: {best_C}, Best error rate: {best_epsilon:.4f}\")\n",
    "pad = 2 * (1 - 2 * best_epsilon)\n",
    "print(f\"PAD = {pad:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for SGDClassifier\n",
    "Can train the classifier batch by batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 1e-06, Error rate: 0.0011\n",
      "alpha: 4.641588833612782e-06, Error rate: 0.0011\n",
      "alpha: 2.1544346900318823e-05, Error rate: 0.0011\n",
      "alpha: 0.0001, Error rate: 0.0005\n",
      "alpha: 0.00046415888336127773, Error rate: 0.0016\n",
      "alpha: 0.002154434690031882, Error rate: 0.0011\n",
      "alpha: 0.01, Error rate: 0.0016\n",
      "alpha: 0.046415888336127725, Error rate: 0.0022\n",
      "alpha: 0.21544346900318823, Error rate: 0.0033\n",
      "alpha: 1.0, Error rate: 0.0201\n",
      "Best alpha: 0.0001, Best error rate: 0.0005\n",
      "PAD (SGDClassifier) = 1.9978\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X1_scaled = scaler.fit_transform(X1)\n",
    "X2_scaled = scaler.transform(X2)\n",
    "\n",
    "alpha_values = np.logspace(-6, 0, 10)\n",
    "best_epsilon_sgd = 1.0\n",
    "best_alpha = None\n",
    "for alpha in alpha_values:\n",
    "    sgd = SGDClassifier(loss='hinge', alpha=alpha, max_iter=5000, tol=1e-5, random_state=42)\n",
    "    sgd.fit(X1_scaled, y1)\n",
    "    y2_pred = sgd.predict(X2_scaled)\n",
    "    error_rate = 1 - accuracy_score(y2, y2_pred)\n",
    "    print(f\"alpha: {alpha}, Error rate: {error_rate:.4f}\")\n",
    "    if error_rate < best_epsilon_sgd:\n",
    "        best_epsilon_sgd = error_rate\n",
    "        best_alpha = alpha\n",
    "print(f\"Best alpha: {best_alpha}, Best error rate: {best_epsilon_sgd:.4f}\")\n",
    "pad_sgd = 2 * (1 - 2 * best_epsilon_sgd)\n",
    "print(f\"PAD (SGDClassifier) = {pad_sgd:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test SGD for batches\n",
    "Loading from features_source.h5 and features_target.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 1e-06, Error rate: 0.0729\n",
      "alpha: 4.641588833612782e-06, Error rate: 0.0729\n",
      "alpha: 2.1544346900318823e-05, Error rate: 0.0729\n",
      "alpha: 0.0001, Error rate: 0.0729\n",
      "alpha: 0.00046415888336127773, Error rate: 0.0729\n",
      "alpha: 0.002154434690031882, Error rate: 0.0729\n",
      "alpha: 0.01, Error rate: 0.0729\n",
      "alpha: 0.046415888336127725, Error rate: 0.0729\n",
      "alpha: 0.21544346900318823, Error rate: 0.0729\n",
      "alpha: 1.0, Error rate: 0.0729\n",
      "PAD (SGDClassifier) = 1.7083\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "with h5py.File('features_source.h5', 'r') as f_source, h5py.File('features_target.h5', 'r') as f_target:\n",
    "    n_source = f_source['features'].shape[0]\n",
    "    n_target = f_target['features'].shape[0]\n",
    "    total = n_source + n_target\n",
    "\n",
    "    # Create index and label arrays\n",
    "    indices = np.arange(total)\n",
    "    labels = np.zeros(total, dtype=int)\n",
    "    labels[n_source:] = 1  # target samples get label 1\n",
    "\n",
    "    # # Shuffle indices\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # Select half for your split\n",
    "    split = total // 2\n",
    "    train_indices = indices[:split]\n",
    "    train_labels = labels[train_indices]\n",
    "    test_indices = indices[split:]\n",
    "    test_labels = labels[test_indices]\n",
    "\n",
    "    batch_size = 16\n",
    "    alpha_values = np.logspace(-6, 0, 10)\n",
    "    best_epsilon_sgd = 1.0\n",
    "    best_alpha = None\n",
    "    \n",
    "    # --- Train SGDClassifier batch by batch ---\n",
    "    for alpha in alpha_values:\n",
    "        clf = SGDClassifier(loss='hinge', alpha=alpha, max_iter=5000, tol=1e-5, random_state=42)\n",
    "        classes = np.array([0, 1])\n",
    "        first_batch = True\n",
    "        \n",
    "\n",
    "        for i in range(0, len(train_indices), batch_size):\n",
    "            batch_indices = train_indices[i:i+batch_size]\n",
    "            batch_labels = train_labels[i:i+batch_size]\n",
    "            batch_features = []\n",
    "            for idx in batch_indices:\n",
    "                if idx < n_source:\n",
    "                    feat = f_source['features'][idx]\n",
    "                else:\n",
    "                    feat = f_target['features'][idx - n_source]\n",
    "                batch_features.append(feat)\n",
    "            batch_features = np.stack(batch_features, axis=0)\n",
    "            batch_features = batch_features.reshape(batch_features.shape[0], -1)\n",
    "            if first_batch:\n",
    "                clf.partial_fit(batch_features, batch_labels, classes=classes)\n",
    "                first_batch = False\n",
    "            else:\n",
    "                clf.partial_fit(batch_features, batch_labels)\n",
    "\n",
    "        # --- Test SGDClassifier batch by batch ---\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        for i in range(0, len(test_indices), batch_size):\n",
    "            batch_indices = test_indices[i:i+batch_size]\n",
    "            batch_labels = test_labels[i:i+batch_size]\n",
    "            batch_features = []\n",
    "            for idx in batch_indices:\n",
    "                if idx < n_source:\n",
    "                    feat = f_source['features'][idx]\n",
    "                else:\n",
    "                    feat = f_target['features'][idx - n_source]\n",
    "                batch_features.append(feat)\n",
    "            batch_features = np.stack(batch_features, axis=0)\n",
    "            batch_features = batch_features.reshape(batch_features.shape[0], -1)\n",
    "            preds = clf.predict(batch_features)\n",
    "            y_true.extend(batch_labels)\n",
    "            y_pred.extend(preds)\n",
    "\n",
    "        error_rate = 1 - accuracy_score(y_true, y_pred)\n",
    "        print(f\"alpha: {alpha}, Error rate: {error_rate:.4f}\")\n",
    "        if error_rate < best_epsilon_sgd:\n",
    "            best_epsilon_sgd = error_rate\n",
    "            best_alpha = alpha\n",
    "\n",
    "        \n",
    "    pad = 2 * (1 - 2 * best_epsilon_sgd)\n",
    "    print(f\"PAD (SGDClassifier) = {pad:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 1e-06, Error rate: 0.0729\n",
      "alpha: 4.641588833612782e-06, Error rate: 0.0729\n",
      "alpha: 2.1544346900318823e-05, Error rate: 0.0729\n",
      "alpha: 0.0001, Error rate: 0.0729\n",
      "alpha: 0.00046415888336127773, Error rate: 0.0729\n",
      "alpha: 0.002154434690031882, Error rate: 0.0729\n",
      "alpha: 0.01, Error rate: 0.0729\n",
      "alpha: 0.046415888336127725, Error rate: 0.0729\n",
      "alpha: 0.21544346900318823, Error rate: 0.0729\n",
      "alpha: 1.0, Error rate: 0.0729\n",
      "PAD (SGDClassifier) = 1.7083\n"
     ]
    }
   ],
   "source": [
    "from helper import PAD\n",
    "pad = PAD.cal_PAD_SGD('features_source.h5', 'features_target.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
