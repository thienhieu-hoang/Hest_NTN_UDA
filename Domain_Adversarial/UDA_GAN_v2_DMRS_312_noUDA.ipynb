{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "465b0cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 17:25:57.352397: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1759094757.366566   68401 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1759094757.370864   68401 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1759094757.381859   68401 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1759094757.381872   68401 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1759094757.381874   68401 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1759094757.381875   68401 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-09-28 17:25:57.385843: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.19.0\n",
      "Available GPUs: 3\n",
      "Memory growth enabled for GPU: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "Memory growth enabled for GPU: PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')\n",
      "Memory growth enabled for GPU: PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU')\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2')\n",
      "Number of devices in strategy: 3\n",
      "/home/thien/Code/H_predict_UDA/H_predict_Sionna/Domain_Adaptation/Domain_Adversarial\n",
      "/home/thien/Code/H_predict_UDA/H_predict_Sionna/Domain_Adaptation/Domain_Adversarial/helper\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1759094761.510249   68401 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9543 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:1a:00.0, compute capability: 7.5\n",
      "I0000 00:00:1759094761.510770   68401 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 9543 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:67:00.0, compute capability: 7.5\n",
      "I0000 00:00:1759094761.511295   68401 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 9353 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:68:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "\n",
    "# Robust GPU configuration to prevent CUDA errors\n",
    "try:\n",
    "    # Get all available GPUs\n",
    "    gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "    print(f\"Available GPUs: {len(gpus)}\")\n",
    "    \n",
    "    if gpus:\n",
    "        # Enable memory growth for all GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(f\"Memory growth enabled for GPU: {gpu}\")\n",
    "            \n",
    "        # Create a MirroredStrategy for multi-GPU training\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "        print(f\"Number of devices in strategy: {strategy.num_replicas_in_sync}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No GPUs found, using CPU\")\n",
    "        # Create a default strategy for CPU\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "        \n",
    "except RuntimeError as e:\n",
    "    print(f\"GPU configuration error: {e}\")\n",
    "    print(\"Falling back to CPU execution\")\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected GPU error: {e}\")\n",
    "    print(\"Continuing with default GPU settings\")\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "# Make strategy available globally\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pickle\n",
    "from scipy.io import savemat, loadmat\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "print(notebook_dir)\n",
    "sys.path.append(os.path.abspath(os.path.join(notebook_dir, 'helper')))\n",
    "\n",
    "print(os.path.abspath(os.path.join(notebook_dir, 'helper')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8d193a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils_GAN_copy as utils_GAN\n",
    "import PAD, utils_GAN_FiLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a1a73e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/thien/Code/H_predict_UDA/H_predict_Sionna\n"
     ]
    }
   ],
   "source": [
    "print(os.path.abspath(os.path.join(notebook_dir, '..', '..')))\n",
    "sys.path.append(os.path.abspath(os.path.join(notebook_dir, '..', '..')))\n",
    "import Est_btween_CSIRS.helper.utils as utils_CNN\n",
    "import Est_btween_CSIRS.helper.loader as loader\n",
    "import Est_btween_CSIRS.helper.plotfig as plotfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "250e1bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1759094763.849103   68401 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 312, 14, 2)\n",
      "(16, 8, 4, 1)\n",
      "(16, 18, 14, 512)\n",
      "N_samp_source =  2048\n",
      "N_samp_target =  2048\n",
      "train_size =  1840\n",
      "val_size =  208\n",
      "size loader_H_true_train =  115\n",
      "size loader_H_true_val =  13\n"
     ]
    }
   ],
   "source": [
    "\n",
    "source_data_file_path = os.path.abspath(os.path.join(notebook_dir, '..', '..', 'Generate_Data', 'CDL_Channel', 'generatedChannel', 'ver9_', '0dB', 'mapBaseData.mat'))\n",
    "target_data_file_path = os.path.abspath(os.path.join(notebook_dir, '..', '..', 'Generate_Data', 'Sionna', 'generatedChannel', 'ver3_', 'sionnaTrue.mat'))\n",
    "\n",
    "norm_approach = 'minmax' # can be set to 'std'\n",
    "lower_range = -1 \n",
    "    # if norm_approach = 'minmax': \n",
    "        # =  0 for scaling to  [0 1]\n",
    "        # = -1 for scaling to [-1 1]\n",
    "    # if norm_approach = 'std': can be any value, but need to be defined\n",
    "adv_weight=0.005\n",
    "est_weight=1\n",
    "domain_weight=0 # 0.5 for Domain Discriminator, 0 for no Domain Discriminator\n",
    "\n",
    "# snr_start = -25\n",
    "# snr_step = 5\n",
    "# snr_end = 25\n",
    "# SNR = np.arange(snr_start, snr_end+1, snr_step)\n",
    "\n",
    "# SNR = np.array([0])\n",
    "\n",
    "# if len(SNR) >1:\n",
    "#     SNR_txt = f'{snr_start}:{snr_step}:{snr_end}'\n",
    "# else:\n",
    "#     SNR_txt = f'{SNR[0]}'\n",
    "    \n",
    "# ============ CNN settings ==============\n",
    "if norm_approach == 'minmax':\n",
    "    if lower_range == 0:\n",
    "        norm_txt = 'Using min-max [0 1]'\n",
    "    elif lower_range ==-1:\n",
    "        norm_txt = 'Using min-max [-1 1]'\n",
    "elif norm_approach == 'no':\n",
    "    norm_txt = 'No'\n",
    "    \n",
    "CNN_activation = 'Tanh'\n",
    "CNN_DropOut = 0.2\n",
    "if CNN_DropOut != 0:\n",
    "    dropOut_txt = f'Add p={CNN_DropOut} DropOut'\n",
    "\n",
    "\n",
    "# Paths to save\n",
    "idx_save_path = loader.find_incremental_filename(notebook_dir + '/model/GAN_calcu','ver', '_', '')\n",
    "\n",
    "save_model = 1\n",
    "\n",
    "load_checkpoint = True  # True if continue training\n",
    "if load_checkpoint:\n",
    "    model_path = notebook_dir + '/model/GAN_calcu/ver' + str(idx_save_path-1) + '_' # or replace idx_save_path-1 by the desired folder index\n",
    "else:\n",
    "    model_path = notebook_dir + '/model/GAN_calcu/ver' + str(idx_save_path) + '_'\n",
    "if load_checkpoint:\n",
    "    start_epoch = 21  # This is the epoch we want to CONTINUE FROM (not load from)\n",
    "else:\n",
    "    start_epoch = 0    \n",
    "\n",
    "# figure_path = notebook_dir + '/model/GAN/ver' + str(idx_save_path) + '_/figure'\n",
    "model_readme = model_path + '/readme.txt'\n",
    "\n",
    "# Generate a (16, 792, 14, 2) matrix with random values\n",
    "random_matrix = np.random.randn(16, 312, 14, 2)\n",
    "random_matrix.shape\n",
    "\n",
    "GAN_model = utils_GAN.GAN(n_subc=312)\n",
    "out_put = GAN_model(random_matrix)\n",
    "print(out_put.gen_out.shape)\n",
    "print(out_put.disc_out.shape)\n",
    "print(out_put.extracted_features.shape)\n",
    "\n",
    "source_data_file_path = os.path.abspath(os.path.join(notebook_dir, '..', '..', 'Generate_Data', 'CDL_Channel', 'generatedChannel', 'ver9_', '0dB', 'mapBaseData.mat'))\n",
    "target_data_file_path = os.path.abspath(os.path.join(notebook_dir, '..', '..', 'Generate_Data', 'Sionna', 'generatedChannel', 'ver3_', 'sionnaTrue.mat'))\n",
    "\n",
    "import h5py\n",
    "\n",
    "batch_size=16\n",
    "\n",
    "# ============ Source data ==============\n",
    "source_file = h5py.File(source_data_file_path, 'r')\n",
    "H_true_source = source_file['H_true']\n",
    "N_samp_source = H_true_source.shape[0]\n",
    "print('N_samp_source = ', N_samp_source)\n",
    "\n",
    "# ============ Target data ==============\n",
    "target_file = h5py.File(target_data_file_path, 'r')\n",
    "H_true_target = target_file['H_true']\n",
    "N_samp_target = H_true_target.shape[0]\n",
    "print('N_samp_target = ', N_samp_target)\n",
    "\n",
    "indices_source = np.arange(N_samp_source)\n",
    "np.random.shuffle(indices_source)\n",
    "indices_target = np.arange(N_samp_target)\n",
    "np.random.shuffle(indices_target)\n",
    "#\n",
    "train_size = int(np.floor(N_samp_source * 0.9) // batch_size * batch_size)\n",
    "val_size = N_samp_source - train_size\n",
    "\n",
    "# Repeat the indices to match the maximum number of samples\n",
    "N_samp = max(N_samp_source, N_samp_target) \n",
    "indices_source = np.resize(indices_source, N_samp)\n",
    "indices_target = np.resize(indices_target, N_samp)\n",
    "\n",
    "# =======================================================\n",
    "## Divide the indices into training and validation sets\n",
    "indices_train_source = indices_source[:train_size]\n",
    "indices_val_source   = indices_source[train_size:train_size + val_size]\n",
    "\n",
    "indices_train_target = indices_target[:train_size]\n",
    "indices_val_target   = indices_target[train_size:train_size + val_size]\n",
    "\n",
    "# to test code\n",
    "# indices_train_source = indices_source[:96]\n",
    "# indices_val_source = indices_source[2032:]\n",
    "# indices_train_target = indices_target[:96]\n",
    "# indices_val_target = indices_target[2032:]\n",
    "\n",
    "print('train_size = ', indices_train_source.shape[0])\n",
    "print('val_size = ', indices_val_source.shape[0])\n",
    "\n",
    "# =========== Source dataset ==============\n",
    "loader_H_true_train_source = utils_CNN.H5BatchLoader(source_file, dataset_name='H_true', batch_size=batch_size, shuffled_indices=indices_train_source)\n",
    "loader_H_practical_train_source = utils_CNN.H5BatchLoader(source_file, 'H_practical_save', batch_size=batch_size, shuffled_indices=indices_train_source)\n",
    "loader_H_linear_train_source = utils_CNN.H5BatchLoader(source_file, 'H_linear_save', batch_size=batch_size, shuffled_indices=indices_train_source)\n",
    "\n",
    "loader_H_true_val_source = utils_CNN.H5BatchLoader(source_file, dataset_name='H_true', batch_size=batch_size, shuffled_indices=indices_val_source)\n",
    "loader_H_practical_val_source = utils_CNN.H5BatchLoader(source_file, 'H_practical_save', batch_size=batch_size, shuffled_indices=indices_val_source)\n",
    "loader_H_linear_val_source = utils_CNN.H5BatchLoader(source_file, 'H_linear_save', batch_size=batch_size, shuffled_indices=indices_val_source)\n",
    "\n",
    "\n",
    "# =========== Target dataset ==============\n",
    "# replace source_file by target_file\n",
    "loader_H_true_train_target = utils_CNN.H5BatchLoader(source_file, dataset_name='H_true', batch_size=batch_size, shuffled_indices=indices_train_target)\n",
    "    # actually at target domain, we don't have true channels, just use this for evaluating the model\n",
    "loader_H_practical_train_target = utils_CNN.H5BatchLoader(source_file, 'H_practical_save', batch_size=batch_size, shuffled_indices=indices_train_target)\n",
    "    # channel at symbol 2 of slots 1,6,11 (channel corresponding to CSI-RS 1, 2)\n",
    "loader_H_true_val_target = utils_CNN.H5BatchLoader(source_file, dataset_name='H_true', batch_size=batch_size, shuffled_indices=indices_val_target)\n",
    "loader_H_practical_val_target = utils_CNN.H5BatchLoader(source_file, 'H_practical_save', batch_size=batch_size, shuffled_indices=indices_val_target)\n",
    "\n",
    "print('size loader_H_true_train = ', loader_H_true_train_target.total_batches)\n",
    "print('size loader_H_true_val = ', loader_H_true_val_target.total_batches)\n",
    "\n",
    "class DataLoaders:\n",
    "    def __init__(self, file, indices_train, indices_val, tag='practical', batch_size=32):\n",
    "        self.true_train = utils_CNN.H5BatchLoader(file, dataset_name='H_true', batch_size=batch_size, shuffled_indices=indices_train)\n",
    "        self.true_val = utils_CNN.H5BatchLoader(file, dataset_name='H_true', batch_size=batch_size, shuffled_indices=indices_val)\n",
    "\n",
    "        self.input_train = utils_CNN.H5BatchLoader(file, f'H_{tag}_save', batch_size=batch_size, shuffled_indices=indices_train)\n",
    "        self.input_val = utils_CNN.H5BatchLoader(file, f'H_{tag}_save', batch_size=batch_size, shuffled_indices=indices_val)\n",
    "\n",
    "# Source domain\n",
    "class_dict_source = {\n",
    "    'GAN_practical': DataLoaders(source_file, indices_train_source, indices_val_source, tag='practical', batch_size=batch_size),\n",
    "    'GAN_linear': DataLoaders(source_file, indices_train_source, indices_val_source, tag='linear', batch_size=batch_size)\n",
    "}\n",
    "\n",
    "# Target domain\n",
    "# replace source_file by target_file when run UDA\n",
    "class_dict_target = {\n",
    "    'GAN_practical': DataLoaders(source_file, indices_train_target, indices_val_target, tag='practical', batch_size=batch_size),\n",
    "    'GAN_linear': DataLoaders(source_file, indices_train_target, indices_val_target, tag='linear', batch_size=batch_size)\n",
    "}\n",
    "\n",
    "loss_fn_ce = tf.keras.losses.MeanSquaredError()  # Channel estimation loss (generator loss)\n",
    "loss_fn_bce = tf.keras.losses.BinaryCrossentropy(from_logits=False) # Binary cross-entropy loss for discriminator\n",
    "loss_fn_domain = tf.keras.losses.BinaryCrossentropy()  # Domain classification loss\n",
    "\n",
    "import time\n",
    "start = time.perf_counter()\n",
    "\n",
    "n_epochs= 300\n",
    "epoch_min = 10\n",
    "epoch_step = 10\n",
    "# n_epochs= 3\n",
    "# epoch_min = 0\n",
    "# epoch_step = 1\n",
    "\n",
    "sub_folder_ = ['GAN_linear'] # , 'GAN_practical']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f765f0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: GAN_linear\n",
      "Loading checkpoint from epoch 21 to continue training from epoch 21...\n",
      "Optimizers created from saved configs\n",
      "✅ Checkpoint restored successfully: /home/thien/Code/H_predict_UDA/H_predict_Sionna/Domain_Adaptation/Domain_Adversarial/model/GAN_calcu/ver1_/GAN_linear/model/epoch_21\n",
      "Loaded 21 epochs of training history.\n",
      "Last loaded training loss: 0.0042967889457941055\n",
      "Time 91.22515334095806 seconds\n",
      "epoch 22/300 Average Training Loss: 0.004373\n",
      "epoch 22/300 Average Estimation Loss (in Source domain): 0.004530\n",
      "epoch 22/300 Average Disc Loss (in Source domain): 0.071691\n",
      "epoch 22/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 22/300 For observation only - Average Estimation Loss in Target domain: 0.004409\n",
      "epoch 22/300 (Val) Weighted Total Loss: 0.007614\n",
      "epoch 22/300 (Val) Average Estimation Loss (mean): 0.007109\n",
      "epoch 22/300 (Val) Average Estimation Loss (Source): 0.007634\n",
      "epoch 22/300 (Val) GAN Discriminator Loss: 0.100973\n",
      "epoch 22/300 (Val) NMSE (Source): 0.006985, NMSE (Target): 0.006114, NMSE (Mean): 0.006550\n",
      "epoch 22/300 (Val) Domain Discriminator Accuracy (Average): 0.5264\n",
      "Time 183.42918078997172 seconds\n",
      "epoch 23/300 Average Training Loss: 0.004058\n",
      "epoch 23/300 Average Estimation Loss (in Source domain): 0.004181\n",
      "epoch 23/300 Average Disc Loss (in Source domain): 0.068305\n",
      "epoch 23/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 23/300 For observation only - Average Estimation Loss in Target domain: 0.004208\n",
      "epoch 23/300 (Val) Weighted Total Loss: 0.007470\n",
      "epoch 23/300 (Val) Average Estimation Loss (mean): 0.006903\n",
      "epoch 23/300 (Val) Average Estimation Loss (Source): 0.007586\n",
      "epoch 23/300 (Val) GAN Discriminator Loss: 0.113363\n",
      "epoch 23/300 (Val) NMSE (Source): 0.006930, NMSE (Target): 0.005768, NMSE (Mean): 0.006349\n",
      "epoch 23/300 (Val) Domain Discriminator Accuracy (Average): 0.5433\n",
      "Time 275.1067586799618 seconds\n",
      "epoch 24/300 Average Training Loss: 0.003677\n",
      "epoch 24/300 Average Estimation Loss (in Source domain): 0.003848\n",
      "epoch 24/300 Average Disc Loss (in Source domain): 0.062765\n",
      "epoch 24/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 24/300 For observation only - Average Estimation Loss in Target domain: 0.004000\n",
      "epoch 24/300 (Val) Weighted Total Loss: 0.007158\n",
      "epoch 24/300 (Val) Average Estimation Loss (mean): 0.006656\n",
      "epoch 24/300 (Val) Average Estimation Loss (Source): 0.007468\n",
      "epoch 24/300 (Val) GAN Discriminator Loss: 0.100315\n",
      "epoch 24/300 (Val) NMSE (Source): 0.006768, NMSE (Target): 0.005403, NMSE (Mean): 0.006085\n",
      "epoch 24/300 (Val) Domain Discriminator Accuracy (Average): 0.5337\n",
      "Time 368.09735153988004 seconds\n",
      "epoch 25/300 Average Training Loss: 0.003549\n",
      "epoch 25/300 Average Estimation Loss (in Source domain): 0.003727\n",
      "epoch 25/300 Average Disc Loss (in Source domain): 0.058851\n",
      "epoch 25/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 25/300 For observation only - Average Estimation Loss in Target domain: 0.003920\n",
      "epoch 25/300 (Val) Weighted Total Loss: 0.007389\n",
      "epoch 25/300 (Val) Average Estimation Loss (mean): 0.006853\n",
      "epoch 25/300 (Val) Average Estimation Loss (Source): 0.007838\n",
      "epoch 25/300 (Val) GAN Discriminator Loss: 0.107236\n",
      "epoch 25/300 (Val) NMSE (Source): 0.007147, NMSE (Target): 0.005412, NMSE (Mean): 0.006280\n",
      "epoch 25/300 (Val) Domain Discriminator Accuracy (Average): 0.5168\n",
      "Time 460.5414882588666 seconds\n",
      "epoch 26/300 Average Training Loss: 0.003353\n",
      "epoch 26/300 Average Estimation Loss (in Source domain): 0.003526\n",
      "epoch 26/300 Average Disc Loss (in Source domain): 0.064457\n",
      "epoch 26/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 26/300 For observation only - Average Estimation Loss in Target domain: 0.003818\n",
      "epoch 26/300 (Val) Weighted Total Loss: 0.007431\n",
      "epoch 26/300 (Val) Average Estimation Loss (mean): 0.006799\n",
      "epoch 26/300 (Val) Average Estimation Loss (Source): 0.007886\n",
      "epoch 26/300 (Val) GAN Discriminator Loss: 0.126463\n",
      "epoch 26/300 (Val) NMSE (Source): 0.007148, NMSE (Target): 0.005239, NMSE (Mean): 0.006193\n",
      "epoch 26/300 (Val) Domain Discriminator Accuracy (Average): 0.5096\n",
      "Time 552.4700700228568 seconds\n",
      "epoch 27/300 Average Training Loss: 0.003176\n",
      "epoch 27/300 Average Estimation Loss (in Source domain): 0.003373\n",
      "epoch 27/300 Average Disc Loss (in Source domain): 0.050129\n",
      "epoch 27/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 27/300 For observation only - Average Estimation Loss in Target domain: 0.003723\n",
      "epoch 27/300 (Val) Weighted Total Loss: 0.007336\n",
      "epoch 27/300 (Val) Average Estimation Loss (mean): 0.006818\n",
      "epoch 27/300 (Val) Average Estimation Loss (Source): 0.007955\n",
      "epoch 27/300 (Val) GAN Discriminator Loss: 0.103646\n",
      "epoch 27/300 (Val) NMSE (Source): 0.007181, NMSE (Target): 0.005223, NMSE (Mean): 0.006202\n",
      "epoch 27/300 (Val) Domain Discriminator Accuracy (Average): 0.5000\n",
      "Time 643.9562661128584 seconds\n",
      "epoch 28/300 Average Training Loss: 0.002960\n",
      "epoch 28/300 Average Estimation Loss (in Source domain): 0.003161\n",
      "epoch 28/300 Average Disc Loss (in Source domain): 0.052784\n",
      "epoch 28/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 28/300 For observation only - Average Estimation Loss in Target domain: 0.003549\n",
      "epoch 28/300 (Val) Weighted Total Loss: 0.008572\n",
      "epoch 28/300 (Val) Average Estimation Loss (mean): 0.007956\n",
      "epoch 28/300 (Val) Average Estimation Loss (Source): 0.009212\n",
      "epoch 28/300 (Val) GAN Discriminator Loss: 0.123265\n",
      "epoch 28/300 (Val) NMSE (Source): 0.008222, NMSE (Target): 0.006032, NMSE (Mean): 0.007127\n",
      "epoch 28/300 (Val) Domain Discriminator Accuracy (Average): 0.4856\n",
      "Time 735.8676141549367 seconds\n",
      "epoch 29/300 Average Training Loss: 0.002921\n",
      "epoch 29/300 Average Estimation Loss (in Source domain): 0.003137\n",
      "epoch 29/300 Average Disc Loss (in Source domain): 0.046194\n",
      "epoch 29/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 29/300 For observation only - Average Estimation Loss in Target domain: 0.003535\n",
      "epoch 29/300 (Val) Weighted Total Loss: 0.008039\n",
      "epoch 29/300 (Val) Average Estimation Loss (mean): 0.007525\n",
      "epoch 29/300 (Val) Average Estimation Loss (Source): 0.008828\n",
      "epoch 29/300 (Val) GAN Discriminator Loss: 0.102656\n",
      "epoch 29/300 (Val) NMSE (Source): 0.007827, NMSE (Target): 0.005587, NMSE (Mean): 0.006707\n",
      "epoch 29/300 (Val) Domain Discriminator Accuracy (Average): 0.4976\n",
      "Time 829.3251493969001 seconds\n",
      "epoch 30/300 Average Training Loss: 0.002790\n",
      "epoch 30/300 Average Estimation Loss (in Source domain): 0.002998\n",
      "epoch 30/300 Average Disc Loss (in Source domain): 0.046638\n",
      "epoch 30/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 30/300 For observation only - Average Estimation Loss in Target domain: 0.003480\n",
      "epoch 30/300 (Val) Weighted Total Loss: 0.007834\n",
      "epoch 30/300 (Val) Average Estimation Loss (mean): 0.007476\n",
      "epoch 30/300 (Val) Average Estimation Loss (Source): 0.008811\n",
      "epoch 30/300 (Val) GAN Discriminator Loss: 0.071671\n",
      "epoch 30/300 (Val) NMSE (Source): 0.007882, NMSE (Target): 0.005555, NMSE (Mean): 0.006718\n",
      "epoch 30/300 (Val) Domain Discriminator Accuracy (Average): 0.5000\n",
      "Time 921.9242529128678 seconds\n",
      "epoch 31/300 Average Training Loss: 0.002708\n",
      "epoch 31/300 Average Estimation Loss (in Source domain): 0.002934\n",
      "epoch 31/300 Average Disc Loss (in Source domain): 0.044783\n",
      "epoch 31/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 31/300 For observation only - Average Estimation Loss in Target domain: 0.003410\n",
      "epoch 31/300 (Val) Weighted Total Loss: 0.006616\n",
      "epoch 31/300 (Val) Average Estimation Loss (mean): 0.006143\n",
      "epoch 31/300 (Val) Average Estimation Loss (Source): 0.007456\n",
      "epoch 31/300 (Val) GAN Discriminator Loss: 0.094739\n",
      "epoch 31/300 (Val) NMSE (Source): 0.006652, NMSE (Target): 0.004401, NMSE (Mean): 0.005527\n",
      "epoch 31/300 (Val) Domain Discriminator Accuracy (Average): 0.5120\n",
      "✅ Checkpoint saved at epoch 31: /home/thien/Code/H_predict_UDA/H_predict_Sionna/Domain_Adaptation/Domain_Adversarial/model/GAN_calcu/ver1_/GAN_linear/model//epoch_31\n",
      "Time 1014.1530635620002 seconds\n",
      "epoch 32/300 Average Training Loss: 0.002626\n",
      "epoch 32/300 Average Estimation Loss (in Source domain): 0.002875\n",
      "epoch 32/300 Average Disc Loss (in Source domain): 0.043272\n",
      "epoch 32/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 32/300 For observation only - Average Estimation Loss in Target domain: 0.003368\n",
      "epoch 32/300 (Val) Weighted Total Loss: 0.007222\n",
      "epoch 32/300 (Val) Average Estimation Loss (mean): 0.006825\n",
      "epoch 32/300 (Val) Average Estimation Loss (Source): 0.008246\n",
      "epoch 32/300 (Val) GAN Discriminator Loss: 0.079346\n",
      "epoch 32/300 (Val) NMSE (Source): 0.007423, NMSE (Target): 0.004965, NMSE (Mean): 0.006194\n",
      "epoch 32/300 (Val) Domain Discriminator Accuracy (Average): 0.5264\n",
      "Time 1106.1218171168584 seconds\n",
      "epoch 33/300 Average Training Loss: 0.002545\n",
      "epoch 33/300 Average Estimation Loss (in Source domain): 0.002769\n",
      "epoch 33/300 Average Disc Loss (in Source domain): 0.044757\n",
      "epoch 33/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 33/300 For observation only - Average Estimation Loss in Target domain: 0.003343\n",
      "epoch 33/300 (Val) Weighted Total Loss: 0.007308\n",
      "epoch 33/300 (Val) Average Estimation Loss (mean): 0.006880\n",
      "epoch 33/300 (Val) Average Estimation Loss (Source): 0.008306\n",
      "epoch 33/300 (Val) GAN Discriminator Loss: 0.085722\n",
      "epoch 33/300 (Val) NMSE (Source): 0.007464, NMSE (Target): 0.004971, NMSE (Mean): 0.006217\n",
      "epoch 33/300 (Val) Domain Discriminator Accuracy (Average): 0.5168\n",
      "Time 1198.5630193438847 seconds\n",
      "epoch 34/300 Average Training Loss: 0.002604\n",
      "epoch 34/300 Average Estimation Loss (in Source domain): 0.002824\n",
      "epoch 34/300 Average Disc Loss (in Source domain): 0.040334\n",
      "epoch 34/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 34/300 For observation only - Average Estimation Loss in Target domain: 0.003306\n",
      "epoch 34/300 (Val) Weighted Total Loss: 0.008104\n",
      "epoch 34/300 (Val) Average Estimation Loss (mean): 0.007777\n",
      "epoch 34/300 (Val) Average Estimation Loss (Source): 0.009309\n",
      "epoch 34/300 (Val) GAN Discriminator Loss: 0.065469\n",
      "epoch 34/300 (Val) NMSE (Source): 0.008436, NMSE (Target): 0.005714, NMSE (Mean): 0.007075\n",
      "epoch 34/300 (Val) Domain Discriminator Accuracy (Average): 0.5144\n",
      "Time 1290.9034128040075 seconds\n",
      "epoch 35/300 Average Training Loss: 0.002475\n",
      "epoch 35/300 Average Estimation Loss (in Source domain): 0.002675\n",
      "epoch 35/300 Average Disc Loss (in Source domain): 0.039229\n",
      "epoch 35/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 35/300 For observation only - Average Estimation Loss in Target domain: 0.003283\n",
      "epoch 35/300 (Val) Weighted Total Loss: 0.008476\n",
      "epoch 35/300 (Val) Average Estimation Loss (mean): 0.008063\n",
      "epoch 35/300 (Val) Average Estimation Loss (Source): 0.009644\n",
      "epoch 35/300 (Val) GAN Discriminator Loss: 0.082560\n",
      "epoch 35/300 (Val) NMSE (Source): 0.008821, NMSE (Target): 0.005991, NMSE (Mean): 0.007406\n",
      "epoch 35/300 (Val) Domain Discriminator Accuracy (Average): 0.5072\n",
      "Time 1383.8998241450172 seconds\n",
      "epoch 36/300 Average Training Loss: 0.002394\n",
      "epoch 36/300 Average Estimation Loss (in Source domain): 0.002601\n",
      "epoch 36/300 Average Disc Loss (in Source domain): 0.040131\n",
      "epoch 36/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 36/300 For observation only - Average Estimation Loss in Target domain: 0.003237\n",
      "epoch 36/300 (Val) Weighted Total Loss: 0.011306\n",
      "epoch 36/300 (Val) Average Estimation Loss (mean): 0.010968\n",
      "epoch 36/300 (Val) Average Estimation Loss (Source): 0.012464\n",
      "epoch 36/300 (Val) GAN Discriminator Loss: 0.067521\n",
      "epoch 36/300 (Val) NMSE (Source): 0.011407, NMSE (Target): 0.008688, NMSE (Mean): 0.010048\n",
      "epoch 36/300 (Val) Domain Discriminator Accuracy (Average): 0.5240\n",
      "Time 1476.672219075961 seconds\n",
      "epoch 37/300 Average Training Loss: 0.002364\n",
      "epoch 37/300 Average Estimation Loss (in Source domain): 0.002561\n",
      "epoch 37/300 Average Disc Loss (in Source domain): 0.039765\n",
      "epoch 37/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 37/300 For observation only - Average Estimation Loss in Target domain: 0.003198\n",
      "epoch 37/300 (Val) Weighted Total Loss: 0.007417\n",
      "epoch 37/300 (Val) Average Estimation Loss (mean): 0.007070\n",
      "epoch 37/300 (Val) Average Estimation Loss (Source): 0.008569\n",
      "epoch 37/300 (Val) GAN Discriminator Loss: 0.069331\n",
      "epoch 37/300 (Val) NMSE (Source): 0.007940, NMSE (Target): 0.005174, NMSE (Mean): 0.006557\n",
      "epoch 37/300 (Val) Domain Discriminator Accuracy (Average): 0.5096\n",
      "Time 1569.0705194019247 seconds\n",
      "epoch 38/300 Average Training Loss: 0.002390\n",
      "epoch 38/300 Average Estimation Loss (in Source domain): 0.002532\n",
      "epoch 38/300 Average Disc Loss (in Source domain): 0.037739\n",
      "epoch 38/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 38/300 For observation only - Average Estimation Loss in Target domain: 0.003126\n",
      "epoch 38/300 (Val) Weighted Total Loss: 0.007732\n",
      "epoch 38/300 (Val) Average Estimation Loss (mean): 0.007356\n",
      "epoch 38/300 (Val) Average Estimation Loss (Source): 0.008911\n",
      "epoch 38/300 (Val) GAN Discriminator Loss: 0.075203\n",
      "epoch 38/300 (Val) NMSE (Source): 0.008303, NMSE (Target): 0.005396, NMSE (Mean): 0.006850\n",
      "epoch 38/300 (Val) Domain Discriminator Accuracy (Average): 0.5000\n",
      "Time 1661.3411916189361 seconds\n",
      "epoch 39/300 Average Training Loss: 0.002275\n",
      "epoch 39/300 Average Estimation Loss (in Source domain): 0.002391\n",
      "epoch 39/300 Average Disc Loss (in Source domain): 0.037623\n",
      "epoch 39/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 39/300 For observation only - Average Estimation Loss in Target domain: 0.003077\n",
      "epoch 39/300 (Val) Weighted Total Loss: 0.007903\n",
      "epoch 39/300 (Val) Average Estimation Loss (mean): 0.007585\n",
      "epoch 39/300 (Val) Average Estimation Loss (Source): 0.009276\n",
      "epoch 39/300 (Val) GAN Discriminator Loss: 0.063722\n",
      "epoch 39/300 (Val) NMSE (Source): 0.008621, NMSE (Target): 0.005521, NMSE (Mean): 0.007071\n",
      "epoch 39/300 (Val) Domain Discriminator Accuracy (Average): 0.5192\n",
      "Time 1754.096037272131 seconds\n",
      "epoch 40/300 Average Training Loss: 0.002317\n",
      "epoch 40/300 Average Estimation Loss (in Source domain): 0.002464\n",
      "epoch 40/300 Average Disc Loss (in Source domain): 0.036455\n",
      "epoch 40/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 40/300 For observation only - Average Estimation Loss in Target domain: 0.003131\n",
      "epoch 40/300 (Val) Weighted Total Loss: 0.007430\n",
      "epoch 40/300 (Val) Average Estimation Loss (mean): 0.007110\n",
      "epoch 40/300 (Val) Average Estimation Loss (Source): 0.008804\n",
      "epoch 40/300 (Val) GAN Discriminator Loss: 0.064022\n",
      "epoch 40/300 (Val) NMSE (Source): 0.008209, NMSE (Target): 0.005092, NMSE (Mean): 0.006651\n",
      "epoch 40/300 (Val) Domain Discriminator Accuracy (Average): 0.5048\n",
      "Time 1846.7170962558594 seconds\n",
      "epoch 41/300 Average Training Loss: 0.002182\n",
      "epoch 41/300 Average Estimation Loss (in Source domain): 0.002324\n",
      "epoch 41/300 Average Disc Loss (in Source domain): 0.036599\n",
      "epoch 41/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 41/300 For observation only - Average Estimation Loss in Target domain: 0.003001\n",
      "epoch 41/300 (Val) Weighted Total Loss: 0.008116\n",
      "epoch 41/300 (Val) Average Estimation Loss (mean): 0.007774\n",
      "epoch 41/300 (Val) Average Estimation Loss (Source): 0.009484\n",
      "epoch 41/300 (Val) GAN Discriminator Loss: 0.068281\n",
      "epoch 41/300 (Val) NMSE (Source): 0.008776, NMSE (Target): 0.005611, NMSE (Mean): 0.007194\n",
      "epoch 41/300 (Val) Domain Discriminator Accuracy (Average): 0.4928\n",
      "✅ Checkpoint saved at epoch 41: /home/thien/Code/H_predict_UDA/H_predict_Sionna/Domain_Adaptation/Domain_Adversarial/model/GAN_calcu/ver1_/GAN_linear/model//epoch_41\n",
      "Time 1940.3664517321158 seconds\n",
      "epoch 42/300 Average Training Loss: 0.002196\n",
      "epoch 42/300 Average Estimation Loss (in Source domain): 0.002347\n",
      "epoch 42/300 Average Disc Loss (in Source domain): 0.035795\n",
      "epoch 42/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 42/300 For observation only - Average Estimation Loss in Target domain: 0.003050\n",
      "epoch 42/300 (Val) Weighted Total Loss: 0.007755\n",
      "epoch 42/300 (Val) Average Estimation Loss (mean): 0.007272\n",
      "epoch 42/300 (Val) Average Estimation Loss (Source): 0.009184\n",
      "epoch 42/300 (Val) GAN Discriminator Loss: 0.096612\n",
      "epoch 42/300 (Val) NMSE (Source): 0.008255, NMSE (Target): 0.004869, NMSE (Mean): 0.006562\n",
      "epoch 42/300 (Val) Domain Discriminator Accuracy (Average): 0.4952\n",
      "Time 2032.9858512219507 seconds\n",
      "epoch 43/300 Average Training Loss: 0.002226\n",
      "epoch 43/300 Average Estimation Loss (in Source domain): 0.002381\n",
      "epoch 43/300 Average Disc Loss (in Source domain): 0.032586\n",
      "epoch 43/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 43/300 For observation only - Average Estimation Loss in Target domain: 0.003006\n",
      "epoch 43/300 (Val) Weighted Total Loss: 0.007428\n",
      "epoch 43/300 (Val) Average Estimation Loss (mean): 0.007110\n",
      "epoch 43/300 (Val) Average Estimation Loss (Source): 0.009170\n",
      "epoch 43/300 (Val) GAN Discriminator Loss: 0.063747\n",
      "epoch 43/300 (Val) NMSE (Source): 0.008255, NMSE (Target): 0.004585, NMSE (Mean): 0.006420\n",
      "epoch 43/300 (Val) Domain Discriminator Accuracy (Average): 0.4976\n",
      "Time 2125.5344716508407 seconds\n",
      "epoch 44/300 Average Training Loss: 0.002008\n",
      "epoch 44/300 Average Estimation Loss (in Source domain): 0.002172\n",
      "epoch 44/300 Average Disc Loss (in Source domain): 0.036770\n",
      "epoch 44/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 44/300 For observation only - Average Estimation Loss in Target domain: 0.002920\n",
      "epoch 44/300 (Val) Weighted Total Loss: 0.007160\n",
      "epoch 44/300 (Val) Average Estimation Loss (mean): 0.006767\n",
      "epoch 44/300 (Val) Average Estimation Loss (Source): 0.008876\n",
      "epoch 44/300 (Val) GAN Discriminator Loss: 0.078617\n",
      "epoch 44/300 (Val) NMSE (Source): 0.008000, NMSE (Target): 0.004269, NMSE (Mean): 0.006134\n",
      "epoch 44/300 (Val) Domain Discriminator Accuracy (Average): 0.4880\n",
      "Time 2218.1933670828585 seconds\n",
      "epoch 45/300 Average Training Loss: 0.002035\n",
      "epoch 45/300 Average Estimation Loss (in Source domain): 0.002195\n",
      "epoch 45/300 Average Disc Loss (in Source domain): 0.032637\n",
      "epoch 45/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 45/300 For observation only - Average Estimation Loss in Target domain: 0.002983\n",
      "epoch 45/300 (Val) Weighted Total Loss: 0.007439\n",
      "epoch 45/300 (Val) Average Estimation Loss (mean): 0.006999\n",
      "epoch 45/300 (Val) Average Estimation Loss (Source): 0.009103\n",
      "epoch 45/300 (Val) GAN Discriminator Loss: 0.088078\n",
      "epoch 45/300 (Val) NMSE (Source): 0.008213, NMSE (Target): 0.004500, NMSE (Mean): 0.006357\n",
      "epoch 45/300 (Val) Domain Discriminator Accuracy (Average): 0.4856\n",
      "Time 2311.0014265591744 seconds\n",
      "epoch 46/300 Average Training Loss: 0.002126\n",
      "epoch 46/300 Average Estimation Loss (in Source domain): 0.002292\n",
      "epoch 46/300 Average Disc Loss (in Source domain): 0.033950\n",
      "epoch 46/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 46/300 For observation only - Average Estimation Loss in Target domain: 0.002957\n",
      "epoch 46/300 (Val) Weighted Total Loss: 0.007533\n",
      "epoch 46/300 (Val) Average Estimation Loss (mean): 0.007249\n",
      "epoch 46/300 (Val) Average Estimation Loss (Source): 0.009415\n",
      "epoch 46/300 (Val) GAN Discriminator Loss: 0.056873\n",
      "epoch 46/300 (Val) NMSE (Source): 0.008485, NMSE (Target): 0.004699, NMSE (Mean): 0.006592\n",
      "epoch 46/300 (Val) Domain Discriminator Accuracy (Average): 0.4832\n",
      "Time 2402.85106303473 seconds\n",
      "epoch 47/300 Average Training Loss: 0.001934\n",
      "epoch 47/300 Average Estimation Loss (in Source domain): 0.002132\n",
      "epoch 47/300 Average Disc Loss (in Source domain): 0.033197\n",
      "epoch 47/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 47/300 For observation only - Average Estimation Loss in Target domain: 0.002839\n",
      "epoch 47/300 (Val) Weighted Total Loss: 0.009076\n",
      "epoch 47/300 (Val) Average Estimation Loss (mean): 0.008799\n",
      "epoch 47/300 (Val) Average Estimation Loss (Source): 0.010997\n",
      "epoch 47/300 (Val) GAN Discriminator Loss: 0.055481\n",
      "epoch 47/300 (Val) NMSE (Source): 0.010016, NMSE (Target): 0.006205, NMSE (Mean): 0.008111\n",
      "epoch 47/300 (Val) Domain Discriminator Accuracy (Average): 0.4832\n",
      "Time 2495.9539308201056 seconds\n",
      "epoch 48/300 Average Training Loss: 0.001957\n",
      "epoch 48/300 Average Estimation Loss (in Source domain): 0.002146\n",
      "epoch 48/300 Average Disc Loss (in Source domain): 0.031610\n",
      "epoch 48/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 48/300 For observation only - Average Estimation Loss in Target domain: 0.002855\n",
      "epoch 48/300 (Val) Weighted Total Loss: 0.007981\n",
      "epoch 48/300 (Val) Average Estimation Loss (mean): 0.007456\n",
      "epoch 48/300 (Val) Average Estimation Loss (Source): 0.009574\n",
      "epoch 48/300 (Val) GAN Discriminator Loss: 0.104952\n",
      "epoch 48/300 (Val) NMSE (Source): 0.008712, NMSE (Target): 0.004954, NMSE (Mean): 0.006833\n",
      "epoch 48/300 (Val) Domain Discriminator Accuracy (Average): 0.5048\n",
      "Time 2589.302621935727 seconds\n",
      "epoch 49/300 Average Training Loss: 0.001948\n",
      "epoch 49/300 Average Estimation Loss (in Source domain): 0.002133\n",
      "epoch 49/300 Average Disc Loss (in Source domain): 0.033133\n",
      "epoch 49/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 49/300 For observation only - Average Estimation Loss in Target domain: 0.002860\n",
      "epoch 49/300 (Val) Weighted Total Loss: 0.007727\n",
      "epoch 49/300 (Val) Average Estimation Loss (mean): 0.007273\n",
      "epoch 49/300 (Val) Average Estimation Loss (Source): 0.009458\n",
      "epoch 49/300 (Val) GAN Discriminator Loss: 0.090930\n",
      "epoch 49/300 (Val) NMSE (Source): 0.008542, NMSE (Target): 0.004691, NMSE (Mean): 0.006617\n",
      "epoch 49/300 (Val) Domain Discriminator Accuracy (Average): 0.4952\n",
      "Time 2682.9987990821246 seconds\n",
      "epoch 50/300 Average Training Loss: 0.001889\n",
      "epoch 50/300 Average Estimation Loss (in Source domain): 0.002079\n",
      "epoch 50/300 Average Disc Loss (in Source domain): 0.030432\n",
      "epoch 50/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 50/300 For observation only - Average Estimation Loss in Target domain: 0.002856\n",
      "epoch 50/300 (Val) Weighted Total Loss: 0.008370\n",
      "epoch 50/300 (Val) Average Estimation Loss (mean): 0.008048\n",
      "epoch 50/300 (Val) Average Estimation Loss (Source): 0.010199\n",
      "epoch 50/300 (Val) GAN Discriminator Loss: 0.064480\n",
      "epoch 50/300 (Val) NMSE (Source): 0.009181, NMSE (Target): 0.005472, NMSE (Mean): 0.007327\n",
      "epoch 50/300 (Val) Domain Discriminator Accuracy (Average): 0.4904\n",
      "Time 2776.237571333768 seconds\n",
      "epoch 51/300 Average Training Loss: 0.001887\n",
      "epoch 51/300 Average Estimation Loss (in Source domain): 0.002097\n",
      "epoch 51/300 Average Disc Loss (in Source domain): 0.031871\n",
      "epoch 51/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 51/300 For observation only - Average Estimation Loss in Target domain: 0.002803\n",
      "epoch 51/300 (Val) Weighted Total Loss: 0.008652\n",
      "epoch 51/300 (Val) Average Estimation Loss (mean): 0.008289\n",
      "epoch 51/300 (Val) Average Estimation Loss (Source): 0.010366\n",
      "epoch 51/300 (Val) GAN Discriminator Loss: 0.072584\n",
      "epoch 51/300 (Val) NMSE (Source): 0.009348, NMSE (Target): 0.005775, NMSE (Mean): 0.007562\n",
      "epoch 51/300 (Val) Domain Discriminator Accuracy (Average): 0.4976\n",
      "✅ Checkpoint saved at epoch 51: /home/thien/Code/H_predict_UDA/H_predict_Sionna/Domain_Adaptation/Domain_Adversarial/model/GAN_calcu/ver1_/GAN_linear/model//epoch_51\n",
      "Time 2870.4465474181343 seconds\n",
      "epoch 52/300 Average Training Loss: 0.001838\n",
      "epoch 52/300 Average Estimation Loss (in Source domain): 0.002028\n",
      "epoch 52/300 Average Disc Loss (in Source domain): 0.028334\n",
      "epoch 52/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 52/300 For observation only - Average Estimation Loss in Target domain: 0.002775\n",
      "epoch 52/300 (Val) Weighted Total Loss: 0.008991\n",
      "epoch 52/300 (Val) Average Estimation Loss (mean): 0.008722\n",
      "epoch 52/300 (Val) Average Estimation Loss (Source): 0.010754\n",
      "epoch 52/300 (Val) GAN Discriminator Loss: 0.053807\n",
      "epoch 52/300 (Val) NMSE (Source): 0.009783, NMSE (Target): 0.006214, NMSE (Mean): 0.007999\n",
      "epoch 52/300 (Val) Domain Discriminator Accuracy (Average): 0.5144\n",
      "Time 2963.2057287760545 seconds\n",
      "epoch 53/300 Average Training Loss: 0.001791\n",
      "epoch 53/300 Average Estimation Loss (in Source domain): 0.001993\n",
      "epoch 53/300 Average Disc Loss (in Source domain): 0.028716\n",
      "epoch 53/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 53/300 For observation only - Average Estimation Loss in Target domain: 0.002770\n",
      "epoch 53/300 (Val) Weighted Total Loss: 0.008676\n",
      "epoch 53/300 (Val) Average Estimation Loss (mean): 0.008361\n",
      "epoch 53/300 (Val) Average Estimation Loss (Source): 0.010324\n",
      "epoch 53/300 (Val) GAN Discriminator Loss: 0.062988\n",
      "epoch 53/300 (Val) NMSE (Source): 0.009301, NMSE (Target): 0.005907, NMSE (Mean): 0.007604\n",
      "epoch 53/300 (Val) Domain Discriminator Accuracy (Average): 0.4952\n",
      "Time 3056.734402984148 seconds\n",
      "epoch 54/300 Average Training Loss: 0.001753\n",
      "epoch 54/300 Average Estimation Loss (in Source domain): 0.001973\n",
      "epoch 54/300 Average Disc Loss (in Source domain): 0.027882\n",
      "epoch 54/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 54/300 For observation only - Average Estimation Loss in Target domain: 0.002805\n",
      "epoch 54/300 (Val) Weighted Total Loss: 0.009121\n",
      "epoch 54/300 (Val) Average Estimation Loss (mean): 0.008880\n",
      "epoch 54/300 (Val) Average Estimation Loss (Source): 0.010933\n",
      "epoch 54/300 (Val) GAN Discriminator Loss: 0.048203\n",
      "epoch 54/300 (Val) NMSE (Source): 0.009940, NMSE (Target): 0.006365, NMSE (Mean): 0.008152\n",
      "epoch 54/300 (Val) Domain Discriminator Accuracy (Average): 0.4976\n",
      "Time 3150.546227941988 seconds\n",
      "epoch 55/300 Average Training Loss: 0.001712\n",
      "epoch 55/300 Average Estimation Loss (in Source domain): 0.001957\n",
      "epoch 55/300 Average Disc Loss (in Source domain): 0.030982\n",
      "epoch 55/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 55/300 For observation only - Average Estimation Loss in Target domain: 0.002822\n",
      "epoch 55/300 (Val) Weighted Total Loss: 0.009362\n",
      "epoch 55/300 (Val) Average Estimation Loss (mean): 0.008960\n",
      "epoch 55/300 (Val) Average Estimation Loss (Source): 0.010928\n",
      "epoch 55/300 (Val) GAN Discriminator Loss: 0.080493\n",
      "epoch 55/300 (Val) NMSE (Source): 0.009892, NMSE (Target): 0.006500, NMSE (Mean): 0.008196\n",
      "epoch 55/300 (Val) Domain Discriminator Accuracy (Average): 0.4928\n",
      "Time 3243.9874847151805 seconds\n",
      "epoch 56/300 Average Training Loss: 0.001736\n",
      "epoch 56/300 Average Estimation Loss (in Source domain): 0.001989\n",
      "epoch 56/300 Average Disc Loss (in Source domain): 0.027427\n",
      "epoch 56/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 56/300 For observation only - Average Estimation Loss in Target domain: 0.002855\n",
      "epoch 56/300 (Val) Weighted Total Loss: 0.009531\n",
      "epoch 56/300 (Val) Average Estimation Loss (mean): 0.009115\n",
      "epoch 56/300 (Val) Average Estimation Loss (Source): 0.011111\n",
      "epoch 56/300 (Val) GAN Discriminator Loss: 0.083040\n",
      "epoch 56/300 (Val) NMSE (Source): 0.010166, NMSE (Target): 0.006663, NMSE (Mean): 0.008415\n",
      "epoch 56/300 (Val) Domain Discriminator Accuracy (Average): 0.4880\n",
      "Time 3337.108093731804 seconds\n",
      "epoch 57/300 Average Training Loss: 0.001758\n",
      "epoch 57/300 Average Estimation Loss (in Source domain): 0.002021\n",
      "epoch 57/300 Average Disc Loss (in Source domain): 0.027838\n",
      "epoch 57/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 57/300 For observation only - Average Estimation Loss in Target domain: 0.002816\n",
      "epoch 57/300 (Val) Weighted Total Loss: 0.008508\n",
      "epoch 57/300 (Val) Average Estimation Loss (mean): 0.008173\n",
      "epoch 57/300 (Val) Average Estimation Loss (Source): 0.010316\n",
      "epoch 57/300 (Val) GAN Discriminator Loss: 0.066937\n",
      "epoch 57/300 (Val) NMSE (Source): 0.009324, NMSE (Target): 0.005591, NMSE (Mean): 0.007457\n",
      "epoch 57/300 (Val) Domain Discriminator Accuracy (Average): 0.4880\n",
      "Time 3429.98698206502 seconds\n",
      "epoch 58/300 Average Training Loss: 0.001543\n",
      "epoch 58/300 Average Estimation Loss (in Source domain): 0.001756\n",
      "epoch 58/300 Average Disc Loss (in Source domain): 0.027814\n",
      "epoch 58/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 58/300 For observation only - Average Estimation Loss in Target domain: 0.002655\n",
      "epoch 58/300 (Val) Weighted Total Loss: 0.007753\n",
      "epoch 58/300 (Val) Average Estimation Loss (mean): 0.007474\n",
      "epoch 58/300 (Val) Average Estimation Loss (Source): 0.009664\n",
      "epoch 58/300 (Val) GAN Discriminator Loss: 0.055831\n",
      "epoch 58/300 (Val) NMSE (Source): 0.008698, NMSE (Target): 0.004814, NMSE (Mean): 0.006756\n",
      "epoch 58/300 (Val) Domain Discriminator Accuracy (Average): 0.5000\n",
      "Time 3522.637437131023 seconds\n",
      "epoch 59/300 Average Training Loss: 0.001633\n",
      "epoch 59/300 Average Estimation Loss (in Source domain): 0.001848\n",
      "epoch 59/300 Average Disc Loss (in Source domain): 0.028770\n",
      "epoch 59/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 59/300 For observation only - Average Estimation Loss in Target domain: 0.002693\n",
      "epoch 59/300 (Val) Weighted Total Loss: 0.008201\n",
      "epoch 59/300 (Val) Average Estimation Loss (mean): 0.007587\n",
      "epoch 59/300 (Val) Average Estimation Loss (Source): 0.009961\n",
      "epoch 59/300 (Val) GAN Discriminator Loss: 0.122770\n",
      "epoch 59/300 (Val) NMSE (Source): 0.008865, NMSE (Target): 0.004809, NMSE (Mean): 0.006837\n",
      "epoch 59/300 (Val) Domain Discriminator Accuracy (Average): 0.5048\n",
      "Time 3615.225943970727 seconds\n",
      "epoch 60/300 Average Training Loss: 0.001679\n",
      "epoch 60/300 Average Estimation Loss (in Source domain): 0.001904\n",
      "epoch 60/300 Average Disc Loss (in Source domain): 0.028474\n",
      "epoch 60/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 60/300 For observation only - Average Estimation Loss in Target domain: 0.002681\n",
      "epoch 60/300 (Val) Weighted Total Loss: 0.008472\n",
      "epoch 60/300 (Val) Average Estimation Loss (mean): 0.007819\n",
      "epoch 60/300 (Val) Average Estimation Loss (Source): 0.010369\n",
      "epoch 60/300 (Val) GAN Discriminator Loss: 0.130566\n",
      "epoch 60/300 (Val) NMSE (Source): 0.009234, NMSE (Target): 0.004895, NMSE (Mean): 0.007064\n",
      "epoch 60/300 (Val) Domain Discriminator Accuracy (Average): 0.5096\n",
      "Time 3708.2060756718274 seconds\n",
      "epoch 61/300 Average Training Loss: 0.001601\n",
      "epoch 61/300 Average Estimation Loss (in Source domain): 0.001810\n",
      "epoch 61/300 Average Disc Loss (in Source domain): 0.026441\n",
      "epoch 61/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 61/300 For observation only - Average Estimation Loss in Target domain: 0.002611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thien/Code/H_predict_UDA/H_predict_Sionna/Est_btween_CSIRS/helper/plotfig.py:50: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  plt.figure(figsize=(10, 5))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 61/300 (Val) Weighted Total Loss: 0.007619\n",
      "epoch 61/300 (Val) Average Estimation Loss (mean): 0.007135\n",
      "epoch 61/300 (Val) Average Estimation Loss (Source): 0.009675\n",
      "epoch 61/300 (Val) GAN Discriminator Loss: 0.096745\n",
      "epoch 61/300 (Val) NMSE (Source): 0.008593, NMSE (Target): 0.004244, NMSE (Mean): 0.006418\n",
      "epoch 61/300 (Val) Domain Discriminator Accuracy (Average): 0.5168\n",
      "✅ Checkpoint saved at epoch 61: /home/thien/Code/H_predict_UDA/H_predict_Sionna/Domain_Adaptation/Domain_Adversarial/model/GAN_calcu/ver1_/GAN_linear/model//epoch_61\n",
      "Time 3801.7294353100006 seconds\n",
      "epoch 62/300 Average Training Loss: 0.001615\n",
      "epoch 62/300 Average Estimation Loss (in Source domain): 0.001836\n",
      "epoch 62/300 Average Disc Loss (in Source domain): 0.029377\n",
      "epoch 62/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 62/300 For observation only - Average Estimation Loss in Target domain: 0.002615\n",
      "epoch 62/300 (Val) Weighted Total Loss: 0.007813\n",
      "epoch 62/300 (Val) Average Estimation Loss (mean): 0.007362\n",
      "epoch 62/300 (Val) Average Estimation Loss (Source): 0.009938\n",
      "epoch 62/300 (Val) GAN Discriminator Loss: 0.090166\n",
      "epoch 62/300 (Val) NMSE (Source): 0.008888, NMSE (Target): 0.004438, NMSE (Mean): 0.006663\n",
      "epoch 62/300 (Val) Domain Discriminator Accuracy (Average): 0.5144\n",
      "Time 3895.150733618764 seconds\n",
      "epoch 63/300 Average Training Loss: 0.001458\n",
      "epoch 63/300 Average Estimation Loss (in Source domain): 0.001703\n",
      "epoch 63/300 Average Disc Loss (in Source domain): 0.027048\n",
      "epoch 63/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 63/300 For observation only - Average Estimation Loss in Target domain: 0.002565\n",
      "epoch 63/300 (Val) Weighted Total Loss: 0.007230\n",
      "epoch 63/300 (Val) Average Estimation Loss (mean): 0.006968\n",
      "epoch 63/300 (Val) Average Estimation Loss (Source): 0.009493\n",
      "epoch 63/300 (Val) GAN Discriminator Loss: 0.052396\n",
      "epoch 63/300 (Val) NMSE (Source): 0.008441, NMSE (Target): 0.004111, NMSE (Mean): 0.006276\n",
      "epoch 63/300 (Val) Domain Discriminator Accuracy (Average): 0.5168\n",
      "Time 3987.457921614172 seconds\n",
      "epoch 64/300 Average Training Loss: 0.001456\n",
      "epoch 64/300 Average Estimation Loss (in Source domain): 0.001702\n",
      "epoch 64/300 Average Disc Loss (in Source domain): 0.026031\n",
      "epoch 64/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 64/300 For observation only - Average Estimation Loss in Target domain: 0.002550\n",
      "epoch 64/300 (Val) Weighted Total Loss: 0.007770\n",
      "epoch 64/300 (Val) Average Estimation Loss (mean): 0.007541\n",
      "epoch 64/300 (Val) Average Estimation Loss (Source): 0.010215\n",
      "epoch 64/300 (Val) GAN Discriminator Loss: 0.045947\n",
      "epoch 64/300 (Val) NMSE (Source): 0.009117, NMSE (Target): 0.004431, NMSE (Mean): 0.006774\n",
      "epoch 64/300 (Val) Domain Discriminator Accuracy (Average): 0.4952\n",
      "Time 4080.901539018145 seconds\n",
      "epoch 65/300 Average Training Loss: 0.001515\n",
      "epoch 65/300 Average Estimation Loss (in Source domain): 0.001757\n",
      "epoch 65/300 Average Disc Loss (in Source domain): 0.027793\n",
      "epoch 65/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 65/300 For observation only - Average Estimation Loss in Target domain: 0.002551\n",
      "epoch 65/300 (Val) Weighted Total Loss: 0.008489\n",
      "epoch 65/300 (Val) Average Estimation Loss (mean): 0.007957\n",
      "epoch 65/300 (Val) Average Estimation Loss (Source): 0.010750\n",
      "epoch 65/300 (Val) GAN Discriminator Loss: 0.106257\n",
      "epoch 65/300 (Val) NMSE (Source): 0.009643, NMSE (Target): 0.004743, NMSE (Mean): 0.007193\n",
      "epoch 65/300 (Val) Domain Discriminator Accuracy (Average): 0.5120\n",
      "Time 4174.26326891291 seconds\n",
      "epoch 66/300 Average Training Loss: 0.001371\n",
      "epoch 66/300 Average Estimation Loss (in Source domain): 0.001630\n",
      "epoch 66/300 Average Disc Loss (in Source domain): 0.025807\n",
      "epoch 66/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 66/300 For observation only - Average Estimation Loss in Target domain: 0.002459\n",
      "epoch 66/300 (Val) Weighted Total Loss: 0.008064\n",
      "epoch 66/300 (Val) Average Estimation Loss (mean): 0.007752\n",
      "epoch 66/300 (Val) Average Estimation Loss (Source): 0.010443\n",
      "epoch 66/300 (Val) GAN Discriminator Loss: 0.062380\n",
      "epoch 66/300 (Val) NMSE (Source): 0.009377, NMSE (Target): 0.004611, NMSE (Mean): 0.006994\n",
      "epoch 66/300 (Val) Domain Discriminator Accuracy (Average): 0.5120\n",
      "Time 4267.289036292816 seconds\n",
      "epoch 67/300 Average Training Loss: 0.001334\n",
      "epoch 67/300 Average Estimation Loss (in Source domain): 0.001613\n",
      "epoch 67/300 Average Disc Loss (in Source domain): 0.026209\n",
      "epoch 67/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 67/300 For observation only - Average Estimation Loss in Target domain: 0.002436\n",
      "epoch 67/300 (Val) Weighted Total Loss: 0.007047\n",
      "epoch 67/300 (Val) Average Estimation Loss (mean): 0.006628\n",
      "epoch 67/300 (Val) Average Estimation Loss (Source): 0.009248\n",
      "epoch 67/300 (Val) GAN Discriminator Loss: 0.083903\n",
      "epoch 67/300 (Val) NMSE (Source): 0.008378, NMSE (Target): 0.003711, NMSE (Mean): 0.006044\n",
      "epoch 67/300 (Val) Domain Discriminator Accuracy (Average): 0.4928\n",
      "Time 4359.904733948177 seconds\n",
      "epoch 68/300 Average Training Loss: 0.001336\n",
      "epoch 68/300 Average Estimation Loss (in Source domain): 0.001615\n",
      "epoch 68/300 Average Disc Loss (in Source domain): 0.026339\n",
      "epoch 68/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 68/300 For observation only - Average Estimation Loss in Target domain: 0.002436\n",
      "epoch 68/300 (Val) Weighted Total Loss: 0.007050\n",
      "epoch 68/300 (Val) Average Estimation Loss (mean): 0.006762\n",
      "epoch 68/300 (Val) Average Estimation Loss (Source): 0.009401\n",
      "epoch 68/300 (Val) GAN Discriminator Loss: 0.057620\n",
      "epoch 68/300 (Val) NMSE (Source): 0.008586, NMSE (Target): 0.003845, NMSE (Mean): 0.006216\n",
      "epoch 68/300 (Val) Domain Discriminator Accuracy (Average): 0.5192\n",
      "Time 4454.022400747752 seconds\n",
      "epoch 69/300 Average Training Loss: 0.001231\n",
      "epoch 69/300 Average Estimation Loss (in Source domain): 0.001521\n",
      "epoch 69/300 Average Disc Loss (in Source domain): 0.026587\n",
      "epoch 69/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 69/300 For observation only - Average Estimation Loss in Target domain: 0.002351\n",
      "epoch 69/300 (Val) Weighted Total Loss: 0.007588\n",
      "epoch 69/300 (Val) Average Estimation Loss (mean): 0.007179\n",
      "epoch 69/300 (Val) Average Estimation Loss (Source): 0.009847\n",
      "epoch 69/300 (Val) GAN Discriminator Loss: 0.081947\n",
      "epoch 69/300 (Val) NMSE (Source): 0.008970, NMSE (Target): 0.004158, NMSE (Mean): 0.006564\n",
      "epoch 69/300 (Val) Domain Discriminator Accuracy (Average): 0.5024\n",
      "Time 4546.360924210167 seconds\n",
      "epoch 70/300 Average Training Loss: 0.001273\n",
      "epoch 70/300 Average Estimation Loss (in Source domain): 0.001566\n",
      "epoch 70/300 Average Disc Loss (in Source domain): 0.027121\n",
      "epoch 70/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 70/300 For observation only - Average Estimation Loss in Target domain: 0.002389\n",
      "epoch 70/300 (Val) Weighted Total Loss: 0.007837\n",
      "epoch 70/300 (Val) Average Estimation Loss (mean): 0.007561\n",
      "epoch 70/300 (Val) Average Estimation Loss (Source): 0.010221\n",
      "epoch 70/300 (Val) GAN Discriminator Loss: 0.055224\n",
      "epoch 70/300 (Val) NMSE (Source): 0.009421, NMSE (Target): 0.004679, NMSE (Mean): 0.007050\n",
      "epoch 70/300 (Val) Domain Discriminator Accuracy (Average): 0.5072\n",
      "Time 4639.141072110971 seconds\n",
      "epoch 71/300 Average Training Loss: 0.001229\n",
      "epoch 71/300 Average Estimation Loss (in Source domain): 0.001544\n",
      "epoch 71/300 Average Disc Loss (in Source domain): 0.026128\n",
      "epoch 71/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 71/300 For observation only - Average Estimation Loss in Target domain: 0.002331\n",
      "epoch 71/300 (Val) Weighted Total Loss: 0.007202\n",
      "epoch 71/300 (Val) Average Estimation Loss (mean): 0.006938\n",
      "epoch 71/300 (Val) Average Estimation Loss (Source): 0.009504\n",
      "epoch 71/300 (Val) GAN Discriminator Loss: 0.052787\n",
      "epoch 71/300 (Val) NMSE (Source): 0.008773, NMSE (Target): 0.004151, NMSE (Mean): 0.006462\n",
      "epoch 71/300 (Val) Domain Discriminator Accuracy (Average): 0.5024\n",
      "✅ Checkpoint saved at epoch 71: /home/thien/Code/H_predict_UDA/H_predict_Sionna/Domain_Adaptation/Domain_Adversarial/model/GAN_calcu/ver1_/GAN_linear/model//epoch_71\n",
      "Time 4733.282259905944 seconds\n",
      "epoch 72/300 Average Training Loss: 0.001230\n",
      "epoch 72/300 Average Estimation Loss (in Source domain): 0.001527\n",
      "epoch 72/300 Average Disc Loss (in Source domain): 0.024876\n",
      "epoch 72/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 72/300 For observation only - Average Estimation Loss in Target domain: 0.002315\n",
      "epoch 72/300 (Val) Weighted Total Loss: 0.006812\n",
      "epoch 72/300 (Val) Average Estimation Loss (mean): 0.006542\n",
      "epoch 72/300 (Val) Average Estimation Loss (Source): 0.008895\n",
      "epoch 72/300 (Val) GAN Discriminator Loss: 0.053990\n",
      "epoch 72/300 (Val) NMSE (Source): 0.008145, NMSE (Target): 0.003959, NMSE (Mean): 0.006052\n",
      "epoch 72/300 (Val) Domain Discriminator Accuracy (Average): 0.5144\n",
      "Time 4826.5924961690325 seconds\n",
      "epoch 73/300 Average Training Loss: 0.001261\n",
      "epoch 73/300 Average Estimation Loss (in Source domain): 0.001575\n",
      "epoch 73/300 Average Disc Loss (in Source domain): 0.025204\n",
      "epoch 73/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 73/300 For observation only - Average Estimation Loss in Target domain: 0.002317\n",
      "epoch 73/300 (Val) Weighted Total Loss: 0.007243\n",
      "epoch 73/300 (Val) Average Estimation Loss (mean): 0.006873\n",
      "epoch 73/300 (Val) Average Estimation Loss (Source): 0.009244\n",
      "epoch 73/300 (Val) GAN Discriminator Loss: 0.073943\n",
      "epoch 73/300 (Val) NMSE (Source): 0.008444, NMSE (Target): 0.004178, NMSE (Mean): 0.006311\n",
      "epoch 73/300 (Val) Domain Discriminator Accuracy (Average): 0.5312\n",
      "Time 4919.7583445280325 seconds\n",
      "epoch 74/300 Average Training Loss: 0.001148\n",
      "epoch 74/300 Average Estimation Loss (in Source domain): 0.001491\n",
      "epoch 74/300 Average Disc Loss (in Source domain): 0.024724\n",
      "epoch 74/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 74/300 For observation only - Average Estimation Loss in Target domain: 0.002242\n",
      "epoch 74/300 (Val) Weighted Total Loss: 0.007520\n",
      "epoch 74/300 (Val) Average Estimation Loss (mean): 0.007187\n",
      "epoch 74/300 (Val) Average Estimation Loss (Source): 0.009575\n",
      "epoch 74/300 (Val) GAN Discriminator Loss: 0.066639\n",
      "epoch 74/300 (Val) NMSE (Source): 0.008623, NMSE (Target): 0.004332, NMSE (Mean): 0.006477\n",
      "epoch 74/300 (Val) Domain Discriminator Accuracy (Average): 0.5120\n",
      "Time 5013.003923067125 seconds\n",
      "epoch 75/300 Average Training Loss: 0.001179\n",
      "epoch 75/300 Average Estimation Loss (in Source domain): 0.001478\n",
      "epoch 75/300 Average Disc Loss (in Source domain): 0.023930\n",
      "epoch 75/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 75/300 For observation only - Average Estimation Loss in Target domain: 0.002280\n",
      "epoch 75/300 (Val) Weighted Total Loss: 0.008047\n",
      "epoch 75/300 (Val) Average Estimation Loss (mean): 0.007729\n",
      "epoch 75/300 (Val) Average Estimation Loss (Source): 0.010043\n",
      "epoch 75/300 (Val) GAN Discriminator Loss: 0.063569\n",
      "epoch 75/300 (Val) NMSE (Source): 0.008874, NMSE (Target): 0.004806, NMSE (Mean): 0.006840\n",
      "epoch 75/300 (Val) Domain Discriminator Accuracy (Average): 0.4952\n",
      "Time 5106.038973070914 seconds\n",
      "epoch 76/300 Average Training Loss: 0.001148\n",
      "epoch 76/300 Average Estimation Loss (in Source domain): 0.001460\n",
      "epoch 76/300 Average Disc Loss (in Source domain): 0.024080\n",
      "epoch 76/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 76/300 For observation only - Average Estimation Loss in Target domain: 0.002272\n",
      "epoch 76/300 (Val) Weighted Total Loss: 0.006660\n",
      "epoch 76/300 (Val) Average Estimation Loss (mean): 0.006302\n",
      "epoch 76/300 (Val) Average Estimation Loss (Source): 0.008730\n",
      "epoch 76/300 (Val) GAN Discriminator Loss: 0.071777\n",
      "epoch 76/300 (Val) NMSE (Source): 0.007801, NMSE (Target): 0.003533, NMSE (Mean): 0.005667\n",
      "epoch 76/300 (Val) Domain Discriminator Accuracy (Average): 0.5072\n",
      "Time 5199.438730246155 seconds\n",
      "epoch 77/300 Average Training Loss: 0.001153\n",
      "epoch 77/300 Average Estimation Loss (in Source domain): 0.001473\n",
      "epoch 77/300 Average Disc Loss (in Source domain): 0.024055\n",
      "epoch 77/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 77/300 For observation only - Average Estimation Loss in Target domain: 0.002251\n",
      "epoch 77/300 (Val) Weighted Total Loss: 0.006739\n",
      "epoch 77/300 (Val) Average Estimation Loss (mean): 0.006410\n",
      "epoch 77/300 (Val) Average Estimation Loss (Source): 0.008834\n",
      "epoch 77/300 (Val) GAN Discriminator Loss: 0.065819\n",
      "epoch 77/300 (Val) NMSE (Source): 0.007814, NMSE (Target): 0.003615, NMSE (Mean): 0.005715\n",
      "epoch 77/300 (Val) Domain Discriminator Accuracy (Average): 0.5096\n",
      "Time 5291.765001951018 seconds\n",
      "epoch 78/300 Average Training Loss: 0.001062\n",
      "epoch 78/300 Average Estimation Loss (in Source domain): 0.001392\n",
      "epoch 78/300 Average Disc Loss (in Source domain): 0.023027\n",
      "epoch 78/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 78/300 For observation only - Average Estimation Loss in Target domain: 0.002215\n",
      "epoch 78/300 (Val) Weighted Total Loss: 0.007963\n",
      "epoch 78/300 (Val) Average Estimation Loss (mean): 0.007591\n",
      "epoch 78/300 (Val) Average Estimation Loss (Source): 0.010144\n",
      "epoch 78/300 (Val) GAN Discriminator Loss: 0.074282\n",
      "epoch 78/300 (Val) NMSE (Source): 0.009023, NMSE (Target): 0.004620, NMSE (Mean): 0.006821\n",
      "epoch 78/300 (Val) Domain Discriminator Accuracy (Average): 0.5096\n",
      "Time 5383.999519871781 seconds\n",
      "epoch 79/300 Average Training Loss: 0.001136\n",
      "epoch 79/300 Average Estimation Loss (in Source domain): 0.001437\n",
      "epoch 79/300 Average Disc Loss (in Source domain): 0.025115\n",
      "epoch 79/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 79/300 For observation only - Average Estimation Loss in Target domain: 0.002246\n",
      "epoch 79/300 (Val) Weighted Total Loss: 0.007202\n",
      "epoch 79/300 (Val) Average Estimation Loss (mean): 0.006788\n",
      "epoch 79/300 (Val) Average Estimation Loss (Source): 0.009274\n",
      "epoch 79/300 (Val) GAN Discriminator Loss: 0.082698\n",
      "epoch 79/300 (Val) NMSE (Source): 0.008324, NMSE (Target): 0.004097, NMSE (Mean): 0.006211\n",
      "epoch 79/300 (Val) Domain Discriminator Accuracy (Average): 0.5000\n",
      "Time 5476.182794900844 seconds\n",
      "epoch 80/300 Average Training Loss: 0.001090\n",
      "epoch 80/300 Average Estimation Loss (in Source domain): 0.001434\n",
      "epoch 80/300 Average Disc Loss (in Source domain): 0.023593\n",
      "epoch 80/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 80/300 For observation only - Average Estimation Loss in Target domain: 0.002213\n",
      "epoch 80/300 (Val) Weighted Total Loss: 0.006930\n",
      "epoch 80/300 (Val) Average Estimation Loss (mean): 0.006436\n",
      "epoch 80/300 (Val) Average Estimation Loss (Source): 0.008967\n",
      "epoch 80/300 (Val) GAN Discriminator Loss: 0.098850\n",
      "epoch 80/300 (Val) NMSE (Source): 0.007991, NMSE (Target): 0.003663, NMSE (Mean): 0.005827\n",
      "epoch 80/300 (Val) Domain Discriminator Accuracy (Average): 0.5000\n",
      "Time 5569.25814018189 seconds\n",
      "epoch 81/300 Average Training Loss: 0.000989\n",
      "epoch 81/300 Average Estimation Loss (in Source domain): 0.001324\n",
      "epoch 81/300 Average Disc Loss (in Source domain): 0.023056\n",
      "epoch 81/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 81/300 For observation only - Average Estimation Loss in Target domain: 0.002105\n",
      "epoch 81/300 (Val) Weighted Total Loss: 0.006707\n",
      "epoch 81/300 (Val) Average Estimation Loss (mean): 0.006385\n",
      "epoch 81/300 (Val) Average Estimation Loss (Source): 0.008957\n",
      "epoch 81/300 (Val) GAN Discriminator Loss: 0.064435\n",
      "epoch 81/300 (Val) NMSE (Source): 0.008062, NMSE (Target): 0.003567, NMSE (Mean): 0.005814\n",
      "epoch 81/300 (Val) Domain Discriminator Accuracy (Average): 0.5120\n",
      "✅ Checkpoint saved at epoch 81: /home/thien/Code/H_predict_UDA/H_predict_Sionna/Domain_Adaptation/Domain_Adversarial/model/GAN_calcu/ver1_/GAN_linear/model//epoch_81\n",
      "Time 5662.051753967768 seconds\n",
      "epoch 82/300 Average Training Loss: 0.001042\n",
      "epoch 82/300 Average Estimation Loss (in Source domain): 0.001395\n",
      "epoch 82/300 Average Disc Loss (in Source domain): 0.023158\n",
      "epoch 82/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 82/300 For observation only - Average Estimation Loss in Target domain: 0.002172\n",
      "epoch 82/300 (Val) Weighted Total Loss: 0.006620\n",
      "epoch 82/300 (Val) Average Estimation Loss (mean): 0.006388\n",
      "epoch 82/300 (Val) Average Estimation Loss (Source): 0.009051\n",
      "epoch 82/300 (Val) GAN Discriminator Loss: 0.046486\n",
      "epoch 82/300 (Val) NMSE (Source): 0.008219, NMSE (Target): 0.003436, NMSE (Mean): 0.005828\n",
      "epoch 82/300 (Val) Domain Discriminator Accuracy (Average): 0.5024\n",
      "Time 5755.766442292137 seconds\n",
      "epoch 83/300 Average Training Loss: 0.000993\n",
      "epoch 83/300 Average Estimation Loss (in Source domain): 0.001327\n",
      "epoch 83/300 Average Disc Loss (in Source domain): 0.022880\n",
      "epoch 83/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 83/300 For observation only - Average Estimation Loss in Target domain: 0.002149\n",
      "epoch 83/300 (Val) Weighted Total Loss: 0.006868\n",
      "epoch 83/300 (Val) Average Estimation Loss (mean): 0.006611\n",
      "epoch 83/300 (Val) Average Estimation Loss (Source): 0.009369\n",
      "epoch 83/300 (Val) GAN Discriminator Loss: 0.051528\n",
      "epoch 83/300 (Val) NMSE (Source): 0.008557, NMSE (Target): 0.003571, NMSE (Mean): 0.006064\n",
      "epoch 83/300 (Val) Domain Discriminator Accuracy (Average): 0.5120\n",
      "Time 5850.138623845996 seconds\n",
      "epoch 84/300 Average Training Loss: 0.000998\n",
      "epoch 84/300 Average Estimation Loss (in Source domain): 0.001312\n",
      "epoch 84/300 Average Disc Loss (in Source domain): 0.022076\n",
      "epoch 84/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 84/300 For observation only - Average Estimation Loss in Target domain: 0.002166\n",
      "epoch 84/300 (Val) Weighted Total Loss: 0.006876\n",
      "epoch 84/300 (Val) Average Estimation Loss (mean): 0.006509\n",
      "epoch 84/300 (Val) Average Estimation Loss (Source): 0.009153\n",
      "epoch 84/300 (Val) GAN Discriminator Loss: 0.073424\n",
      "epoch 84/300 (Val) NMSE (Source): 0.008356, NMSE (Target): 0.003562, NMSE (Mean): 0.005959\n",
      "epoch 84/300 (Val) Domain Discriminator Accuracy (Average): 0.4952\n",
      "Time 5943.686746291118 seconds\n",
      "epoch 85/300 Average Training Loss: 0.000955\n",
      "epoch 85/300 Average Estimation Loss (in Source domain): 0.001310\n",
      "epoch 85/300 Average Disc Loss (in Source domain): 0.021590\n",
      "epoch 85/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 85/300 For observation only - Average Estimation Loss in Target domain: 0.002176\n",
      "epoch 85/300 (Val) Weighted Total Loss: 0.007011\n",
      "epoch 85/300 (Val) Average Estimation Loss (mean): 0.006759\n",
      "epoch 85/300 (Val) Average Estimation Loss (Source): 0.009618\n",
      "epoch 85/300 (Val) GAN Discriminator Loss: 0.050276\n",
      "epoch 85/300 (Val) NMSE (Source): 0.008804, NMSE (Target): 0.003633, NMSE (Mean): 0.006218\n",
      "epoch 85/300 (Val) Domain Discriminator Accuracy (Average): 0.5096\n",
      "Time 6035.734149639728 seconds\n",
      "epoch 86/300 Average Training Loss: 0.000967\n",
      "epoch 86/300 Average Estimation Loss (in Source domain): 0.001336\n",
      "epoch 86/300 Average Disc Loss (in Source domain): 0.021489\n",
      "epoch 86/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 86/300 For observation only - Average Estimation Loss in Target domain: 0.002167\n",
      "epoch 86/300 (Val) Weighted Total Loss: 0.007738\n",
      "epoch 86/300 (Val) Average Estimation Loss (mean): 0.007376\n",
      "epoch 86/300 (Val) Average Estimation Loss (Source): 0.010196\n",
      "epoch 86/300 (Val) GAN Discriminator Loss: 0.072412\n",
      "epoch 86/300 (Val) NMSE (Source): 0.009428, NMSE (Target): 0.004374, NMSE (Mean): 0.006901\n",
      "epoch 86/300 (Val) Domain Discriminator Accuracy (Average): 0.5000\n",
      "Time 6128.389967040857 seconds\n",
      "epoch 87/300 Average Training Loss: 0.000970\n",
      "epoch 87/300 Average Estimation Loss (in Source domain): 0.001321\n",
      "epoch 87/300 Average Disc Loss (in Source domain): 0.022359\n",
      "epoch 87/300 Average Domain Discrimination Loss: 0.000000\n",
      "epoch 87/300 For observation only - Average Estimation Loss in Target domain: 0.002174\n",
      "epoch 87/300 (Val) Weighted Total Loss: 0.006943\n",
      "epoch 87/300 (Val) Average Estimation Loss (mean): 0.006758\n",
      "epoch 87/300 (Val) Average Estimation Loss (Source): 0.009438\n",
      "epoch 87/300 (Val) GAN Discriminator Loss: 0.037035\n",
      "epoch 87/300 (Val) NMSE (Source): 0.008769, NMSE (Target): 0.003907, NMSE (Mean): 0.006338\n",
      "epoch 87/300 (Val) Domain Discriminator Accuracy (Average): 0.4928\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 121\u001b[39m\n\u001b[32m    118\u001b[39m     return_features = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    120\u001b[39m \u001b[38;5;66;03m##########################\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m train_step_output = \u001b[43mutils_GAN\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_step_wgan_gp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_domain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader_H\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlower_range\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m                        \u001b[49m\u001b[43madv_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43madv_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mest_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mest_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdomain_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdomain_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinear_interp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlinear_interp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    124\u001b[39m train_epoc_loss_est        = train_step_output.avg_epoc_loss_est\n\u001b[32m    125\u001b[39m train_epoc_loss_d          = train_step_output.avg_epoc_loss_d\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/H_predict_UDA/H_predict_Sionna/Domain_Adaptation/Domain_Adversarial/helper/utils_GAN_copy.py:483\u001b[39m, in \u001b[36mtrain_step_wgan_gp\u001b[39m\u001b[34m(model, domain_model, loader_H, loss_fn, optimizers, lower_range, return_features, nsymb, adv_weight, est_weight, domain_weight, linear_interp)\u001b[39m\n\u001b[32m    481\u001b[39m x_src = np.transpose(x_src, (\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m1\u001b[39m))\n\u001b[32m    482\u001b[39m y_src = np.transpose(y_src, (\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m1\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m483\u001b[39m x_scaled_src, x_min_src, x_max_src = \u001b[43mutils_CNN\u001b[49m\u001b[43m.\u001b[49m\u001b[43mminmaxScaler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_src\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlower_range\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlower_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinear_interp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlinear_interp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    484\u001b[39m y_scaled_src, _, _ = utils_CNN.minmaxScaler(y_src, min_pre=x_min_src, max_pre=x_max_src, lower_range=lower_range)\n\u001b[32m    486\u001b[39m \u001b[38;5;66;03m# Preprocess (target)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/H_predict_UDA/H_predict_Sionna/Est_btween_CSIRS/helper/utils.py:103\u001b[39m, in \u001b[36mminmaxScaler\u001b[39m\u001b[34m(x, min_pre, max_pre, lower_range, linear_interp)\u001b[39m\n\u001b[32m    100\u001b[39m     x_min = tf.reduce_min(x_reshaped, axis=\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# [N, 2]\u001b[39;00m\n\u001b[32m    101\u001b[39m     x_max = tf.reduce_max(x_reshaped, axis=\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# [N, 2]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m scale = \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclip_by_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_max\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_min\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1e-8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# avoid divide-by-zero # [N, 2]\u001b[39;00m\n\u001b[32m    105\u001b[39m \u001b[38;5;66;03m# Reshape for broadcasting\u001b[39;00m\n\u001b[32m    106\u001b[39m x_min_broadcast = tf.reshape(x_min, [N, \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/TF_py311/lib/python3.11/site-packages/tensorflow/python/ops/weak_tensor_ops.py:88\u001b[39m, in \u001b[36mweak_tensor_unary_op_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     87\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops.is_auto_dtype_conversion_enabled():\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m   bound_arguments = signature.bind(*args, **kwargs)\n\u001b[32m     90\u001b[39m   bound_arguments.apply_defaults()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/TF_py311/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/TF_py311/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1260\u001b[39m, in \u001b[36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1258\u001b[39m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[32m   1259\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1260\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1261\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[32m   1262\u001b[39m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[32m   1263\u001b[39m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[32m   1264\u001b[39m   result = dispatch(op_dispatch_handler, args, kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/TF_py311/lib/python3.11/site-packages/tensorflow/python/ops/clip_ops.py:111\u001b[39m, in \u001b[36mclip_by_value\u001b[39m\u001b[34m(t, clip_value_min, clip_value_max, name)\u001b[39m\n\u001b[32m    106\u001b[39m values = ops.convert_to_tensor(\n\u001b[32m    107\u001b[39m     t.values \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, indexed_slices.IndexedSlices) \u001b[38;5;28;01melse\u001b[39;00m t,\n\u001b[32m    108\u001b[39m     name=\u001b[33m\"\u001b[39m\u001b[33mt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    110\u001b[39m \u001b[38;5;66;03m# Go through list of tensors, for each value in each tensor clip\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m t_min = \u001b[43mmath_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mminimum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_value_max\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# Assert that the shape is compatible with the initial shape,\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# to prevent unintentional broadcasting.\u001b[39;00m\n\u001b[32m    114\u001b[39m values.shape.assert_is_compatible_with(t_min.shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/TF_py311/lib/python3.11/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142\u001b[39m, in \u001b[36mweak_tensor_binary_op_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    141\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops.is_auto_dtype_conversion_enabled():\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m   bound_arguments = signature.bind(*args, **kwargs)\n\u001b[32m    144\u001b[39m   bound_arguments.apply_defaults()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/TF_py311/lib/python3.11/site-packages/tensorflow/python/ops/gen_math_ops.py:6686\u001b[39m, in \u001b[36mminimum\u001b[39m\u001b[34m(x, y, name)\u001b[39m\n\u001b[32m   6684\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m _result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[32m   6685\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m-> \u001b[39m\u001b[32m6686\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mminimum_eager_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   6687\u001b[39m \u001b[43m      \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6688\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _core._SymbolicException:\n\u001b[32m   6689\u001b[39m   \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/TF_py311/lib/python3.11/site-packages/tensorflow/python/ops/gen_math_ops.py:6731\u001b[39m, in \u001b[36mminimum_eager_fallback\u001b[39m\u001b[34m(x, y, name, ctx)\u001b[39m\n\u001b[32m   6729\u001b[39m _inputs_flat = [x, y]\n\u001b[32m   6730\u001b[39m _attrs = (\u001b[33m\"\u001b[39m\u001b[33mT\u001b[39m\u001b[33m\"\u001b[39m, _attr_T)\n\u001b[32m-> \u001b[39m\u001b[32m6731\u001b[39m _result = \u001b[43m_execute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMinimum\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_inputs_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   6732\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6733\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _execute.must_record_gradient():\n\u001b[32m   6734\u001b[39m   _execute.record_gradient(\n\u001b[32m   6735\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mMinimum\u001b[39m\u001b[33m\"\u001b[39m, _inputs_flat, _attrs, _result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/TF_py311/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for sub_folder in sub_folder_:\n",
    "    print(f\"Processing: {sub_folder}\")\n",
    "    linear_interp = False\n",
    "    if sub_folder == 'GAN_linear':\n",
    "        linear_interp =True # flag to clip values that go beyond the estimated pilot (min, max)\n",
    "    ##\n",
    "    loader_H_true_train_source = class_dict_source[sub_folder].true_train\n",
    "    loader_H_input_train_source = class_dict_source[sub_folder].input_train\n",
    "    loader_H_true_val_source = class_dict_source[sub_folder].true_val\n",
    "    loader_H_input_val_source = class_dict_source[sub_folder].input_val\n",
    "    \n",
    "    loader_H_true_train_target = class_dict_target[sub_folder].true_train\n",
    "    loader_H_input_train_target = class_dict_target[sub_folder].input_train\n",
    "    loader_H_true_val_target = class_dict_target[sub_folder].true_val\n",
    "    loader_H_input_val_target = class_dict_target[sub_folder].input_val\n",
    "    ##\n",
    "    \n",
    "    if not os.path.exists(os.path.dirname(model_path + '/' + sub_folder +'/')):\n",
    "        os.makedirs(os.path.dirname(model_path + '/' + sub_folder + '/'))   # Domain_Adversarial/model/_/ver_/{sub_folder}\n",
    "\n",
    "    \n",
    "    flag = 1 # flag to plot and save H_true\n",
    "    H_to_save = {}          # list to save to .mat file for H\n",
    "    if load_checkpoint==False:\n",
    "        train_loss          = [] # (epoch,1)\n",
    "        train_est_loss      = [] \n",
    "        train_disc_loss     = [] \n",
    "        train_domain_loss   = []\n",
    "        train_est_loss_target = []\n",
    "        #    \n",
    "        val_loss, val_gan_disc_loss, val_domain_disc_loss,\\\n",
    "        val_est_loss_source, val_est_loss_target, val_est_loss,\\\n",
    "        source_acc, target_acc, acc,\\\n",
    "        nmse_val_source, nmse_val_target, nmse_val = [[] for _ in range(12)]\n",
    "        #\n",
    "\n",
    "        model = utils_GAN.GAN(n_subc=312, gen_l2=None, disc_l2=1e-5)  # l2 regularization for generator and discriminator\n",
    "        model_domain = utils_GAN_FiLM.DomainDiscriminator3()\n",
    "        gen_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4, beta_1=0.5, beta_2=0.9)\n",
    "        disc_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, beta_1=0.5, beta_2=0.9)  # WGAN-GP uses Adam optimizer with beta_1=0.5\n",
    "        domain_optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "        ####\n",
    "        optimizer = [gen_optimizer, disc_optimizer, domain_optimizer]\n",
    "        ####\n",
    "    \n",
    "        pad_observe = [] # pad observation over epochs\n",
    "        epoc_pad = []    # epochs that calculating pad (return_features == True)\n",
    "    else:   # load from check_point\n",
    "        model = utils_GAN.GAN(n_subc=312, gen_l2=None, disc_l2=1e-5)\n",
    "        # model.build(input_shape=(16, 312, 14, 2))\n",
    "        dummy_input = tf.random.normal((16, 312, 14, 2))\n",
    "        _ = model(dummy_input)  # This builds the model with proper weights initialization\n",
    "        #\n",
    "        model_domain = utils_GAN_FiLM.DomainDiscriminator3()\n",
    "        # \n",
    "        # Load checkpoint from the epoch we want to continue from (start_epoch-1 because we want to continue FROM start_epoch)\n",
    "        epoch_load = start_epoch - 1  # Load the checkpoint from the previous epoch\n",
    "        \n",
    "        print(f\"Loading checkpoint from epoch {epoch_load+1} to continue training from epoch {start_epoch}...\")\n",
    "\n",
    "        # Load checkpoint (this will also restore optimizers automatically)\n",
    "        gen_optimizer, disc_optimizer, domain_optimizer = utils_GAN.load_checkpoint(\n",
    "            model,\n",
    "            model_path,\n",
    "            sub_folder,\n",
    "            epoch_load,\n",
    "            domain_model=model_domain,\n",
    "            domain_weight=domain_weight  # Use the same domain_weight as current training\n",
    "        )\n",
    "        optimizer = [gen_optimizer, disc_optimizer, domain_optimizer]    \n",
    "        \n",
    "        # Load performance history UP TO start_epoch\n",
    "        loadmat_params = loadmat(f\"{model_path}/{sub_folder}/performance/performance.mat\")\n",
    "        train_loss          = loadmat_params['train_loss'].flatten().tolist()[:start_epoch]\n",
    "        train_est_loss      = loadmat_params['train_est_loss'].flatten().tolist()[:start_epoch]\n",
    "        train_disc_loss     = loadmat_params['train_disc_loss'].flatten().tolist()[:start_epoch]\n",
    "        train_domain_loss   = loadmat_params['train_domain_loss'].flatten().tolist()[:start_epoch]\n",
    "        train_est_loss_target = loadmat_params['train_est_loss_target'].flatten().tolist()[:start_epoch]\n",
    "        #    \n",
    "        val_loss             = loadmat_params['val_loss'].flatten().tolist()[:start_epoch]\n",
    "        val_gan_disc_loss    = loadmat_params['val_gan_disc_loss'].flatten().tolist()[:start_epoch]\n",
    "        val_domain_disc_loss = loadmat_params['val_domain_disc_loss'].flatten().tolist()[:start_epoch]\n",
    "        val_est_loss_source  = loadmat_params['val_est_loss_source'].flatten().tolist()[:start_epoch]\n",
    "        val_est_loss_target  = loadmat_params['val_est_loss_target'].flatten().tolist()[:start_epoch]\n",
    "        val_est_loss         = loadmat_params['val_est_loss'].flatten().tolist()[:start_epoch]\n",
    "        source_acc           = loadmat_params['source_acc'].flatten().tolist()[:start_epoch]\n",
    "        target_acc           = loadmat_params['target_acc'].flatten().tolist()[:start_epoch]\n",
    "        acc                  = loadmat_params['acc'].flatten().tolist()[:start_epoch]\n",
    "        nmse_val_source      = loadmat_params['nmse_val_source'].flatten().tolist()[:start_epoch]\n",
    "        nmse_val_target      = loadmat_params['nmse_val_target'].flatten().tolist()[:start_epoch]\n",
    "        nmse_val             = loadmat_params['nmse_val'].flatten().tolist()[:start_epoch]\n",
    "        pad_observe          = loadmat_params['pad_observe'].flatten().tolist()[:start_epoch]\n",
    "        epoc_pad             = loadmat_params['epoc_pad'].flatten().tolist()[:start_epoch]\n",
    "        \n",
    "        print(f\"Loaded {len(train_loss)} epochs of training history.\")\n",
    "        print(f\"Last loaded training loss: {train_loss[-1] if train_loss else 'No history'}\")\n",
    "        \n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        # ===================== Training =====================\n",
    "        loader_H_true_train_source.reset()\n",
    "        # loader_H_practical_train_source.reset()\n",
    "        loader_H_input_train_source.reset()\n",
    "        loader_H_true_train_target.reset()\n",
    "        # loader_H_practical_train_target.reset()\n",
    "        loader_H_input_train_target.reset()\n",
    "                \n",
    "        # loader_H = [loader_H_practical_train_source, loader_H_true_train_source, loader_H_practical_train_target, loader_H_true_train_target]\n",
    "        loader_H = [loader_H_input_train_source, loader_H_true_train_source, loader_H_input_train_target, loader_H_true_train_target]\n",
    "        \n",
    "        loss_fn = [loss_fn_ce, loss_fn_bce, loss_fn_domain]\n",
    "    \n",
    "        ##########################\n",
    "        if epoch in [int(n_epochs * r) for r in [0, 0.25, 0.5, 0.75, 1.0]]:\n",
    "            # return_features == return features to calculate PAD\n",
    "            return_features = True\n",
    "            epoc_pad.append(epoch)\n",
    "        else:\n",
    "            return_features = False\n",
    "\n",
    "        ##########################\n",
    "        train_step_output = utils_GAN.train_step_wgan_gp(model, model_domain, loader_H, loss_fn, optimizer, lower_range=-1,\n",
    "                                adv_weight=adv_weight, est_weight=est_weight, domain_weight=domain_weight, return_features=return_features, linear_interp=linear_interp)\n",
    "        \n",
    "        train_epoc_loss_est        = train_step_output.avg_epoc_loss_est\n",
    "        train_epoc_loss_d          = train_step_output.avg_epoc_loss_d\n",
    "        train_epoc_loss_domain     = train_step_output.avg_epoc_loss_domain\n",
    "        train_epoc_loss            = train_step_output.avg_epoc_loss\n",
    "        train_epoc_loss_est_target = train_step_output.avg_epoc_loss_est_target\n",
    "                # train_epoc_loss        = total train loss = loss_est + lambda_domain * domain_loss\n",
    "                # train_epoc_loss_est    = loss in estimation network in source domain (labels available)\n",
    "                # train_epoc_loss_domain = loss in domain discrimination network\n",
    "                # train_epoc_loss_est_target - just to monitor - the machine can not calculate because no label available in source domain\n",
    "                # All are already calculated in average over training dataset (source/target - respectively)\n",
    "        print(\"Time\", time.perf_counter() - start, \"seconds\")\n",
    "        # Calculate PAD for the extracted features\n",
    "        if return_features and (domain_weight!=0):\n",
    "            features_source_file = \"features_source.h5\"\n",
    "            features_target_file = \"features_target.h5\"\n",
    "            print(f\"epoch {epoch+1}/{n_epochs}\")\n",
    "            # pad_epoc_sgd  = PAD.cal_PAD_SGD(features_source_file, features_target_file)\n",
    "            pad_epoc  = PAD.cal_PAD2(features_source_file, features_target_file, pca_components=100, batch_size=128)\n",
    "            pad_observe.append(pad_epoc)\n",
    "            if os.path.exists(features_source_file):\n",
    "                os.remove(features_source_file)\n",
    "            if os.path.exists(features_target_file):\n",
    "                os.remove(features_target_file)\n",
    "            print(\"Time\", time.perf_counter() - start, \"seconds\")\n",
    "            \n",
    "        \n",
    "        # Average loss for the epoch\n",
    "        train_loss.append(train_epoc_loss)\n",
    "        print(f\"epoch {epoch+1}/{n_epochs} Average Training Loss: {train_epoc_loss:.6f}\")\n",
    "        #\n",
    "        train_est_loss.append(train_epoc_loss_est)\n",
    "        print(f\"epoch {epoch+1}/{n_epochs} Average Estimation Loss (in Source domain): {train_epoc_loss_est:.6f}\")\n",
    "        #\n",
    "        train_disc_loss.append(train_epoc_loss_d)\n",
    "        print(f\"epoch {epoch+1}/{n_epochs} Average Disc Loss (in Source domain): {train_epoc_loss_d:.6f}\")\n",
    "        #\n",
    "        train_domain_loss.append(train_epoc_loss_domain)\n",
    "        print(f\"epoch {epoch+1}/{n_epochs} Average Domain Discrimination Loss: {train_epoc_loss_domain:.6f}\")\n",
    "        #\n",
    "        train_est_loss_target.append(train_epoc_loss_est_target)\n",
    "        print(f\"epoch {epoch+1}/{n_epochs} For observation only - Average Estimation Loss in Target domain: {train_epoc_loss_est_target:.6f}\")\n",
    "        \n",
    "        \n",
    "        # ===================== Evaluation =====================\n",
    "        loader_H_true_val_source.reset()\n",
    "        loader_H_input_val_source.reset()\n",
    "        loader_H_true_val_target.reset()\n",
    "        loader_H_input_val_target.reset()\n",
    "        loader_H_eval = [loader_H_input_val_source, loader_H_true_val_source, loader_H_input_val_target, loader_H_true_val_target]\n",
    "\n",
    "        loss_fn = [loss_fn_ce, loss_fn_bce, loss_fn_domain]\n",
    "        \n",
    "        # eval_func = utils_UDA_FiLM.val_step\n",
    "        if (epoch==epoch_min) or (epoch+1>epoch_min and (epoch-epoch_min)%epoch_step==0) or epoch==n_epochs-1:\n",
    "            H_sample, epoc_val_return = utils_GAN.val_step_wgan_gp(model, model_domain, loader_H_eval, loss_fn, lower_range, \n",
    "                                            adv_weight=adv_weight, est_weight=est_weight, domain_weight=domain_weight, linear_interp=linear_interp)\n",
    "            utils_GAN.visualize_H(H_sample, H_to_save, epoch, plotfig.figChan, flag, model_path, sub_folder, domain_weight=domain_weight)\n",
    "            flag = 0  # after the first epoch, no need to save H_true anymore\n",
    "            \n",
    "        else:\n",
    "            _, epoc_val_return = utils_GAN.val_step_wgan_gp(model, model_domain, loader_H_eval, loss_fn, lower_range, \n",
    "                                            adv_weight=adv_weight, est_weight=est_weight, domain_weight=domain_weight, linear_interp=linear_interp)\n",
    "        \n",
    "        utils_GAN.post_val(epoc_val_return, epoch, n_epochs, val_est_loss, val_est_loss_source, val_loss, val_est_loss_target,\n",
    "            val_gan_disc_loss, val_domain_disc_loss, nmse_val_source, nmse_val_target, nmse_val, source_acc, target_acc, acc, domain_weight=domain_weight)\n",
    "                \n",
    "        if (epoch==epoch_min) or (epoch+1>epoch_min and (epoch-epoch_min)%epoch_step==0) or epoch==n_epochs-1:\n",
    "            utils_GAN.save_checkpoint(model, save_model, model_path, sub_folder, epoch, plotfig.figLoss, savemat, train_loss, train_est_loss, train_domain_loss, train_est_loss_target,\n",
    "                    val_est_loss, val_est_loss_source, val_loss, val_est_loss_target, val_gan_disc_loss, val_domain_disc_loss,\n",
    "                    source_acc, target_acc, acc, nmse_val_source, nmse_val_target, nmse_val, pad_observe, epoc_pad, train_disc_loss, domain_weight=domain_weight, optimizer=optimizer)\n",
    "    \n",
    "        else:\n",
    "            os.makedirs(f\"{model_path}/{sub_folder}/model/\", exist_ok=True)\n",
    "            # model.save(f\"{model_path}/{sub_folder}/model/epoch_.keras\")\n",
    "            content = \"Model at epoch \" + str(epoch+1)\n",
    "            txt_file = os.path.join(model_path, sub_folder, \"model\", \"readme.txt\")\n",
    "            with open(txt_file, \"w\") as f:\n",
    "                f.write(f\"Model at epoch {epoch+1}\\n\")\n",
    "        \n",
    "    # end of epoch loop\n",
    "    # =====================            \n",
    "    # Save performances\n",
    "    # Save H matrix\n",
    "    savemat(model_path + '/' + sub_folder + '/H_visualize/H_trix.mat', H_to_save)\n",
    "\n",
    "# end of trainmode   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
