{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78959c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 14:26:46.531251: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1763062006.546081   45645 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1763062006.550502   45645 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1763062006.561471   45645 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1763062006.561484   45645 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1763062006.561486   45645 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1763062006.561487   45645 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-13 14:26:46.565284: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n",
      "scipy version: 1.13.1\n",
      "POT version: 0.9.6.post1\n",
      "/home/thien/Code/NTN/Hest_NTN_UDA/Domain_Adversarial\n",
      "Append path /home/thien/Code/NTN/Hest_NTN_UDA/Domain_Adversarial/helper\n",
      "N_samp_source =  2048\n",
      "N_samp_target =  2048\n",
      "Processing: GAN_linear\n",
      "Calculating Wasserstein-1 distance for original input training datasets (before training)...\n",
      "X shape =  (192, 528)\n",
      "X1 shape =  (96, 528) y1 shape =  (96,)\n",
      "(96, 528) (96,)\n",
      "C: 0.01, Error rate: 0.5208\n",
      "C: 0.1, Error rate: 0.5208\n",
      "C: 0.5, Error rate: 0.0000\n",
      "C: 1.0, Error rate: 0.0000\n",
      "C: 2.0, Error rate: 0.0000\n",
      "C: 5.0, Error rate: 0.0000\n",
      "C: 10.0, Error rate: 0.0000\n",
      "C: 50.0, Error rate: 0.0000\n",
      "C: 100.0, Error rate: 0.0000\n",
      "C: 500.0, Error rate: 0.0000\n",
      "C: 1000.0, Error rate: 0.0000\n",
      "Best C: 0.5, Best error rate: 0.0000\n",
      "PAD = 2.0000\n",
      "PAD = 2.0000\n",
      "Fitted PCA on batch: source 96/96, target 96/96\n",
      "Reduced source shape: (96, 100), target shape: (96, 100)\n",
      "== C: 0.01, Error rate: 0.5208\n",
      "== C: 0.1, Error rate: 0.0938\n",
      "== C: 0.5, Error rate: 0.0625\n",
      "== C: 1.0, Error rate: 0.0729\n",
      "== C: 2.0, Error rate: 0.0729\n",
      "== C: 5.0, Error rate: 0.0729\n",
      "== C: 10.0, Error rate: 0.0729\n",
      "== C: 50.0, Error rate: 0.0729\n",
      "== C: 100.0, Error rate: 0.0729\n",
      "== C: 500.0, Error rate: 0.0729\n",
      "== C: 1000.0, Error rate: 0.0729\n",
      "Best C: 0.5, Best error rate: 0.0625\n",
      "============ PAD (SVM) = 1.7500\n",
      "LDA Error rate: 0.3854\n",
      "============ PAD (LDA) = 0.4583\n",
      "Logistic Regression Error rate: 0.4479\n",
      "============ PAD (LogReg) = 0.2083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1763062015.620353   45645 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9231 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:1a:00.0, compute capability: 7.5\n",
      "I0000 00:00:1763062015.620928   45645 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 9543 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:67:00.0, compute capability: 7.5\n",
      "I0000 00:00:1763062015.621393   45645 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 9372 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:68:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import scipy\n",
    "print(f\"scipy version: {scipy.__version__}\")\n",
    "\n",
    "# import subprocess\n",
    "# subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"POT\"])\n",
    "\n",
    "import ot\n",
    "print(f\"POT version: {ot.__version__}\")\n",
    "\n",
    "from scipy.io import savemat, loadmat\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "script_dir = os.path.abspath(os.path.join(os.getcwd(), '..','..','..'))\n",
    "print(script_dir) # need to be in Domain_Adversarial/\n",
    "notebook_dir = script_dir\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..','..','..', 'helper')))\n",
    "# import utils\n",
    "# import loader\n",
    "print('Append path', os.path.abspath(os.path.join(os.getcwd(), '..','..','..', 'helper')))\n",
    "import utils_GAN, PAD\n",
    "\n",
    "\n",
    "import utils\n",
    "import loader\n",
    "import plotfig\n",
    "\n",
    "\n",
    "SNR = -5\n",
    "# source_data_file_path_label = os.path.abspath(os.path.join(notebook_dir, '..', 'generatedChan', 'OpenNTN','H_perfect.mat'))\n",
    "# target_data_file_path = os.path.abspath(os.path.join(notebook_dir, '..', 'generatedChan', 'OpenNTN', f'SNR_{SNR}dB','sionnaNTN.mat'))\n",
    "target_data_file_path = os.path.abspath(os.path.join(notebook_dir, '..', 'generatedChan', 'MATLAB', 'TDL_B_100_300_simple', f'SNR_{SNR}dB','matlabNTN.mat'))\n",
    "source_data_file_path = os.path.abspath(os.path.join(notebook_dir, '..', 'generatedChan', 'MATLAB', 'TDL_A_300_simple', f'SNR_{SNR}dB','matlabNTN.mat'))\n",
    "\n",
    "norm_approach = 'minmax' # can be set to 'std'\n",
    "lower_range = -1 \n",
    "    # if norm_approach = 'minmax': \n",
    "        # =  0 for scaling to  [0 1]\n",
    "        # = -1 for scaling to [-1 1]\n",
    "    # if norm_approach = 'std': can be any value, but need to be defined\n",
    "adv_weight=0.005\n",
    "est_weight=1\n",
    "domain_weight=0.5 \n",
    "if norm_approach == 'minmax':\n",
    "    if lower_range == 0:\n",
    "        norm_txt = 'Using min-max [0 1]'\n",
    "    elif lower_range ==-1:\n",
    "        norm_txt = 'Using min-max [-1 1]'\n",
    "elif norm_approach == 'no':\n",
    "    norm_txt = 'No'\n",
    "    \n",
    "CNN_activation = 'Tanh'\n",
    "CNN_DropOut = 0.2\n",
    "if CNN_DropOut != 0:\n",
    "    dropOut_txt = f'Add p={CNN_DropOut} DropOut'\n",
    "    \n",
    "# Paths to save\n",
    "path_temp = notebook_dir + f'/model/GAN_cal/{SNR}_dB/'\n",
    "os.makedirs(os.path.dirname(path_temp), exist_ok=True)\n",
    "idx_save_path = loader.find_incremental_filename(path_temp,'ver', '_', '')\n",
    "\n",
    "save_model = True\n",
    "model_path = notebook_dir + f'/model/GAN_cal/{SNR}_dB/ver' + str(idx_save_path) + '_'\n",
    "# figure_path = notebook_dir + '/model/GAN/ver' + str(idx_save_path) + '_/figure'\n",
    "model_readme = model_path + '/readme.txt'\n",
    "\n",
    "import h5py\n",
    "import scipy.io\n",
    "\n",
    "batch_size=16\n",
    "\n",
    "# ============ Source data ==============\n",
    "source_file = h5py.File(source_data_file_path, 'r')\n",
    "H_true_source = source_file['H_perfect']\n",
    "N_samp_source = H_true_source.shape[0]\n",
    "print('N_samp_source = ', N_samp_source)\n",
    "\n",
    "# ============ Target data ==============\n",
    "target_file = h5py.File(target_data_file_path, 'r')\n",
    "H_true_target = target_file['H_perfect']\n",
    "N_samp_target = H_true_target.shape[0]\n",
    "print('N_samp_target = ', N_samp_target)\n",
    "\n",
    "# Store random state \n",
    "rng_state = np.random.get_state()\n",
    "\n",
    "# --- Set a temporary seed for reproducible split ---\n",
    "np.random.seed(1234)   # any fixed integer seed\n",
    "# Random but repeatable split\n",
    "indices_source = np.arange(N_samp_source)\n",
    "np.random.shuffle(indices_source)\n",
    "indices_target = np.arange(N_samp_target)\n",
    "np.random.shuffle(indices_target)\n",
    "# Restore previous random state (so other code stays random)\n",
    "np.random.set_state(rng_state)\n",
    "#\n",
    "train_size = int(np.floor(N_samp_source * 0.9) // batch_size * batch_size)\n",
    "val_size = N_samp_source - train_size\n",
    "\n",
    "# Repeat the indices to match the maximum number of samples\n",
    "N_samp = max(N_samp_source, N_samp_target) \n",
    "indices_source = np.resize(indices_source, N_samp)\n",
    "indices_target = np.resize(indices_target, N_samp)\n",
    "\n",
    "# =======================================================\n",
    "## Divide the indices into training and validation sets\n",
    "# indices_train_source = indices_source[:train_size]\n",
    "# indices_val_source   = indices_source[train_size:train_size + val_size]\n",
    "\n",
    "# indices_train_target = indices_target[:train_size]\n",
    "# indices_val_target   = indices_target[train_size:train_size + val_size]\n",
    "\n",
    "# to test code\n",
    "indices_train_source = indices_source[:96]\n",
    "indices_val_source = indices_source[2016:] # 2032\n",
    "indices_train_target = indices_target[:96]\n",
    "indices_val_target = indices_target[2016:]\n",
    "\n",
    "class DataLoaders:\n",
    "    def __init__(self, file, indices_train, indices_val, tag='prac', batch_size=32): \n",
    "        # tag = 'prac' or 'li' or 'ls'\n",
    "        self.true_train = utils.H5BatchLoader(file, dataset_name='H_perfect', batch_size=batch_size, shuffled_indices=indices_train)\n",
    "        self.true_val = utils.H5BatchLoader(file, dataset_name='H_perfect', batch_size=batch_size, shuffled_indices=indices_val)\n",
    "\n",
    "        self.input_train = utils.H5BatchLoader(file, f'H_{tag}', batch_size=batch_size, shuffled_indices=indices_train)\n",
    "        self.input_val = utils.H5BatchLoader(file, f'H_{tag}', batch_size=batch_size, shuffled_indices=indices_val)\n",
    "\n",
    "# Source domain\n",
    "class_dict_source = {\n",
    "    'GAN_practical': DataLoaders(source_file, indices_train_source, indices_val_source, tag='prac', batch_size=batch_size),\n",
    "    'GAN_linear': DataLoaders(source_file, indices_train_source, indices_val_source, tag='li', batch_size=batch_size),\n",
    "    'GAN_ls': DataLoaders(source_file, indices_train_source, indices_val_source, tag='ls', batch_size=batch_size)\n",
    "}\n",
    "\n",
    "# Target domain\n",
    "class_dict_target = {\n",
    "    'GAN_practical': DataLoaders(target_file, indices_train_target, indices_val_target, tag='prac', batch_size=batch_size),\n",
    "    'GAN_linear': DataLoaders(target_file, indices_train_target, indices_val_target, tag='li', batch_size=batch_size),\n",
    "    'GAN_ls': DataLoaders(target_file, indices_train_target, indices_val_target, tag='ls', batch_size=batch_size)\n",
    "}\n",
    "\n",
    "loss_fn_ce = tf.keras.losses.MeanSquaredError()  # Channel estimation loss (generator loss)\n",
    "loss_fn_bce = tf.keras.losses.BinaryCrossentropy(from_logits=False) # Binary cross-entropy loss for discriminator\n",
    "loss_fn_domain = tf.keras.losses.BinaryCrossentropy()  # Domain classification loss\n",
    "\n",
    "load_checkpoint = False  # True if continue training\n",
    "if load_checkpoint:\n",
    "    # model_path = notebook_dir + '/model/GAN_cal/ver' + str(idx_save_path-1) + '_' # or replace idx_save_path-1 by the desired folder index\n",
    "    model_path = notebook_dir + f'/model/GAN_cal/{SNR}_dB/ver' + str(idx_save_path-1) + '_'\n",
    "else:\n",
    "    model_path = notebook_dir + f'/model/GAN_cal/{SNR}_dB/ver' + str(idx_save_path) + '_'\n",
    "if load_checkpoint:\n",
    "    start_epoch = 3  # This is the epoch we want to CONTINUE FROM (not load from)\n",
    "else:\n",
    "    start_epoch = 0    \n",
    "\n",
    "import time\n",
    "start = time.perf_counter()\n",
    "\n",
    "# n_epochs= 300\n",
    "# epoch_min = 20\n",
    "# epoch_step = 20\n",
    "n_epochs= 3\n",
    "epoch_min = 0\n",
    "epoch_step = 1\n",
    "\n",
    "sub_folder = 'GAN_linear'  # 'GAN_linear', 'GAN_practical', 'GAN_ls'\n",
    "print(f\"Processing: {sub_folder}\")\n",
    "\n",
    "w_dist = []\n",
    "pad_pca_lda = []\n",
    "pad_pca_logreg = []\n",
    "pad_pca_svm = []\n",
    "linear_interp = False\n",
    "if sub_folder == 'GAN_linear':\n",
    "    linear_interp =True # flag to clip values that go beyond the estimated pilot (min, max)\n",
    "##\n",
    "loader_H_true_train_source = class_dict_source[sub_folder].true_train\n",
    "loader_H_input_train_source = class_dict_source[sub_folder].input_train\n",
    "loader_H_true_val_source = class_dict_source[sub_folder].true_val\n",
    "loader_H_input_val_source = class_dict_source[sub_folder].input_val\n",
    "\n",
    "loader_H_true_train_target = class_dict_target[sub_folder].true_train\n",
    "loader_H_input_train_target = class_dict_target[sub_folder].input_train\n",
    "loader_H_true_val_target = class_dict_target[sub_folder].true_val\n",
    "loader_H_input_val_target = class_dict_target[sub_folder].input_val\n",
    "##\n",
    "\n",
    "# Distribution of original input training datasets (or before training)\n",
    "plotfig.plotHist(loader_H_input_train_source, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name='source_beforeTrain', percent=99)\n",
    "plotfig.plotHist(loader_H_input_train_target, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name='target_beforeTrain', percent=99)\n",
    "\n",
    "plotfig.plotHist(loader_H_input_train_source, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name='source_beforeTrain', percent=95)\n",
    "plotfig.plotHist(loader_H_input_train_target, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name='target_beforeTrain', percent=95)\n",
    "\n",
    "plotfig.plotHist(loader_H_input_train_source, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name='source_beforeTrain', percent=90)\n",
    "plotfig.plotHist(loader_H_input_train_target, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name='target_beforeTrain', percent=90)\n",
    "\n",
    "# Calculate Wasserstein-1 distance for original input training datasets (before training)\n",
    "if load_checkpoint==False:\n",
    "    print(\"Calculating Wasserstein-1 distance for original input training datasets (before training)...\")\n",
    "    w_dist_epoc = plotfig.wasserstein_approximate(loader_H_input_train_source, loader_H_input_train_target)\n",
    "    w_dist.append(w_dist_epoc)\n",
    "\n",
    "    # Calculate     PAD for original input training datasets with SVM\n",
    "    pad_svm = PAD.original_PAD(loader_H_input_train_source, loader_H_input_train_target)\n",
    "    print(f\"PAD = {pad_svm:.4f}\")\n",
    "\n",
    "    # Calculate PCA_PAD for original input training datasets with PCA_SVM, PCA_LDA, PCA_LogReg\n",
    "    X_features_, y_features_ = PAD.extract_features_with_pca(loader_H_input_train_source, loader_H_input_train_target, pca_components=100)\n",
    "    pad_pca_svm_epoc = PAD.calc_pad_svm(X_features_, y_features_)\n",
    "    pad_pca_lda_epoc = PAD.calc_pad_lda(X_features_, y_features_)\n",
    "    pad_pca_logreg_epoc = PAD.calc_pad_logreg(X_features_, y_features_)\n",
    "\n",
    "    pad_pca_lda.append(pad_pca_lda_epoc)\n",
    "    pad_pca_logreg.append(pad_pca_logreg_epoc)\n",
    "    pad_pca_svm.append(pad_pca_svm_epoc)\n",
    "## \n",
    "\n",
    "if not os.path.exists(os.path.dirname(model_path + '/' + sub_folder +'/')):\n",
    "    os.makedirs(os.path.dirname(model_path + '/' + sub_folder + '/'))   # Domain_Adversarial/model/_/ver_/{sub_folder}\n",
    "\n",
    "flag = 1 # flag to plot and save H_true\n",
    "H_to_save = {} \n",
    "\n",
    "if load_checkpoint==False:\n",
    "    train_loss          = [] # (epoch,1)\n",
    "    train_est_loss      = [] \n",
    "    train_disc_loss     = [] \n",
    "    train_domain_loss   = []\n",
    "    train_est_loss_target = []\n",
    "    #    \n",
    "    val_loss, val_gan_disc_loss, val_domain_disc_loss,\\\n",
    "    val_est_loss_source, val_est_loss_target, val_est_loss,\\\n",
    "    source_acc, target_acc, acc,\\\n",
    "    nmse_val_source, nmse_val_target, nmse_val = [[] for _ in range(12)]\n",
    "    #\n",
    "\n",
    "    model = utils_GAN.GAN(n_subc=312, gen_l2=None, disc_l2=1e-5)  # l2 regularization for generator and discriminator\n",
    "    model_domain = utils_GAN.DomainDisc()\n",
    "    gen_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4, beta_1=0.5, beta_2=0.9)\n",
    "    disc_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, beta_1=0.5, beta_2=0.9)  # WGAN-GP uses Adam optimizer with beta_1=0.5\n",
    "    domain_optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "    ####\n",
    "    optimizer = [gen_optimizer, disc_optimizer, domain_optimizer]\n",
    "    ####\n",
    "\n",
    "    epoc_pad = []    # epochs in which we calculate pad (return_features == True)\n",
    "    pad_pca_lda = []\n",
    "    pad_pca_logreg = []\n",
    "    pad_pca_svm = []\n",
    "    pad_svm = 0\n",
    "else:   # load from check_point\n",
    "    model = utils_GAN.GAN(n_subc=132, gen_l2=None, disc_l2=1e-5)\n",
    "    # model.build(input_shape=(None, 16, 312, 14, 2))\n",
    "    dummy_input = tf.random.normal((batch_size, 132, 14, 2))\n",
    "    _ = model(dummy_input)  # This builds the model with proper weights initialization\n",
    "    # Build domain discriminator\n",
    "    model_domain = utils_GAN.DomainDisc()\n",
    "    dummy_input = tf.random.normal((batch_size, 7, 14, 256))\n",
    "    _ = model_domain(dummy_input)\n",
    "    # \n",
    "    # Load checkpoint from the epoch we want to continue from (start_epoch-1 because we want to continue FROM start_epoch)\n",
    "    epoch_load = start_epoch - 1  # Load the checkpoint from the previous epoch\n",
    "    \n",
    "    print(f\"Loading checkpoint from epoch {epoch_load+1} to continue training from epoch {start_epoch}...\")\n",
    "\n",
    "    # Load checkpoint (this will also restore optimizers automatically)\n",
    "    gen_optimizer, disc_optimizer, domain_optimizer = utils_GAN.load_checkpoint(\n",
    "        model,\n",
    "        model_path,\n",
    "        sub_folder,\n",
    "        epoch_load,\n",
    "        domain_model=model_domain,\n",
    "        domain_weight=domain_weight  # Use the same domain_weight as current training\n",
    "    )\n",
    "    optimizer = [gen_optimizer, disc_optimizer, domain_optimizer]    \n",
    "    \n",
    "    print(\"=== Optimizer State Verification ===\")\n",
    "    print(f\"Generator optimizer learning rate: {gen_optimizer.learning_rate.numpy()}\")\n",
    "    print(f\"Discriminator optimizer learning rate: {disc_optimizer.learning_rate.numpy()}\")\n",
    "    print(f\"Domain optimizer learning rate: {domain_optimizer.learning_rate.numpy()}\")\n",
    "\n",
    "    # Check if optimizers have momentum/state from previous training\n",
    "    print(f\"Gen optimizer iterations: {gen_optimizer.iterations.numpy()}\")\n",
    "    print(f\"Disc optimizer iterations: {disc_optimizer.iterations.numpy()}\")\n",
    "    print(f\"Domain optimizer iterations: {domain_optimizer.iterations.numpy()}\")\n",
    "    \n",
    "    # Load performance history UP TO start_epoch (not including it)\n",
    "    loadmat_params = loadmat(f\"{model_path}/{sub_folder}/performance/performance.mat\")\n",
    "    train_loss          = loadmat_params['train_loss'].flatten().tolist()[:start_epoch]\n",
    "    train_est_loss      = loadmat_params['train_est_loss'].flatten().tolist()[:start_epoch]\n",
    "    train_disc_loss     = loadmat_params['train_disc_loss'].flatten().tolist()[:start_epoch]\n",
    "    train_domain_loss   = loadmat_params['train_domain_loss'].flatten().tolist()[:start_epoch]\n",
    "    train_est_loss_target = loadmat_params['train_est_loss_target'].flatten().tolist()[:start_epoch]\n",
    "    #    \n",
    "    val_loss             = loadmat_params['val_loss'].flatten().tolist()[:start_epoch]\n",
    "    val_gan_disc_loss    = loadmat_params['val_gan_disc_loss'].flatten().tolist()[:start_epoch]\n",
    "    val_domain_disc_loss = loadmat_params['val_domain_disc_loss'].flatten().tolist()[:start_epoch]\n",
    "    val_est_loss_source  = loadmat_params['val_est_loss_source'].flatten().tolist()[:start_epoch]\n",
    "    val_est_loss_target  = loadmat_params['val_est_loss_target'].flatten().tolist()[:start_epoch]\n",
    "    val_est_loss         = loadmat_params['val_est_loss'].flatten().tolist()[:start_epoch]\n",
    "    source_acc           = loadmat_params['source_acc'].flatten().tolist()[:start_epoch]\n",
    "    target_acc           = loadmat_params['target_acc'].flatten().tolist()[:start_epoch]\n",
    "    acc                  = loadmat_params['acc'].flatten().tolist()[:start_epoch]\n",
    "    nmse_val_source      = loadmat_params['nmse_val_source'].flatten().tolist()[:start_epoch]\n",
    "    nmse_val_target      = loadmat_params['nmse_val_target'].flatten().tolist()[:start_epoch]\n",
    "    nmse_val             = loadmat_params['nmse_val'].flatten().tolist()[:start_epoch]\n",
    "    #\n",
    "    epoc_pad             = loadmat_params['epoc_pad'].flatten().tolist()\n",
    "    pad_pca_lda          = loadmat_params['pad_pca_lda'].flatten().tolist()\n",
    "    pad_pca_logreg       = loadmat_params['pad_pca_logreg'].flatten().tolist()\n",
    "    pad_pca_svm          = loadmat_params['pad_pca_svm'].flatten().tolist()\n",
    "    pad_svm              = loadmat_params['pad_svm']\n",
    "\n",
    "    print(f\"Loaded {len(train_loss)} epochs of training history.\")\n",
    "    print(f\"Last loaded training loss: {train_loss[-1] if train_loss else 'No history'}\")\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a531a917",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1763062022.791454   45645 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 22.46975646202918 seconds\n",
      "epoch 1/3\n",
      "Fitting IncrementalPCA on batches from features_source.h5 and features_target.h5\n",
      "Fitted PCA on batch: source 96/96, target 96/96\n",
      "Reduced source shape: (96, 100), target shape: (96, 100)\n",
      "== C: 0.01, Error rate: 0.5208\n",
      "== C: 0.1, Error rate: 0.0000\n",
      "== C: 0.5, Error rate: 0.0000\n",
      "== C: 1.0, Error rate: 0.0000\n",
      "== C: 2.0, Error rate: 0.0000\n",
      "== C: 5.0, Error rate: 0.0000\n",
      "== C: 10.0, Error rate: 0.0000\n",
      "== C: 50.0, Error rate: 0.0000\n",
      "== C: 100.0, Error rate: 0.0000\n",
      "== C: 500.0, Error rate: 0.0000\n",
      "== C: 1000.0, Error rate: 0.0000\n",
      "Best C: 0.1, Best error rate: 0.0000\n",
      "============ PAD (SVM) = 2.0000\n",
      "LDA Error rate: 0.3125\n",
      "============ PAD (LDA) = 0.7500\n",
      "Logistic Regression Error rate: 0.3854\n",
      "============ PAD (LogReg) = 0.4583\n",
      "Time 25.763858322054148 seconds\n",
      "epoch 1/3 Average Training Loss: 0.296534\n",
      "epoch 1/3 Average Estimation Loss (in Source domain): 0.119500\n",
      "epoch 1/3 Average Disc Loss (in Source domain): 4606.098633\n",
      "epoch 1/3 Average Domain Discrimination Loss: 0.728810\n",
      "epoch 1/3 For observation only - Average Estimation Loss in Target domain: 0.127383\n",
      "epoch 1/3 (Val) Weighted Total Loss: 58.528797\n",
      "epoch 1/3 (Val) Average Estimation Loss (mean): 0.184723\n",
      "epoch 1/3 (Val) Average Estimation Loss (Source): 0.155780\n",
      "epoch 1/3 (Val) Average Estimation Loss (Target): 0.213667\n",
      "epoch 1/3 (Val) GAN Discriminator Loss: 11524.785156\n",
      "epoch 1/3 (Val) Domain Discriminator Loss: 1.440300\n",
      "epoch 1/3 (Val) NMSE (Source): 3.008508, NMSE (Target): 4.813875, NMSE (Mean): 3.911192\n",
      "epoch 1/3 (Val) Domain Discriminator Accuracy (Average): 0.0469\n",
      "Checkpoint saved at epoch 1: /home/thien/Code/NTN/Hest_NTN_UDA/Domain_Adversarial/model/GAN_cal/-5_dB/ver6_/GAN_linear/model//epoch_1-1\n",
      "Optimizer configs saved to: /home/thien/Code/NTN/Hest_NTN_UDA/Domain_Adversarial/model/GAN_cal/-5_dB/ver6_/GAN_linear/model//optimizer_configs.json\n",
      "Time 32.3486137019936 seconds\n",
      "epoch 2/3\n",
      "Fitting IncrementalPCA on batches from features_source.h5 and features_target.h5\n",
      "Fitted PCA on batch: source 96/96, target 96/96\n",
      "Reduced source shape: (96, 100), target shape: (96, 100)\n",
      "== C: 0.01, Error rate: 0.5208\n",
      "== C: 0.1, Error rate: 0.0000\n",
      "== C: 0.5, Error rate: 0.0000\n",
      "== C: 1.0, Error rate: 0.0000\n",
      "== C: 2.0, Error rate: 0.0000\n",
      "== C: 5.0, Error rate: 0.0000\n",
      "== C: 10.0, Error rate: 0.0000\n",
      "== C: 50.0, Error rate: 0.0000\n",
      "== C: 100.0, Error rate: 0.0000\n",
      "== C: 500.0, Error rate: 0.0000\n",
      "== C: 1000.0, Error rate: 0.0000\n",
      "Best C: 0.1, Best error rate: 0.0000\n",
      "============ PAD (SVM) = 2.0000\n",
      "LDA Error rate: 0.1458\n",
      "============ PAD (LDA) = 1.4167\n",
      "Logistic Regression Error rate: 0.3854\n",
      "============ PAD (LogReg) = 0.4583\n",
      "Time 35.47030225605704 seconds\n",
      "epoch 2/3 Average Training Loss: 0.260624\n",
      "epoch 2/3 Average Estimation Loss (in Source domain): 0.077107\n",
      "epoch 2/3 Average Disc Loss (in Source domain): 3729.290771\n",
      "epoch 2/3 Average Domain Discrimination Loss: 0.751968\n",
      "epoch 2/3 For observation only - Average Estimation Loss in Target domain: 0.091905\n",
      "epoch 2/3 (Val) Weighted Total Loss: 48.259842\n",
      "epoch 2/3 (Val) Average Estimation Loss (mean): 0.141004\n",
      "epoch 2/3 (Val) Average Estimation Loss (Source): 0.110930\n",
      "epoch 2/3 (Val) Average Estimation Loss (Target): 0.171078\n",
      "epoch 2/3 (Val) GAN Discriminator Loss: 9475.115234\n",
      "epoch 2/3 (Val) Domain Discriminator Loss: 1.486526\n",
      "epoch 2/3 (Val) NMSE (Source): 2.157201, NMSE (Target): 3.904162, NMSE (Mean): 3.030681\n",
      "epoch 2/3 (Val) Domain Discriminator Accuracy (Average): 0.2188\n",
      "Checkpoint saved at epoch 2: /home/thien/Code/NTN/Hest_NTN_UDA/Domain_Adversarial/model/GAN_cal/-5_dB/ver6_/GAN_linear/model//epoch_2-1\n",
      "Time 41.178136984002776 seconds\n",
      "epoch 3/3\n",
      "Fitting IncrementalPCA on batches from features_source.h5 and features_target.h5\n",
      "Fitted PCA on batch: source 96/96, target 96/96\n",
      "Reduced source shape: (96, 100), target shape: (96, 100)\n",
      "== C: 0.01, Error rate: 0.5208\n",
      "== C: 0.1, Error rate: 0.0000\n",
      "== C: 0.5, Error rate: 0.0000\n",
      "== C: 1.0, Error rate: 0.0104\n",
      "== C: 2.0, Error rate: 0.0104\n",
      "== C: 5.0, Error rate: 0.0104\n",
      "== C: 10.0, Error rate: 0.0104\n",
      "== C: 50.0, Error rate: 0.0104\n",
      "== C: 100.0, Error rate: 0.0104\n",
      "== C: 500.0, Error rate: 0.0104\n",
      "== C: 1000.0, Error rate: 0.0104\n",
      "Best C: 0.1, Best error rate: 0.0000\n",
      "============ PAD (SVM) = 2.0000\n",
      "LDA Error rate: 0.3542\n",
      "============ PAD (LDA) = 0.5833\n",
      "Logistic Regression Error rate: 0.3542\n",
      "============ PAD (LogReg) = 0.5833\n",
      "Time 44.28138586098794 seconds\n",
      "epoch 3/3 Average Training Loss: 0.248413\n",
      "epoch 3/3 Average Estimation Loss (in Source domain): 0.060085\n",
      "epoch 3/3 Average Disc Loss (in Source domain): 3679.891602\n",
      "epoch 3/3 Average Domain Discrimination Loss: 0.769838\n",
      "epoch 3/3 For observation only - Average Estimation Loss in Target domain: 0.077452\n",
      "epoch 3/3 (Val) Weighted Total Loss: 48.003891\n",
      "epoch 3/3 (Val) Average Estimation Loss (mean): 0.123018\n",
      "epoch 3/3 (Val) Average Estimation Loss (Source): 0.092653\n",
      "epoch 3/3 (Val) Average Estimation Loss (Target): 0.153383\n",
      "epoch 3/3 (Val) GAN Discriminator Loss: 9425.137695\n",
      "epoch 3/3 (Val) Domain Discriminator Loss: 1.510379\n",
      "epoch 3/3 (Val) NMSE (Source): 1.805045, NMSE (Target): 3.526843, NMSE (Mean): 2.665944\n",
      "epoch 3/3 (Val) Domain Discriminator Accuracy (Average): 0.3594\n",
      "Checkpoint saved at epoch 3: /home/thien/Code/NTN/Hest_NTN_UDA/Domain_Adversarial/model/GAN_cal/-5_dB/ver6_/GAN_linear/model//epoch_3-1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    # ===================== Training =====================\n",
    "    loader_H_true_train_source.reset()\n",
    "    # loader_H_practical_train_source.reset()\n",
    "    loader_H_input_train_source.reset()\n",
    "    loader_H_true_train_target.reset()\n",
    "    # loader_H_practical_train_target.reset()\n",
    "    loader_H_input_train_target.reset()\n",
    "            \n",
    "    # loader_H = [loader_H_practical_train_source, loader_H_true_train_source, loader_H_practical_train_target, loader_H_true_train_target]\n",
    "    loader_H = [loader_H_input_train_source, loader_H_true_train_source, loader_H_input_train_target, loader_H_true_train_target]\n",
    "    \n",
    "    loss_fn = [loss_fn_ce, loss_fn_bce, loss_fn_domain]\n",
    "\n",
    "    ##########################\n",
    "    if epoch in [int(n_epochs * r) for r in [0, 0.25, 0.5, 0.75]] or epoch == n_epochs-1:\n",
    "        # return_features == return features to calculate PAD\n",
    "        return_features = True\n",
    "        epoc_pad.append(epoch)\n",
    "    else:\n",
    "        return_features = False\n",
    "\n",
    "    ##########################\n",
    "    train_step_output = utils_GAN.train_step_wgan_gp(model, model_domain, loader_H, loss_fn, optimizer, lower_range=-1,\n",
    "                            adv_weight=adv_weight, est_weight=est_weight, domain_weight=domain_weight, return_features=return_features, linear_interp=linear_interp)\n",
    "    \n",
    "    train_epoc_loss_est        = train_step_output.avg_epoc_loss_est\n",
    "    train_epoc_loss_d          = train_step_output.avg_epoc_loss_d\n",
    "    train_epoc_loss_domain     = train_step_output.avg_epoc_loss_domain\n",
    "    train_epoc_loss            = train_step_output.avg_epoc_loss\n",
    "    train_epoc_loss_est_target = train_step_output.avg_epoc_loss_est_target\n",
    "            # train_epoc_loss        = total train loss = loss_est + lambda_domain * domain_loss\n",
    "            # train_epoc_loss_est    = loss in estimation network in source domain (labels available)\n",
    "            # train_epoc_loss_domain = loss in domain discrimination network\n",
    "            # train_epoc_loss_est_target - just to monitor - the machine can not calculate because no label available in source domain\n",
    "            # All are already calculated in average over training dataset (source/target - respectively)\n",
    "    print(\"Time\", time.perf_counter() - start, \"seconds\")\n",
    "    \n",
    "    # Calculate PAD for the extracted features\n",
    "    if return_features and (domain_weight!=0):\n",
    "        features_source_file = \"features_source.h5\"\n",
    "        features_target_file = \"features_target.h5\"\n",
    "        print(f\"epoch {epoch+1}/{n_epochs}\")\n",
    "        ## Calculate PCA_PAD for extracted features with PCA_SVM, PCA_LDA, PCA_LogReg\n",
    "        X_features, y_features = PAD.extract_features_with_pca(features_source_file, features_target_file, pca_components=100)\n",
    "        pad_svm_epoc = PAD.calc_pad_svm(X_features, y_features)\n",
    "        pad_pca_svm.append(pad_svm_epoc)\n",
    "        #\n",
    "        pad_lda_epoc = PAD.calc_pad_lda(X_features, y_features)\n",
    "        pad_pca_lda.append(pad_lda_epoc)\n",
    "        #\n",
    "        pad_logreg_epoc = PAD.calc_pad_logreg(X_features, y_features)\n",
    "        pad_pca_logreg.append(pad_logreg_epoc)\n",
    "        \n",
    "        ## Distribution of extracted features\n",
    "        plotfig.plotHist(features_source_file, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name=f'source_epoch_{epoch+1}', percent=99)\n",
    "        plotfig.plotHist(features_target_file, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name=f'target_epoch_{epoch+1}', percent=99)\n",
    "        #\n",
    "        plotfig.plotHist(features_source_file, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name=f'source_epoch_{epoch+1}', percent=95)\n",
    "        plotfig.plotHist(features_target_file, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name=f'target_epoch_{epoch+1}', percent=95)\n",
    "        #\n",
    "        plotfig.plotHist(features_source_file, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name=f'source_epoch_{epoch+1}', percent=90)\n",
    "        plotfig.plotHist(features_target_file, fig_show = False, save_path=f\"{model_path}/{sub_folder}/Distribution/\", name=f'target_epoch_{epoch+1}', percent=90)\n",
    "        # Calculate Wasserstein-1 distance for extracted features\n",
    "        # print(\"Calculating Wasserstein-1 distance for extracted features ...\")\n",
    "        # w_dist_epoc = plotfig.wasserstein_approximate(features_source_file, features_target_file)\n",
    "        # w_dist.append(w_dist_epoc)\n",
    "        \n",
    "\n",
    "        if os.path.exists(features_source_file):\n",
    "            os.remove(features_source_file)\n",
    "        if os.path.exists(features_target_file):\n",
    "            os.remove(features_target_file)\n",
    "        print(\"Time\", time.perf_counter() - start, \"seconds\")\n",
    "        \n",
    "    \n",
    "    # Average loss for the epoch\n",
    "    train_loss.append(train_epoc_loss)\n",
    "    print(f\"epoch {epoch+1}/{n_epochs} Average Training Loss: {train_epoc_loss:.6f}\")\n",
    "    #\n",
    "    train_est_loss.append(train_epoc_loss_est)\n",
    "    print(f\"epoch {epoch+1}/{n_epochs} Average Estimation Loss (in Source domain): {train_epoc_loss_est:.6f}\")\n",
    "    #\n",
    "    train_disc_loss.append(train_epoc_loss_d)\n",
    "    print(f\"epoch {epoch+1}/{n_epochs} Average Disc Loss (in Source domain): {train_epoc_loss_d:.6f}\")\n",
    "    #\n",
    "    train_domain_loss.append(train_epoc_loss_domain)\n",
    "    print(f\"epoch {epoch+1}/{n_epochs} Average Domain Discrimination Loss: {train_epoc_loss_domain:.6f}\")\n",
    "    #\n",
    "    train_est_loss_target.append(train_epoc_loss_est_target)\n",
    "    print(f\"epoch {epoch+1}/{n_epochs} For observation only - Average Estimation Loss in Target domain: {train_epoc_loss_est_target:.6f}\")\n",
    "    \n",
    "    \n",
    "    # ===================== Evaluation =====================\n",
    "    loader_H_true_val_source.reset()\n",
    "    loader_H_input_val_source.reset()\n",
    "    loader_H_true_val_target.reset()\n",
    "    loader_H_input_val_target.reset()\n",
    "    loader_H_eval = [loader_H_input_val_source, loader_H_true_val_source, loader_H_input_val_target, loader_H_true_val_target]\n",
    "\n",
    "    loss_fn = [loss_fn_ce, loss_fn_bce, loss_fn_domain]\n",
    "    \n",
    "    # eval_func = utils_UDA_FiLM.val_step\n",
    "    if (epoch==epoch_min) or (epoch+1>epoch_min and (epoch-epoch_min)%epoch_step==0) and epoch!=n_epochs-1:\n",
    "        H_sample, epoc_val_return = utils_GAN.val_step_wgan_gp(model, model_domain, loader_H_eval, loss_fn, lower_range, \n",
    "                                        adv_weight=adv_weight, est_weight=est_weight, domain_weight=domain_weight, linear_interp=linear_interp)\n",
    "        utils_GAN.visualize_H(H_sample, H_to_save, epoch, plotfig.figChan, flag, model_path, sub_folder, domain_weight=domain_weight)\n",
    "        flag = 0  # after the first epoch, no need to save H_true anymore\n",
    "\n",
    "    elif epoch==n_epochs-1: # last epoch   \n",
    "        _, epoc_val_return, H_val_gen = utils_GAN.val_step_wgan_gp(model, model_domain, loader_H_eval, loss_fn, lower_range, \n",
    "                                        adv_weight=adv_weight, est_weight=est_weight, domain_weight=domain_weight, \n",
    "                                        linear_interp=linear_interp, return_H_gen=True)\n",
    "        \n",
    "    else:\n",
    "        _, epoc_val_return = utils_GAN.val_step_wgan_gp(model, model_domain, loader_H_eval, loss_fn, lower_range, \n",
    "                                        adv_weight=adv_weight, est_weight=est_weight, domain_weight=domain_weight, linear_interp=linear_interp)\n",
    "    \n",
    "    utils_GAN.post_val(epoc_val_return, epoch, n_epochs, val_est_loss, val_est_loss_source, val_loss, val_est_loss_target,\n",
    "        val_gan_disc_loss, val_domain_disc_loss, nmse_val_source, nmse_val_target, nmse_val, source_acc, target_acc, acc, domain_weight=domain_weight)\n",
    "    \n",
    "    \n",
    "    if (epoch==epoch_min) or (epoch+1>epoch_min and (epoch-epoch_min)%epoch_step==0) or epoch==n_epochs-1:\n",
    "        utils_GAN.save_checkpoint(model, save_model, model_path, sub_folder, epoch, plotfig.figLoss, savemat, train_loss, train_est_loss, train_domain_loss, train_est_loss_target,\n",
    "                val_est_loss, val_est_loss_source, val_loss, val_est_loss_target, val_gan_disc_loss, val_domain_disc_loss,\n",
    "                source_acc, target_acc, acc, nmse_val_source, nmse_val_target, nmse_val, pad_pca_svm, pad_pca_lda, pad_pca_logreg, epoc_pad, pad_svm, train_disc_loss, \n",
    "                domain_weight=domain_weight, optimizer=optimizer, domain_model=model_domain)\n",
    "        \n",
    "# end of epoch loop\n",
    "# =====================            \n",
    "# Save performances\n",
    "# Save H matrix\n",
    "savemat(model_path + '/' + sub_folder + '/H_visualize/H_trix.mat', H_to_save)\n",
    "savemat(model_path + '/' + sub_folder + '/H_visualize/H_val_generated.mat', \n",
    "        {'H_val_gen': H_val_gen,\n",
    "        'indices_source': indices_source,\n",
    "        'indices_target': indices_target})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b5a3bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/thien/Code/NTN/Hest_NTN_UDA/Domain_Adversarial/model/GAN_cal/-5_dB/ver6_/GAN_linear/H_visualize/H_trix.mat'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path + '/' + sub_folder + '/H_visualize/H_trix.mat'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
